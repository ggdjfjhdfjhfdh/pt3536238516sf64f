#!/usr/bin/env python3
"""
Optimizador de Entrenamiento ML para PentestExpress

Este módulo implementa técnicas avanzadas para mejorar el entrenamiento
de modelos ML con datos de mejor calidad y técnicas de optimización.
"""

import logging
import json
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import List, Dict, Any, Tuple, Optional
from dataclasses import dataclass, asdict
from pathlib import Path
import random
from copy import deepcopy
import joblib
import psutil
import re

# Scikit-learn imports
from sklearn.model_selection import (
    train_test_split, cross_val_score, RandomizedSearchCV,
    StratifiedKFold, TimeSeriesSplit
)
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from sklearn.ensemble import (
    RandomForestClassifier, GradientBoostingClassifier,
    ExtraTreesClassifier, VotingClassifier
)
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, balanced_accuracy_score, matthews_corrcoef,
    classification_report, confusion_matrix
)

# Imbalanced-learn imports
try:
    from imblearn.over_sampling import SMOTE, ADASYN
    from imblearn.under_sampling import RandomUnderSampler, EditedNearestNeighbours
    from imblearn.pipeline import Pipeline as ImbPipeline
    IMBALANCED_LEARN_AVAILABLE = True
except ImportError:
    IMBALANCED_LEARN_AVAILABLE = False
    logging.warning("imbalanced-learn no disponible. Instalar con: pip install imbalanced-learn")

# Imports locales
from .ml_predictive_analysis import SecurityEvent, MLPredictiveAnalyzer
from .advanced_ml_engine import AdvancedSecurityEvent, AdvancedMLEngine
from .config.ml_config import MLConfig

logger = logging.getLogger(__name__)

@dataclass
class TrainingQualityMetrics:
    """Métricas de calidad de datos de entrenamiento"""
    completeness: float
    consistency: float
    diversity: float
    balance: float
    temporal_coverage: float
    feature_correlation: float
    overall_score: float
    sample_count: int
    malicious_ratio: float

@dataclass
class OptimizedTrainingConfig:
    """Configuración para entrenamiento optimizado"""
    augmentation_factor: int = 2
    cv_folds: int = 5
    optimize_hyperparams: bool = True
    feature_selection: bool = True
    ensemble_methods: bool = True
    balance_classes: bool = True
    temporal_validation: bool = True
    min_samples_required: int = 100
    test_size: float = 0.2
    random_state: int = 42

@dataclass
class DataQualityMetrics:
    """Métricas de calidad de datos de entrenamiento"""
    completeness: float
    consistency: float
    diversity: float
    balance: float
    temporal_coverage: float
    overall_score: float
    sample_count: int
    malicious_ratio: float

def assess_training_data_quality(events: List[Dict[str, Any]]) -> DataQualityMetrics:
    """Evalúa la calidad de los datos de entrenamiento"""
    import re
    
    if not events:
        return DataQualityMetrics(
            completeness=0.0, consistency=0.0, diversity=0.0,
            balance=0.0, temporal_coverage=0.0, overall_score=0.0,
            sample_count=0, malicious_ratio=0.0
        )
    
    # Evaluar completitud (campos requeridos presentes)
    required_fields = ['timestamp', 'source_ip', 'target_domain', 'threat_score', 'is_malicious']
    completeness_scores = []
    
    for event in events:
        present_fields = sum(1 for field in required_fields if field in event and event[field] is not None)
        completeness_scores.append(present_fields / len(required_fields))
    
    completeness = np.mean(completeness_scores)
    
    # Evaluar consistencia (valores en rangos esperados)
    consistency_scores = []
    for event in events:
        score = 1.0
        
        # Verificar threat_score en rango [0, 1]
        if 'threat_score' in event:
            threat_score = event['threat_score']
            if not (0 <= threat_score <= 1):
                score -= 0.2
        
        # Verificar formato de IP
        if 'source_ip' in event:
            ip = event['source_ip']
            if not re.match(r'^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}$', str(ip)):
                score -= 0.2
        
        # Verificar formato de dominio
        if 'target_domain' in event:
            domain = event['target_domain']
            if not re.match(r'^[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$', str(domain)):
                score -= 0.2
        
        consistency_scores.append(max(0, score))
    
    consistency = np.mean(consistency_scores)
    
    # Evaluar diversidad (variedad de dominios, IPs, etc.)
    unique_domains = len(set(event.get('target_domain', '') for event in events))
    unique_ips = len(set(event.get('source_ip', '') for event in events))
    
    diversity = min(1.0, (unique_domains + unique_ips) / (len(events) * 0.5))
    
    # Evaluar balance de clases
    malicious_count = sum(1 for event in events if event.get('is_malicious', False))
    benign_count = len(events) - malicious_count
    
    if malicious_count == 0 or benign_count == 0:
        balance = 0.0
        malicious_ratio = malicious_count / len(events)
    else:
        # Balance ideal es 50/50, calcular qué tan cerca estamos
        ratio = min(malicious_count, benign_count) / max(malicious_count, benign_count)
        balance = ratio
        malicious_ratio = malicious_count / len(events)
    
    # Evaluar cobertura temporal
    timestamps = []
    for event in events:
        if 'timestamp' in event:
            try:
                if isinstance(event['timestamp'], str):
                    ts = datetime.fromisoformat(event['timestamp'].replace('Z', '+00:00'))
                else:
                    ts = event['timestamp']
                timestamps.append(ts)
            except:
                continue
    
    if len(timestamps) < 2:
        temporal_coverage = 0.0
    else:
        time_span = (max(timestamps) - min(timestamps)).total_seconds()
        # Normalizar a días (cobertura ideal: al menos 7 días)
        days_coverage = time_span / (24 * 3600)
        temporal_coverage = min(1.0, days_coverage / 7.0)
    
    # Calcular puntuación general
    overall_score = np.mean([
        completeness * 0.25,
        consistency * 0.25,
        diversity * 0.20,
        balance * 0.20,
        temporal_coverage * 0.10
    ])
    
    return DataQualityMetrics(
        completeness=completeness,
        consistency=consistency,
        diversity=diversity,
        balance=balance,
        temporal_coverage=temporal_coverage,
        overall_score=overall_score,
        sample_count=len(events),
        malicious_ratio=malicious_ratio
    )

@dataclass
class TrainingResults:
    """Resultados del entrenamiento optimizado"""
    models: Dict[str, Any]
    metrics: Dict[str, float]
    feature_importance: Dict[str, float]
    training_time: float
    data_quality: TrainingQualityMetrics
    config_used: OptimizedTrainingConfig
    timestamp: datetime

class DataQualityAssessor:
    """Evaluador de calidad de datos de entrenamiento"""
    
    def __init__(self):
        self.quality_thresholds = {
            'completeness': 0.8,
            'consistency': 0.7,
            'diversity': 0.6,
            'balance': 0.3,  # Mínimo 30% de la clase minoritaria
            'temporal_coverage': 0.5
        }
    
    def assess_data_quality(self, events: List[SecurityEvent]) -> TrainingQualityMetrics:
        """Evalúa la calidad de los datos de entrenamiento"""
        logger.info(f"Evaluando calidad de {len(events)} eventos")
        
        # Calcular métricas individuales
        completeness = self._calculate_completeness(events)
        consistency = self._calculate_consistency(events)
        diversity = self._calculate_diversity(events)
        balance = self._calculate_class_balance(events)
        temporal_coverage = self._calculate_temporal_coverage(events)
        feature_correlation = self._calculate_feature_correlation(events)
        
        # Calcular puntuación general
        metrics_values = [completeness, consistency, diversity, balance, temporal_coverage]
        overall_score = np.mean(metrics_values)
        
        # Estadísticas adicionales
        malicious_count = sum(1 for e in events if e.is_malicious)
        malicious_ratio = malicious_count / len(events) if events else 0
        
        return TrainingQualityMetrics(
            completeness=completeness,
            consistency=consistency,
            diversity=diversity,
            balance=balance,
            temporal_coverage=temporal_coverage,
            feature_correlation=feature_correlation,
            overall_score=overall_score,
            sample_count=len(events),
            malicious_ratio=malicious_ratio
        )
    
    def _calculate_completeness(self, events: List[SecurityEvent]) -> float:
        """Calcula completitud de datos"""
        if not events:
            return 0.0
        
        required_fields = ['target_domain', 'timestamp', 'event_type']
        complete_events = 0
        
        for event in events:
            if all(getattr(event, field, None) for field in required_fields):
                complete_events += 1
        
        return complete_events / len(events)
    
    def _calculate_consistency(self, events: List[SecurityEvent]) -> float:
        """Calcula consistencia de datos"""
        if not events:
            return 0.0
        
        consistent_events = 0
        
        for event in events:
            is_consistent = True
            
            # Validar rangos de valores
            if hasattr(event, 'response_time') and event.response_time:
                if event.response_time < 0 or event.response_time > 60:
                    is_consistent = False
            
            # Validar formato de dominio
            if event.target_domain and not self._is_valid_domain(event.target_domain):
                is_consistent = False
            
            # Validar coherencia temporal
            if event.timestamp and event.timestamp > datetime.now():
                is_consistent = False
            
            if is_consistent:
                consistent_events += 1
        
        return consistent_events / len(events)
    
    def _calculate_diversity(self, events: List[SecurityEvent]) -> float:
        """Calcula diversidad de datos"""
        if not events:
            return 0.0
        
        # Diversidad de dominios
        unique_domains = len(set(e.target_domain for e in events if e.target_domain))
        domain_diversity = min(unique_domains / len(events), 1.0)
        
        # Diversidad de tipos de evento
        unique_types = len(set(e.event_type for e in events if e.event_type))
        type_diversity = min(unique_types / 10, 1.0)  # Normalizar a máximo 10 tipos
        
        # Diversidad temporal (distribución a lo largo del tiempo)
        temporal_diversity = self._calculate_temporal_diversity(events)
        
        return np.mean([domain_diversity, type_diversity, temporal_diversity])
    
    def _calculate_class_balance(self, events: List[SecurityEvent]) -> float:
        """Calcula balance de clases"""
        if not events:
            return 0.0
        
        malicious_count = sum(1 for e in events if e.is_malicious)
        benign_count = len(events) - malicious_count
        
        if malicious_count == 0 or benign_count == 0:
            return 0.0
        
        # Calcular ratio de la clase minoritaria
        minority_ratio = min(malicious_count, benign_count) / len(events)
        
        # Normalizar: 0.5 es balance perfecto, 0.0 es completamente desbalanceado
        return min(minority_ratio * 2, 1.0)
    
    def _calculate_temporal_coverage(self, events: List[SecurityEvent]) -> float:
        """Calcula cobertura temporal"""
        if not events:
            return 0.0
        
        timestamps = [e.timestamp for e in events if e.timestamp]
        if len(timestamps) < 2:
            return 0.0
        
        timestamps.sort()
        time_span = (timestamps[-1] - timestamps[0]).total_seconds()
        
        # Normalizar: 30 días = 1.0, menos tiempo = proporcionalmente menor
        thirty_days = 30 * 24 * 3600
        return min(time_span / thirty_days, 1.0)
    
    def _calculate_feature_correlation(self, events: List[SecurityEvent]) -> float:
        """Calcula correlación de características"""
        # Implementación simplificada
        # En una implementación completa, se calcularía la correlación entre características
        return 0.7  # Valor por defecto
    
    def _calculate_temporal_diversity(self, events: List[SecurityEvent]) -> float:
        """Calcula diversidad temporal"""
        timestamps = [e.timestamp for e in events if e.timestamp]
        if len(timestamps) < 2:
            return 0.0
        
        # Calcular distribución por horas del día
        hours = [t.hour for t in timestamps]
        unique_hours = len(set(hours))
        
        return unique_hours / 24  # Normalizar a 24 horas
    
    def _is_valid_domain(self, domain: str) -> bool:
        """Valida formato de dominio"""
        if not domain or len(domain) < 3:
            return False
        
        # Validaciones básicas
        if domain.startswith('.') or domain.endswith('.'):
            return False
        
        if '..' in domain:
            return False
        
        return True

class SecurityDataAugmenter:
    """Augmentador de datos de seguridad"""
    
    def __init__(self):
        self.related_ports = {
            80: [8080, 8000, 3000, 8888],
            443: [8443, 9443, 8080],
            22: [2222, 2200, 22000],
            21: [2121, 21000],
            3306: [3307, 3308, 33060],
            5432: [5433, 54320],
            6379: [6380, 16379],
            27017: [27018, 27019]
        }
        
        self.technology_families = {
            'apache': ['nginx', 'lighttpd', 'iis'],
            'mysql': ['postgresql', 'mariadb', 'sqlite'],
            'php': ['python', 'nodejs', 'ruby'],
            'wordpress': ['drupal', 'joomla', 'magento']
        }
    
    def smart_augmentation(self, events: List[SecurityEvent], 
                          augmentation_factor: int = 2) -> List[SecurityEvent]:
        """Augmentación inteligente de datos de seguridad"""
        logger.info(f"Augmentando {len(events)} eventos con factor {augmentation_factor}")
        
        augmented_events = list(events)  # Mantener originales
        
        for event in events:
            for _ in range(augmentation_factor):
                augmented_event = self._create_augmented_event(event)
                augmented_events.append(augmented_event)
        
        logger.info(f"Augmentación completada: {len(augmented_events)} eventos totales")
        return augmented_events
    
    def _create_augmented_event(self, original_event: SecurityEvent) -> SecurityEvent:
        """Crea una variación del evento original"""
        augmented = deepcopy(original_event)
        
        # Variaciones temporales
        if augmented.timestamp:
            time_delta = random.randint(-3600, 3600)  # ±1 hora
            augmented.timestamp += timedelta(seconds=time_delta)
        
        # Variaciones de respuesta
        if hasattr(augmented, 'response_time') and augmented.response_time:
            augmented.response_time *= random.uniform(0.8, 1.2)
        
        # Variaciones de puertos
        if hasattr(augmented, 'ports_open') and augmented.ports_open:
            augmented.ports_open = self._vary_ports(augmented.ports_open)
        
        # Variaciones de tecnologías
        if hasattr(augmented, 'technologies') and augmented.technologies:
            augmented.technologies = self._vary_technologies(augmented.technologies)
        
        # Variaciones de dominio (sutiles)
        if augmented.target_domain:
            augmented.target_domain = self._vary_domain(augmented.target_domain)
        
        # Nuevo ID para el evento augmentado
        augmented.event_id = f"{original_event.event_id}_aug_{random.randint(1000, 9999)}"
        
        return augmented
    
    def _vary_ports(self, original_ports: List[int]) -> List[int]:
        """Varía puertos manteniendo coherencia"""
        varied_ports = list(original_ports)
        
        for port in original_ports:
            if port in self.related_ports and random.random() < 0.3:
                related = random.choice(self.related_ports[port])
                if related not in varied_ports:
                    varied_ports.append(related)
        
        # Ocasionalmente remover un puerto
        if len(varied_ports) > 1 and random.random() < 0.2:
            varied_ports.pop(random.randint(0, len(varied_ports) - 1))
        
        return varied_ports
    
    def _vary_technologies(self, original_techs: List[str]) -> List[str]:
        """Varía tecnologías manteniendo coherencia"""
        varied_techs = list(original_techs)
        
        for tech in original_techs:
            tech_lower = tech.lower()
            for family_key, alternatives in self.technology_families.items():
                if family_key in tech_lower and random.random() < 0.2:
                    alternative = random.choice(alternatives)
                    if alternative not in [t.lower() for t in varied_techs]:
                        varied_techs.append(alternative.title())
        
        return varied_techs
    
    def _vary_domain(self, original_domain: str) -> str:
        """Varía dominio sutilmente"""
        # Variaciones muy sutiles para mantener realismo
        if random.random() < 0.1:  # Solo 10% de probabilidad
            parts = original_domain.split('.')
            if len(parts) > 2:
                # Cambiar subdominio
                subdomain_variations = ['www', 'api', 'app', 'secure', 'portal']
                parts[0] = random.choice(subdomain_variations)
                return '.'.join(parts)
        
        return original_domain

class HyperparameterOptimizer:
    """Optimizador de hiperparámetros"""
    
    def __init__(self, config: OptimizedTrainingConfig):
        self.config = config
        self.param_distributions = self._get_param_distributions()
    
    def optimize_multiple_models(self, X: np.ndarray, y: np.ndarray) -> Dict[str, Any]:
        """Optimiza múltiples modelos y retorna los mejores"""
        logger.info("Iniciando optimización de hiperparámetros")
        
        models_to_optimize = {
            'random_forest': RandomForestClassifier(random_state=self.config.random_state),
            'gradient_boosting': GradientBoostingClassifier(random_state=self.config.random_state),
            'extra_trees': ExtraTreesClassifier(random_state=self.config.random_state)
        }
        
        optimized_models = {}
        
        for model_name, base_model in models_to_optimize.items():
            logger.info(f"Optimizando {model_name}")
            
            param_dist = self.param_distributions[model_name]
            
            search = RandomizedSearchCV(
                base_model,
                param_dist,
                n_iter=30,  # Reducido para velocidad
                cv=self.config.cv_folds,
                scoring='f1',
                n_jobs=-1,
                random_state=self.config.random_state,
                verbose=0
            )
            
            search.fit(X, y)
            
            optimized_models[model_name] = {
                'model': search.best_estimator_,
                'best_params': search.best_params_,
                'best_score': search.best_score_
            }
            
            logger.info(f"{model_name} optimizado - Score: {search.best_score_:.3f}")
        
        return optimized_models
    
    def _get_param_distributions(self) -> Dict[str, Dict]:
        """Define distribuciones de parámetros para optimización"""
        from scipy.stats import randint, uniform
        
        return {
            'random_forest': {
                'n_estimators': randint(100, 300),
                'max_depth': randint(5, 20),
                'min_samples_split': randint(2, 20),
                'min_samples_leaf': randint(1, 10),
                'max_features': ['sqrt', 'log2', None],
                'class_weight': ['balanced', None]
            },
            'gradient_boosting': {
                'n_estimators': randint(50, 200),
                'learning_rate': uniform(0.01, 0.2),
                'max_depth': randint(3, 10),
                'subsample': uniform(0.6, 0.4),
                'min_samples_split': randint(2, 20)
            },
            'extra_trees': {
                'n_estimators': randint(100, 300),
                'max_depth': randint(5, 20),
                'min_samples_split': randint(2, 20),
                'min_samples_leaf': randint(1, 10),
                'max_features': ['sqrt', 'log2', None],
                'class_weight': ['balanced', None]
            }
        }

class OptimizedMLTrainer:
    """Entrenador ML optimizado con mejores prácticas"""
    
    def __init__(self, config: OptimizedTrainingConfig = None):
        self.config = config or OptimizedTrainingConfig()
        self.quality_assessor = DataQualityAssessor()
        self.data_augmenter = SecurityDataAugmenter()
        self.hyperparameter_optimizer = HyperparameterOptimizer(self.config)
        
        # Métricas de entrenamiento
        self.training_log = []
        self.memory_usage = []
    
    def train_with_best_practices(self, events: List[SecurityEvent]) -> TrainingResults:
        """Entrena modelos siguiendo mejores prácticas"""
        start_time = datetime.now()
        
        try:
            logger.info(f"Iniciando entrenamiento optimizado con {len(events)} eventos")
            
            # Paso 1: Validación inicial
            self._log_step("Validando datos iniciales")
            self._validate_training_data(events)
            
            # Paso 2: Evaluación de calidad
            self._log_step("Evaluando calidad de datos")
            data_quality = self.quality_assessor.assess_data_quality(events)
            logger.info(f"Calidad de datos: {data_quality.overall_score:.3f}")
            
            # Paso 3: Limpieza de datos
            self._log_step("Limpiando datos")
            clean_events = self._clean_training_data(events)
            
            # Paso 4: Extracción de características
            self._log_step("Extrayendo características")
            X, y = self._extract_features(clean_events)
            
            # Paso 5: Augmentación de datos
            if self.config.augmentation_factor > 0:
                self._log_step("Augmentando datos")
                augmented_events = self.data_augmenter.smart_augmentation(
                    clean_events, self.config.augmentation_factor
                )
                X, y = self._extract_features(augmented_events)
            
            # Paso 6: Balanceo de clases
            if self.config.balance_classes and IMBALANCED_LEARN_AVAILABLE:
                self._log_step("Balanceando clases")
                X, y = self._balance_classes(X, y)
            
            # Paso 7: División de datos
            self._log_step("Dividiendo datos")
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=self.config.test_size,
                random_state=self.config.random_state,
                stratify=y
            )
            
            # Paso 8: Selección de características
            if self.config.feature_selection:
                self._log_step("Seleccionando características")
                X_train, X_test = self._select_features(X_train, X_test, y_train)
            
            # Paso 9: Escalado
            self._log_step("Escalando características")
            scaler = RobustScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            # Paso 10: Optimización de modelos
            if self.config.optimize_hyperparams:
                self._log_step("Optimizando hiperparámetros")
                optimized_models = self.hyperparameter_optimizer.optimize_multiple_models(
                    X_train_scaled, y_train
                )
            else:
                optimized_models = self._get_default_models()
            
            # Paso 11: Entrenamiento final
            self._log_step("Entrenando modelos finales")
            final_models = self._train_final_models(
                optimized_models, X_train_scaled, y_train
            )
            
            # Paso 12: Evaluación
            self._log_step("Evaluando modelos")
            evaluation_metrics = self._evaluate_models(
                final_models, X_test_scaled, y_test
            )
            
            # Paso 13: Crear ensemble si está habilitado
            if self.config.ensemble_methods:
                self._log_step("Creando ensemble")
                ensemble_model = self._create_ensemble(final_models)
                final_models['ensemble'] = ensemble_model
                
                # Evaluar ensemble
                ensemble_metrics = self._evaluate_single_model(
                    ensemble_model, X_test_scaled, y_test
                )
                evaluation_metrics['ensemble'] = ensemble_metrics
            
            # Calcular importancia de características
            feature_importance = self._calculate_feature_importance(final_models)
            
            training_time = (datetime.now() - start_time).total_seconds()
            
            results = TrainingResults(
                models=final_models,
                metrics=evaluation_metrics,
                feature_importance=feature_importance,
                training_time=training_time,
                data_quality=data_quality,
                config_used=self.config,
                timestamp=datetime.now()
            )
            
            logger.info(f"Entrenamiento completado en {training_time:.2f} segundos")
            return results
            
        except Exception as e:
            logger.error(f"Error en entrenamiento optimizado: {e}")
            raise
    
    def _log_step(self, step_name: str):
        """Registra paso del entrenamiento"""
        memory_mb = psutil.Process().memory_info().rss / 1024 / 1024
        
        step_info = {
            'step': step_name,
            'timestamp': datetime.now(),
            'memory_mb': memory_mb
        }
        
        self.training_log.append(step_info)
        self.memory_usage.append(memory_mb)
        
        logger.info(f"Entrenamiento: {step_name} (Memoria: {memory_mb:.1f} MB)")
    
    def _validate_training_data(self, events: List[SecurityEvent]):
        """Valida datos de entrenamiento"""
        if len(events) < self.config.min_samples_required:
            raise ValueError(
                f"Se requieren al menos {self.config.min_samples_required} eventos, "
                f"recibidos: {len(events)}"
            )
        
        # Verificar que hay eventos maliciosos y benignos
        malicious_count = sum(1 for e in events if e.is_malicious)
        benign_count = len(events) - malicious_count
        
        if malicious_count == 0 or benign_count == 0:
            raise ValueError("Se requieren eventos tanto maliciosos como benignos")
    
    def _clean_training_data(self, events: List[SecurityEvent]) -> List[SecurityEvent]:
        """Limpia datos de entrenamiento"""
        cleaned_events = []
        
        for event in events:
            # Validar campos requeridos
            if not event.target_domain or not event.timestamp:
                continue
            
            # Normalizar dominio
            event.target_domain = event.target_domain.lower().strip()
            
            # Validar y corregir response_time
            if hasattr(event, 'response_time') and event.response_time:
                event.response_time = max(0, min(event.response_time, 60))
            
            # Filtrar duplicados (simplificado)
            if not self._is_duplicate_event(event, cleaned_events):
                cleaned_events.append(event)
        
        logger.info(f"Datos limpiados: {len(cleaned_events)} de {len(events)} eventos")
        return cleaned_events
    
    def _is_duplicate_event(self, event: SecurityEvent, 
                           existing_events: List[SecurityEvent]) -> bool:
        """Detecta eventos duplicados (implementación simplificada)"""
        for existing in existing_events[-100:]:  # Solo verificar últimos 100
            if (event.target_domain == existing.target_domain and
                event.event_type == existing.event_type and
                abs((event.timestamp - existing.timestamp).total_seconds()) < 60):
                return True
        return False
    
    def _extract_features(self, events: List[SecurityEvent]) -> Tuple[np.ndarray, np.ndarray]:
        """Extrae características de eventos"""
        # Usar el extractor de características existente
        analyzer = MLPredictiveAnalyzer()
        
        features = []
        labels = []
        
        for event in events:
            feature_vector = analyzer.extract_features(event).flatten()
            features.append(feature_vector)
            labels.append(1 if event.is_malicious else 0)
        
        return np.array(features), np.array(labels)
    
    def _balance_classes(self, X: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """Balancea clases usando SMOTE"""
        try:
            # Usar SMOTE para over-sampling
            smote = SMOTE(random_state=self.config.random_state)
            X_balanced, y_balanced = smote.fit_resample(X, y)
            
            logger.info(f"Clases balanceadas: {len(X)} -> {len(X_balanced)} muestras")
            return X_balanced, y_balanced
            
        except Exception as e:
            logger.warning(f"Error en balanceo de clases: {e}. Usando datos originales.")
            return X, y
    
    def _select_features(self, X_train: np.ndarray, X_test: np.ndarray, 
                        y_train: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """Selecciona características más importantes"""
        # Usar SelectKBest para selección de características
        k = min(30, X_train.shape[1])  # Máximo 30 características
        selector = SelectKBest(f_classif, k=k)
        
        X_train_selected = selector.fit_transform(X_train, y_train)
        X_test_selected = selector.transform(X_test)
        
        logger.info(f"Características seleccionadas: {k} de {X_train.shape[1]}")
        return X_train_selected, X_test_selected
    
    def _get_default_models(self) -> Dict[str, Dict]:
        """Retorna modelos con parámetros por defecto"""
        return {
            'random_forest': {
                'model': RandomForestClassifier(
                    n_estimators=200, max_depth=15, random_state=self.config.random_state,
                    class_weight='balanced', n_jobs=-1
                ),
                'best_params': {},
                'best_score': 0.0
            },
            'gradient_boosting': {
                'model': GradientBoostingClassifier(
                    n_estimators=100, learning_rate=0.1, max_depth=6,
                    random_state=self.config.random_state
                ),
                'best_params': {},
                'best_score': 0.0
            }
        }
    
    def _train_final_models(self, optimized_models: Dict, 
                           X_train: np.ndarray, y_train: np.ndarray) -> Dict[str, Any]:
        """Entrena modelos finales"""
        final_models = {}
        
        for model_name, model_info in optimized_models.items():
            model = model_info['model']
            model.fit(X_train, y_train)
            final_models[model_name] = model
            
            logger.info(f"Modelo {model_name} entrenado")
        
        return final_models
    
    def _evaluate_models(self, models: Dict[str, Any], 
                        X_test: np.ndarray, y_test: np.ndarray) -> Dict[str, Dict]:
        """Evalúa todos los modelos"""
        evaluation_results = {}
        
        for model_name, model in models.items():
            metrics = self._evaluate_single_model(model, X_test, y_test)
            evaluation_results[model_name] = metrics
            
            logger.info(
                f"{model_name} - Accuracy: {metrics['accuracy']:.3f}, "
                f"F1: {metrics['f1_score']:.3f}, AUC: {metrics['auc_roc']:.3f}"
            )
        
        return evaluation_results
    
    def _evaluate_single_model(self, model: Any, 
                              X_test: np.ndarray, y_test: np.ndarray) -> Dict[str, float]:
        """Evalúa un modelo individual"""
        y_pred = model.predict(X_test)
        y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None
        
        metrics = {
            'accuracy': accuracy_score(y_test, y_pred),
            'precision': precision_score(y_test, y_pred, zero_division=0),
            'recall': recall_score(y_test, y_pred, zero_division=0),
            'f1_score': f1_score(y_test, y_pred, zero_division=0),
            'balanced_accuracy': balanced_accuracy_score(y_test, y_pred),
            'matthews_corrcoef': matthews_corrcoef(y_test, y_pred)
        }
        
        if y_pred_proba is not None:
            metrics['auc_roc'] = roc_auc_score(y_test, y_pred_proba)
        
        return metrics
    
    def _create_ensemble(self, models: Dict[str, Any]) -> VotingClassifier:
        """Crea ensemble de modelos"""
        estimators = [(name, model) for name, model in models.items()]
        
        ensemble = VotingClassifier(
            estimators=estimators,
            voting='soft'
        )
        
        return ensemble
    
    def _calculate_feature_importance(self, models: Dict[str, Any]) -> Dict[str, float]:
        """Calcula importancia promedio de características"""
        feature_importances = {}
        
        for model_name, model in models.items():
            if hasattr(model, 'feature_importances_'):
                importances = model.feature_importances_
                for i, importance in enumerate(importances):
                    feature_name = f"feature_{i}"
                    if feature_name not in feature_importances:
                        feature_importances[feature_name] = []
                    feature_importances[feature_name].append(importance)
        
        # Promediar importancias
        avg_importances = {}
        for feature, importances in feature_importances.items():
            avg_importances[feature] = np.mean(importances)
        
        return avg_importances

# Función de utilidad para entrenamiento optimizado
def train_optimized_models(events: List[SecurityEvent], 
                          config: OptimizedTrainingConfig = None) -> TrainingResults:
    """Función de conveniencia para entrenamiento optimizado"""
    trainer = OptimizedMLTrainer(config)
    return trainer.train_with_best_practices(events)

# Función para evaluar calidad de datos
def assess_training_data_quality(events: List[SecurityEvent]) -> TrainingQualityMetrics:
    """Función de conveniencia para evaluar calidad de datos"""
    assessor = DataQualityAssessor()
    return assessor.assess_data_quality(events)