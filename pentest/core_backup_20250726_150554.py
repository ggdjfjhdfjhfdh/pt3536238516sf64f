"""Pipeline principal del escáner de seguridad con notificaciones de progreso."""

from __future__ import annotations
import json
from .enhanced_integration import EnhancedTechIntegrator
import logging
import os
import shutil
import sys
import tempfile
import datetime as dt
import asyncio
import redis
from pathlib import Path
from typing import Dict, Optional, Callable, List, Any, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed, Future
from dataclasses import dataclass, field
from enum import Enum
from contextlib import contextmanager
from rq import Queue, Worker


# --- módulos de tu proyecto -----------------------------------------------
from pentest.config import REDIS_URL, SAFE_DOMAIN, DEFAULT_TIMEOUT
from pentest.exceptions import ScanError
from pentest.recon import recon
from pentest.fingerprint import fingerprint
from pentest.nuclei_scan import nuclei_scan
from pentest.leaks import check_leaks
from pentest.typosquat import check_typosquats
from pentest.cve_scan import cve_scan
from pentest.nmap_scan import nmap_scan
from pentest.security_config import security_config_scan
from pentest.dir_brute import dir_brute_scan
from pentest.threat_intel import check_threat_feeds

from pentest.report import build_pdf, send_notification
from pentest.cisa_kev import cisa_kev_monitor
from pentest.greynoise import is_ip_malicious_greynoise

# ---------------------------------------------------------------------------
# CONFIGURACIÓN Y TIPOS
# ---------------------------------------------------------------------------

class ScanStage(Enum):
    """Estados posibles del escaneo."""
    QUEUED = "queued"
    WORKING = "working"
    COMPLETED = "completed"
    FAILED = "failed"

@dataclass
class ScanProgress:
    """Modelo para el progreso del escaneo."""
    job_id: str
    stage: ScanStage
    step: str
    percentage: int
    error: Optional[str] = None
    extra_data: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        """Convierte el progreso a diccionario para serialización."""
        return {
            "state": self.stage.value,
            "step": self.step,
            "pct": self.percentage,
            "error": self.error,
            **self.extra_data
        }

@dataclass
class ScanStep:
    """Modelo para un paso del escaneo."""
    key: str
    percentage: int
    runner: Callable
    dependencies: List[str] = field(default_factory=list)
    parallel: bool = True

@dataclass
class ScanResult:
    """Resultado de un escaneo completo."""
    success: bool
    message: str
    job_id: str
    report_path: Optional[str] = None
    error: Optional[str] = None
    metrics: Dict[str, int] = field(default_factory=dict)

# ---------------------------------------------------------------------------
# CONFIGURACIÓN DE LOGGING MEJORADA
# ---------------------------------------------------------------------------

class DeduplicateFilter(logging.Filter):
    """Filtro optimizado para deduplicar mensajes de log repetidos."""
    
    def __init__(self, name: str = '', interval: int = 5, max_entries: int = 1000):
        super().__init__(name)
        self.last_messages: Dict[Tuple[str, str, str], dt.datetime] = {}
        self.deduplication_interval = interval
        self.max_entries = max_entries

    def filter(self, record: logging.LogRecord) -> bool:
        """Filtra mensajes duplicados con limpieza periódica."""
        message = record.getMessage()
        unique_key = (record.levelname, record.name, message)
        
        now = dt.datetime.now()
        
        # Limpieza periódica del cache
        if len(self.last_messages) > self.max_entries:
            cutoff = now - dt.timedelta(seconds=self.deduplication_interval * 2)
            self.last_messages = {
                k: v for k, v in self.last_messages.items() if v > cutoff
            }
        
        if unique_key in self.last_messages:
            last_time = self.last_messages[unique_key]
            if (now - last_time).total_seconds() < self.deduplication_interval:
                return False
        
        self.last_messages[unique_key] = now
        return True

def setup_logging():
    """Configuración optimizada del sistema de logging."""
    logging.basicConfig(
        stream=sys.stdout,
        level=logging.DEBUG,  # Cambiar a DEBUG para obtener más detalles
        format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S"
    )
    
    # Aplicar filtro de deduplicación (desactivado temporalmente para depuración)
    # dedupe_filter = DeduplicateFilter()
    # for handler in logging.root.handlers:
    #     handler.addFilter(dedupe_filter)

# ---------------------------------------------------------------------------
# GESTIÓN DE REDIS MEJORADA
# ---------------------------------------------------------------------------

def get_redis_connection():
    return redis.Redis.from_url(REDIS_URL)

class RedisManager:
    """Gestor optimizado para operaciones Redis con pool de conexiones."""
    
    def __init__(self, url: str):
        self.pool = redis.ConnectionPool.from_url(
            url, 
            max_connections=20,
            retry_on_timeout=True,
            socket_keepalive=True,
            socket_keepalive_options={}
        )
        self._redis = redis.Redis(connection_pool=self.pool)
    
    def publish_progress(self, progress: ScanProgress) -> None:
        """Publica progreso con manejo de errores mejorado."""
        try:
            payload = json.dumps(progress.to_dict())
            pipe = self._redis.pipeline()
            pipe.publish(progress.job_id, payload)
            pipe.hset(f"rq:job:{progress.job_id}", "meta", 
                     json.dumps({"progress": progress.to_dict()}))
            pipe.execute()
        except redis.ConnectionError:
            logging.getLogger(__name__).warning(
                f"Redis connection failed for job {progress.job_id}"
            )
        except Exception as e:
            logging.getLogger(__name__).error(
                f"Unexpected Redis error for job {progress.job_id}: {e}"
            )
    
    def get_job_progress(self, job_id: str) -> Optional[Dict]:
        """Obtiene el progreso de un trabajo."""
        try:
            meta = self._redis.hget(f"rq:job:{job_id}", "meta")
            if meta:
                return json.loads(meta).get("progress")
        except Exception:
            pass
        return None
    
    def blpop(self, key: str, timeout: int = 0) -> Tuple[str, str]:
        """Operación BLPOP con manejo mejorado."""
        return self._redis.blpop(key, timeout)

# ---------------------------------------------------------------------------
# PIPELINE DE ESCANEO OPTIMIZADO
# ---------------------------------------------------------------------------

class ScanPipeline:
    """Pipeline optimizado para escaneos de seguridad."""
    
    def __init__(self):
        self.redis_manager = RedisManager(REDIS_URL)
        self.logger = logging.getLogger(self.__class__.__name__)
        self._setup_steps()
        self.logger.info("🚀 [PIPELINE] ScanPipeline inicializado")
    
    def _setup_steps(self):
        """Configuración de pasos con dependencias explícitas."""
        self.steps = {
            "recon": ScanStep(
                key="recon", 
                percentage=10, 
                runner=recon,
                parallel=False  # Debe ejecutarse primero
            ),
            "finger": ScanStep(
                key="finger", 
                percentage=30, 
                runner=fingerprint,
                dependencies=["recon"],
                parallel=False  # Depende de recon
            ),
            "nuclei": ScanStep(
                key="nuclei", 
                percentage=55, 
                runner=nuclei_scan,
                dependencies=["finger"]
            ),
            "leaks": ScanStep(
                key="leaks", 
                percentage=85, 
                runner=check_leaks
            ),
            "typos": ScanStep(
                key="typos", 
                percentage=90, 
                runner=check_typosquats
            ),
            "cve": ScanStep(
                key="cve", 
                percentage=95, 
                runner=cve_scan,
                dependencies=["finger"]
            ),
            "nmap": ScanStep(
                key="nmap", 
                percentage=96, 
                runner=nmap_scan,
                dependencies=["finger"]
            ),
            "cisa_kev": ScanStep(
                key="cisa_kev", 
                percentage=97, 
                runner=cisa_kev_monitor
            ),
            "greynoise": ScanStep(
                key="greynoise", 
                percentage=98, 
                runner=is_ip_malicious_greynoise
            ),
            "security_config": ScanStep(
                key="security_config", 
                percentage=98.5, 
                runner=security_config_scan,
                dependencies=["finger"]
            ),
            "dir_brute": ScanStep(
                key="dir_brute", 
                percentage=99, 
                runner=dir_brute_scan,
                dependencies=["finger"]
            ),

        }



    @contextmanager
    def _temp_directory(self, domain: str):
        """Context manager para manejo seguro de directorios temporales."""
        tmp_dir = Path(tempfile.mkdtemp(prefix=f"scan_{domain}_"))
        self.logger.info(f"Created temporary directory: {tmp_dir}")
        try:
            yield tmp_dir
        finally:
            if tmp_dir.exists():
                shutil.rmtree(tmp_dir)
                self.logger.info(f"Cleaned up temporary directory: {tmp_dir}")

    def _validate_domain(self, domain: str) -> None:
        """Validación mejorada del dominio."""
        if not domain or not isinstance(domain, str):
            raise ScanError("Dominio debe ser una cadena no vacía")
        
        if not SAFE_DOMAIN.match(domain):
            raise ScanError(f"Dominio inválido: {domain}")

    def _update_progress(self, job_id: str, step_key: str, percentage: int, 
                        stage: ScanStage = ScanStage.WORKING, 
                        error: str = None, **extra) -> None:
        """Actualiza el progreso del escaneo."""
        progress = ScanProgress(
            job_id=job_id,
            stage=stage,
            step=step_key,
            percentage=percentage,
            error=error,
            extra_data=extra
        )
        self.redis_manager.publish_progress(progress)

    def _execute_step(self, step: ScanStep, job_id: str, domain: str, 
                     tmp_dir: Path, files: Dict[str, Path], 
                     **kwargs) -> Tuple[str, Any]:
        """Ejecuta un paso individual del pipeline."""
        self._update_progress(job_id, step.key, step.percentage)
        self.logger.info(f"🚀 [STEP] [{domain}] Ejecutando paso: {step.key} ({step.percentage}%)")
        
        # Configurar timeout específico por paso
        step_timeout = self._get_step_timeout(step.key)
        self.logger.info(f"🚀 [STEP] [{domain}] Timeout configurado para {step.key}: {step_timeout}s")
        
        try:
            # Preparar argumentos según el paso
            args, step_kwargs = self._prepare_step_args(
                step.key, domain, tmp_dir, files, kwargs
            )
            
            self.logger.info(f"🚀 [STEP] [{domain}] Llamando a {step.runner.__name__} con args={args} y kwargs={step_kwargs}")
            
            # Ejecutar con timeout usando ThreadPoolExecutor
            with ThreadPoolExecutor(max_workers=1) as executor:
                if asyncio.iscoroutinefunction(step.runner):
                    # Ejecutar coroutine en un nuevo loop de eventos
                    self.logger.info(f"🚀 [STEP] [{domain}] Ejecutando función async: {step.runner.__name__}")
                    future = executor.submit(self._run_async_step, step.runner, args, step_kwargs)
                else:
                    self.logger.info(f"🚀 [STEP] [{domain}] Ejecutando función sync: {step.runner.__name__}")
                    future = executor.submit(step.runner, *args, **step_kwargs)
                
                try:
                    result = future.result(timeout=step_timeout)
                except TimeoutError:
                    self.logger.error(f"🚀 [STEP] [{domain}] ⏰ Timeout en {step.key} después de {step_timeout}s")
                    future.cancel()
                    raise ScanError(f"Timeout en paso {step.key} después de {step_timeout}s")
            
            self.logger.info(f"🚀 [STEP] [{domain}] ✅ Completado: {step.key}, resultado: {result}")
            
            # Verificar que el resultado es válido
            if result and isinstance(result, Path):
                if result.exists():
                    self.logger.info(f"🚀 [STEP] [{domain}] ✅ Archivo resultado verificado: {result} (tamaño: {result.stat().st_size} bytes)")
                else:
                    self.logger.warning(f"🚀 [STEP] [{domain}] ⚠️ Archivo resultado no existe: {result}")
            else:
                self.logger.info(f"🚀 [STEP] [{domain}] Resultado no es un archivo: {type(result)} - {result}")
            
            return step.key, result
            
        except Exception as e:
            error_msg = f"Error en {step.key}: {str(e)}"
            self.logger.error(f"🚀 [STEP] [{domain}] ❌ {error_msg}", exc_info=True)
            self._update_progress(job_id, step.key, step.percentage, stage=ScanStage.FAILED, error=error_msg)
            raise ScanError(error_msg) # Relanzar la excepción para detener el pipeline

    def _get_step_timeout(self, step_key: str) -> int:
        """Obtiene el timeout específico para cada paso del pipeline."""
        timeouts = {
            "recon": 300,      # 5 minutos para reconocimiento
            "finger": 180,     # 3 minutos para fingerprinting (puede ser bloqueado por Cloudflare)
            "nuclei": 600,     # 10 minutos para Nuclei
            "nmap": 900,       # 15 minutos para Nmap
            "leaks": 120,      # 2 minutos para HIBP
            "typos": 180,      # 3 minutos para typosquatting
            "dir_brute": 600,  # 10 minutos para directory brute force
            "cve": 300,        # 5 minutos para CVE
            "security_config": 180,  # 3 minutos para security config
            "greynoise": 60,   # 1 minuto para GreyNoise
            "cisa_kev": 120    # 2 minutos para CISA KEV
        }
        return timeouts.get(step_key, DEFAULT_TIMEOUT)

    def _run_async_step(self, async_func, args, kwargs):
        """Ejecuta una función asíncrona en un nuevo loop de eventos."""
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            return loop.run_until_complete(async_func(*args, **kwargs))
        finally:
            loop.close()

    def _prepare_step_args(self, step_key: str, domain: str, tmp_dir: Path, 
                          files: Dict[str, Path], kwargs: Dict) -> Tuple[tuple, Dict]:
        """Prepara argumentos específicos para cada paso."""
        args = []
        step_kwargs = {}
        
        if step_key == "recon":
            args = [domain, tmp_dir]
        elif step_key == "finger":
            args = [files.get("recon"), tmp_dir]
        elif step_key == "leaks":
            args = [domain, tmp_dir, kwargs.get("hibp_api_key")]
        elif step_key == "nuclei":
            finger_file = files.get("finger")
            if finger_file is None:
                return (), {}
            args = [finger_file, tmp_dir, kwargs.get("full_nuclei_scan", False)]
        elif step_key == "nmap":
            finger_file = files.get("finger")
            if finger_file is None:
                return (), {}
            args = [finger_file, tmp_dir, True]
        elif step_key == "typos":
            args = [domain, tmp_dir]
        elif step_key == "dir_brute":
            finger_file = files.get("finger")
            if finger_file is None:
                return (), {}
            args = [finger_file, tmp_dir]
            step_kwargs.update(self._get_dir_brute_config(finger_file))
        elif step_key in ["cve", "security_config"]: 
            finger_file = files.get("finger")
            if finger_file is None:
                return (), {} # Return empty args if finger_file is None
            args = [finger_file, tmp_dir]
        elif step_key == "greynoise":
            api_key = os.environ.get("GREYNOISE_API_KEY")
            if not api_key:
                self.logger.error("GREYNOISE_API_KEY no configurada. El paso greynoise no se ejecutará.")
                raise ScanError("GREYNOISE_API_KEY no configurada. Por favor, configura la variable de entorno.")
            args = [domain, tmp_dir, api_key]
            self.logger.debug(f"[{domain}] Argumentos preparados para greynoise: domain={domain}, tmp_dir={tmp_dir}, api_key_present={bool(api_key)}")
        elif step_key == "cisa_kev":
            tech_stack = self._get_dir_brute_config(files.get("finger")).get("tech_stack")
            # Ensure tech_stack is always a list, even if None is returned
            tech_stack_for_cisa = tech_stack if tech_stack is not None else []
            args = [domain, tmp_dir, tech_stack_for_cisa]
        
        return tuple(args), step_kwargs

    def _get_dir_brute_config(self, httpx_file: Optional[Path]) -> Dict:
        """Configuración optimizada para directory brute force."""
        config = {
            'extensions': ['php', 'html', 'js', 'asp', 'aspx', 'txt', 'zip', 'rar', 'tar.gz'],
            'auto_extend': True,
            'max_concurrent_requests': 50,  # Reducido para evitar sobrecarga
            'delay_between_requests': 0.01,
            'filter_status_codes': [301, 404],
            'filter_content_length': None,
            'tech_stack': None
        }
        
        # Detectar tech stack de los resultados de httpx
        if httpx_file and httpx_file.exists():
            try:
                with open(httpx_file, "r") as f:
                    httpx_results = json.load(f)
                    
                for host_result in httpx_results:
                    tech_list = host_result.get("tech", [])
                    for tech in tech_list:
                        tech_lower = tech.lower()
                        if "wordpress" in tech_lower:
                            config['tech_stack'] = "wordpress"
                            return config
                        elif "laravel" in tech_lower:
                            config['tech_stack'] = "laravel"
                            return config
                    
                    # Si no hay tecnología prioritaria, usar la primera
                    if tech_list and not config['tech_stack']:
                        config['tech_stack'] = tech_list[0]
                        
            except (json.JSONDecodeError, IOError) as e:
                self.logger.warning(f"Error leyendo httpx results: {e}")
        
        return config

    def _get_execution_order(self) -> List[List[str]]:
        """Determina el orden de ejecución basado en dependencias."""
        # Implementación simple por capas
        layers = [
            ["recon"],  # Capa 0: sin dependencias
            ["finger"],  # Capa 1: depende de recon
            # Capa 2: depende de finger y pueden ejecutarse en paralelo
            ["nuclei", "cve", "nmap", "security_config", "dir_brute", "cisa_kev", "greynoise"],
            # Capa 3: independientes
            ["leaks", "typos"]
        ]
        return layers

    def _safe_count_json(self, path: Optional[Path]) -> int:
        """Cuenta elementos en archivo JSON de forma segura."""
        if not path or not path.exists():
            return 0
        try:
            with open(path, "r") as f:
                data = json.load(f)
                if isinstance(data, list):
                    return len(data)
                elif isinstance(data, dict):
                    # Para GreyNoise, si hay datos, consideramos que hay 1 resultado
                    return 1 if data else 0
                return 0
        except Exception:
            return 0

    def _count_lines(self, path: Optional[Path]) -> int:
        """Cuenta líneas no vacías en un archivo."""
        if not path or not path.exists():
            return 0
        try:
            with open(path, "r") as f:
                return len([line for line in f if line.strip()])
        except Exception:
            return 0

    def _enhance_with_threat_intel(self, nmap_file: Path) -> None:
        """Enriquece resultados de Nmap con threat intelligence."""
        if not nmap_file.exists():
            return
        
        try:
            with open(nmap_file, "r") as f:
                nmap_results = json.load(f)
            
            updated_results = []
            for host_data in nmap_results:
                ip_address = host_data.get("ip_address")
                if ip_address:
                    try:
                        threat_hits = check_threat_feeds(ip_address)
                        if threat_hits:
                            host_data["threat_intel"] = threat_hits
                    except Exception as e:
                        self.logger.warning(f"Error checking threat intel for {ip_address}: {e}")
                
                updated_results.append(host_data)
            
            with open(nmap_file, "w") as f:
                json.dump(updated_results, f, indent=2)
            
            self.logger.info("Threat intelligence added to Nmap results")
            
        except Exception as e:
            self.logger.error(f"Error enhancing Nmap results with threat intel: {e}")

    def generate_pdf(self, domain: str, recipient_email: str, job_id: str = None, *, debug: bool = False, full_nuclei_scan: bool = False, hibp_api_key: str = None) -> ScanResult:
        self.logger.info(f"🚀 [PIPELINE] Iniciando escaneo completo para {domain}")
        self.logger.info(f"🚀 [PIPELINE] Dominio recibido (tipo: {type(domain).__name__}, longitud: {len(domain)}): '{domain}'")
        self.logger.info(f"🚀 [PIPELINE] Representación cruda del dominio: {repr(domain)}")
        self.logger.info(f"🚀 [PIPELINE] Parámetros: recipient={recipient_email}, job_id={job_id}, debug={debug}, full_nuclei_scan={full_nuclei_scan}")

        """Ejecuta el pipeline completo de escaneo y genera el informe PDF.

        Args:
            domain: Dominio a escanear
            recipient_email: Email para notificaciones
            job_id: ID del trabajo (se genera automáticamente si no se proporciona)
            hibp_api_key: API key para Have I Been Pwned
            debug: Modo debug (mantiene archivos temporales)
            full_nuclei_scan: Ejecutar escaneo completo con Nuclei
            
        Returns:
            ScanResult con el resultado del escaneo
        """
        # Validaciones iniciales
        self.logger.info(f"🚀 [PIPELINE] Validando dominio: {domain}")
        self._validate_domain(domain)
        self.logger.info(f"🚀 [PIPELINE] ✅ Dominio válido: {domain}")
        
        if not job_id:
            job_id = f"standalone:{dt.datetime.utcnow().isoformat()}"
            self.logger.info(f"🚀 [PIPELINE] Job ID generado: {job_id}")
        
        self._update_progress(job_id, "queue", 5, ScanStage.QUEUED)
        self.logger.info(f"🚀 [PIPELINE] Progreso actualizado: queue (5%)")
        
        tmp_dir = None
        try:
            with self._temp_directory(domain) as tmp_dir_context:
                tmp_dir = tmp_dir_context
                self.logger.info(f"🚀 [PIPELINE] Directorio temporal creado: {tmp_dir}")
                
                # Verificar que el directorio temporal es escribible
                try:
                    test_file = tmp_dir / "test_write.txt"
                    test_file.write_text("test")
                    test_file.unlink()
                    self.logger.info(f"🚀 [PIPELINE] ✅ Directorio temporal escribible")
                except Exception as e:
                    self.logger.error(f"🚀 [PIPELINE] ❌ Error de permisos en directorio temporal: {e}")
                    raise ScanError(f"Error de permisos en directorio temporal: {e}")
                
                files: Dict[str, Path] = {}
                execution_layers = self._get_execution_order()
                self.logger.info(f"🚀 [PIPELINE] Orden de ejecución: {execution_layers}")
                
                # Ejecutar por capas respetando dependencias
                for layer_idx, layer_steps in enumerate(execution_layers):
                    self.logger.info(f"🚀 [PIPELINE] Ejecutando capa {layer_idx}: {layer_steps}")
                    if layer_idx < 2:  # Capas secuenciales (0 y 1)
                        for step_key in layer_steps:
                            if step_key not in self.steps:
                                self.logger.warning(f"🚀 [PIPELINE] ⚠️ Paso {step_key} no encontrado en steps")
                                continue
                            
                            self.logger.info(f"🚀 [PIPELINE] Ejecutando paso secuencial: {step_key}")
                            step = self.steps[step_key]
                            _, result = self._execute_step(
                                step, job_id, domain, tmp_dir, files,
                                hibp_api_key=hibp_api_key,
                                full_nuclei_scan=full_nuclei_scan
                            )
                            files[step_key] = result
                            self.logger.info(f"🚀 [PIPELINE] ✅ Paso {step_key} completado. Archivo resultado: {result}")
                            
                            # Verificar que el archivo existe
                            if result and isinstance(result, Path) and result.exists():
                                self.logger.info(f"🚀 [PIPELINE] ✅ Archivo {step_key} verificado: {result} (tamaño: {result.stat().st_size} bytes)")
                            else:
                                self.logger.warning(f"🚀 [PIPELINE] ⚠️ Archivo {step_key} no existe o es None: {result}")
                    
                    else:  # Capas paralelas (2 en adelante)
                        with ThreadPoolExecutor(max_workers=min(len(layer_steps), 6)) as executor:
                            future_to_step = {}
                            
                            for step_key in layer_steps:
                                if step_key not in self.steps:
                                    self.logger.warning(f"🚀 [PIPELINE] ⚠️ Paso {step_key} no encontrado en steps")
                                    continue
                                
                                self.logger.info(f"🚀 [PIPELINE] Encolando paso paralelo: {step_key}")
                                step = self.steps[step_key]
                                future = executor.submit(
                                    self._execute_step,
                                    step, job_id, domain, tmp_dir, files,
                                    hibp_api_key=hibp_api_key,
                                    full_nuclei_scan=full_nuclei_scan
                                )
                                future_to_step[future] = step_key
                            
                            # Procesar resultados
                            for future in as_completed(future_to_step):
                                step_key = future_to_step[future]
                                try:
                                    _, result = future.result()
                                    files[step_key] = result
                                    self.logger.info(f"🚀 [PIPELINE] ✅ Paso paralelo {step_key} completado. Archivo resultado: {result}")
                                    
                                    # Verificar que el archivo existe
                                    if result and isinstance(result, Path) and result.exists():
                                        self.logger.info(f"🚀 [PIPELINE] ✅ Archivo {step_key} verificado: {result} (tamaño: {result.stat().st_size} bytes)")
                                    else:
                                        self.logger.warning(f"🚀 [PIPELINE] ⚠️ Archivo {step_key} no existe o es None: {result}")
                                except Exception as e:
                                    self.logger.error(f"🚀 [PIPELINE] ❌ Error in parallel step {step_key}: {e}")
                                    files[step_key] = None # Set to None if step fails to prevent further errors
                
                # Resumen de archivos generados
                self.logger.info(f"🚀 [PIPELINE] Resumen de archivos generados:")
                for step_key, file_path in files.items():
                    if file_path and isinstance(file_path, Path) and file_path.exists():
                        self.logger.info(f"🚀 [PIPELINE]   ✅ {step_key}: {file_path} ({file_path.stat().st_size} bytes)")
                    else:
                        self.logger.warning(f"🚀 [PIPELINE]   ❌ {step_key}: {file_path} (no existe o es None)")
                
                # Enriquecer con threat intelligence
                if "nmap" in files and files["nmap"]:
                    self.logger.info(f"🚀 [PIPELINE] Enriqueciendo con threat intelligence")
                    self._enhance_with_threat_intel(files["nmap"])
                else:
                    self.logger.info(f"🚀 [PIPELINE] No hay archivo nmap para enriquecer con threat intelligence")
                
                # Generar informe PDF
                self.logger.info(f"🚀 [PIPELINE] Generando informe PDF para {domain}")
                self.logger.info(f"🚀 [PIPELINE] Archivos que se pasarán a build_pdf:")
                self.logger.info(f"🚀 [PIPELINE]   httpx_file: {files.get('finger')}")
                self.logger.info(f"🚀 [PIPELINE]   nuclei_file: {files.get('nuclei')}")
                self.logger.info(f"🚀 [PIPELINE]   leaks_file: {files.get('leaks')}")
                self.logger.info(f"🚀 [PIPELINE]   typosquats_file: {files.get('typos')}")
                self.logger.info(f"🚀 [PIPELINE]   dir_brute_file: {files.get('dir_brute')}")
                self.logger.info(f"🚀 [PIPELINE]   cves_file: {files.get('cve')}")
                self.logger.info(f"🚀 [PIPELINE]   nmap_file: {files.get('nmap')}")
                self.logger.info(f"🚀 [PIPELINE]   security_config_file: {files.get('security_config')}")
                self.logger.info(f"🚀 [PIPELINE]   cisa_kev_file: {files.get('cisa_kev')}")
                self.logger.info(f"🚀 [PIPELINE]   greynoise_file: {files.get('greynoise')}")
                
                pdf_path = build_pdf(
                    domain, recipient_email, tmp_dir,
                    httpx_file=files.get("finger"),
                    nuclei_file=files.get("nuclei"),
                    leaks_file=files.get("leaks"),
                    typosquats_file=files.get("typos"),
                    dir_brute_file=files.get("dir_brute"),
                    
                    cves_file=files.get("cve"),
                    nmap_file=files.get("nmap"),
                    security_config_file=files.get("security_config"),
                    cisa_kev_file=files.get("cisa_kev"),
                    greynoise_file=files.get("greynoise")
                )
                self.logger.info(f"🚀 [PIPELINE] ✅ PDF generado: {pdf_path}")
                
                # Calcular métricas
                threat_intel_hits = 0
                nmap_file_path = files.get("nmap")
                if nmap_file_path and nmap_file_path.exists():
                    try:
                        with open(nmap_file_path, 'r') as f:
                            nmap_data_for_metrics = json.load(f)
                            if isinstance(nmap_data_for_metrics, list):
                                for item in nmap_data_for_metrics:
                                    if item.get("threat_intel"):
                                        threat_intel_hits += 1
                    except (json.JSONDecodeError, IOError) as e:
                        self.logger.warning(f"Error al leer o decodificar JSON de {nmap_file_path} para métricas de threat intel: {e}")

                metrics = {
                    "subdomains": self._count_lines(files.get("recon")),
                    "vulnerabilities": self._safe_count_json(files.get("nuclei")),
                    "cves": self._safe_count_json(files.get("cve")),
                    "ports": self._safe_count_json(files.get("nmap")),
                    "cisa_kev_vulnerabilities": self._safe_count_json(files.get("cisa_kev")),
                    "greynoise_malicious_ips": self._safe_count_json(files.get("greynoise")),
                    "leaked_credentials": self._safe_count_json(files.get("leaks")),
                    "typosquats": self._safe_count_json(files.get("typos")),
                    "exposed_directories": self._safe_count_json(files.get("dir_brute")),
                    "security_misconfigurations": self._safe_count_json(files.get("security_config")),
                    "threat_intel_hits": threat_intel_hits
                }
                
                self._update_progress(job_id, "report_generation", 100, ScanStage.COMPLETED)
                self.logger.info(f"🚀 [PIPELINE] ✅ Escaneo completado para {domain}. Informe en: {pdf_path}")
                self.logger.info(f"🚀 [PIPELINE] Métricas finales: {metrics}")
                
                # Enviar notificación por email con el informe adjunto
                from pentest.report import send_notification
                try:
                    self.logger.info(f"🚀 [PIPELINE] Enviando notificación por email a {recipient_email}")
                    email_sent = send_notification(
                        job_id=job_id,
                        status_message="Completado exitosamente",
                        status_type="success",
                        pdf_path=pdf_path,
                        recipient_email=recipient_email,
                        subc=metrics["subdomains"],
                        vulc=metrics["vulnerabilities"],
                        domain=domain,
                        cisa_kev_vulnerabilities=metrics["cisa_kev_vulnerabilities"],
                        greynoise_malicious_ips=metrics["greynoise_malicious_ips"],
                        threat_intel_hits=metrics["threat_intel_hits"]
                    )
                    if email_sent:
                        self.logger.info(f"🚀 [PIPELINE] ✅ Notificación enviada exitosamente a {recipient_email}")
                    else:
                        self.logger.warning(f"🚀 [PIPELINE] ⚠️ No se pudo enviar la notificación a {recipient_email}")
                except Exception as e:
                    self.logger.error(f"🚀 [PIPELINE] ❌ Error enviando notificación: {e}")
                
                return ScanResult(
                    success=True,
                    message="Escaneo completado exitosamente",
                    job_id=job_id,
                    report_path=pdf_path,
                    metrics=metrics
                )
        except ScanError as e:
            self.logger.error(f"🚀 [PIPELINE] ❌ Error de escaneo para {domain}: {e}")
            self._update_progress(job_id, "error", 100, ScanStage.FAILED, error=str(e))
            
            # Enviar notificación de error
            from pentest.report import send_notification
            try:
                self.logger.info(f"🚀 [PIPELINE] Enviando notificación de error a {recipient_email}")
                send_notification(
                    job_id=job_id,
                    status_message=f"Error de escaneo: {e}",
                    status_type="failed",
                    pdf_path=None,
                    recipient_email=recipient_email,
                    subc=0,
                    vulc=0,
                    domain=domain,
                    cisa_kev_vulnerabilities=0,
                    greynoise_malicious_ips=0,
                    threat_intel_hits=0
                )
            except Exception as email_error:
                self.logger.error(f"🚀 [PIPELINE] ❌ Error enviando notificación de error: {email_error}")
            
            return ScanResult(
                success=False,
                message=f"Error de escaneo: {e}",
                job_id=job_id,
                report_path=None,
                metrics={}
            )
        except Exception as e:
            self.logger.error(f"🚀 [PIPELINE] ❌ Error fatal durante el escaneo de {domain}: {e}", exc_info=True)
            self._update_progress(job_id, "error", 100, ScanStage.FAILED, error=str(e))
            
            # Enviar notificación de error fatal
            from pentest.report import send_notification
            try:
                self.logger.info(f"🚀 [PIPELINE] Enviando notificación de error fatal a {recipient_email}")
                send_notification(
                    job_id=job_id,
                    status_message=f"Error fatal durante el escaneo: {e}",
                    status_type="failed",
                    pdf_path=None,
                    recipient_email=recipient_email,
                    subc=0,
                    vulc=0,
                    domain=domain,
                    cisa_kev_vulnerabilities=0,
                    greynoise_malicious_ips=0,
                    threat_intel_hits=0
                )
            except Exception as email_error:
                self.logger.error(f"🚀 [PIPELINE] ❌ Error enviando notificación de error fatal: {email_error}")
            
            return ScanResult(
                success=False,
                message=f"Error fatal durante el escaneo: {e}",
                job_id=job_id,
                report_path=None,
                metrics={}
            )
        finally:
            if tmp_dir and not debug:
                self.logger.info(f"🚀 [PIPELINE] Limpiando directorio temporal para {domain}: {tmp_dir}")
                shutil.rmtree(tmp_dir, ignore_errors=True)
            elif debug:
                self.logger.info(f"🚀 [PIPELINE] Modo debug activado. Directorio temporal conservado: {tmp_dir}")

# ---------------------------------------------------------------------------
# WORKER MEJORADO
# ---------------------------------------------------------------------------



# ---------------------------------------------------------------------------
# FUNCIONES DE COMPATIBILIDAD
# ---------------------------------------------------------------------------



def start_worker():
    """Inicia un worker de RQ para procesar la cola de escaneos."""
    # Configurar logging explícitamente para el worker
    setup_logging()
    
    logger = logging.getLogger(__name__)
    logger.info(f"🚀 [WORKER] Intentando conectar a Redis en {REDIS_URL}")
    
    redis_conn = redis.from_url(REDIS_URL)
    logger.info("🚀 [WORKER] ✅ Conectado exitosamente a Redis")
    
    worker = Worker(['scan_queue'], connection=redis_conn)
    logger.info("🚀 [WORKER] Worker RQ inicializado. Escuchando en 'scan_queue'...")
    
    # Configurar logging adicional para RQ
    import rq.worker
    rq_logger = logging.getLogger('rq.worker')
    rq_logger.setLevel(logging.INFO)
    
    logger.info("🚀 [WORKER] Iniciando worker...")
    worker.work(logging_level='INFO')

# ---------------------------------------------------------------------------
# PUNTO DE ENTRADA
# ---------------------------------------------------------------------------

from rq import get_current_job, Queue, Worker

def _run_scan_job(domain: str, recipient_email: str, hibp_api_key: str = None, debug: bool = False, full_nuclei_scan: bool = False):
    """Función wrapper para ejecutar el pipeline de escaneo dentro de un trabajo de RQ."""
    # Configurar logging para asegurar que aparezcan los logs
    setup_logging()
    
    job = get_current_job()
    job_id = job.id if job else "unknown_job"
    
    logger = logging.getLogger(__name__)
    logger.info(f"🚀 [JOB] Iniciando escaneo para dominio: {domain} (Job ID: {job_id})")
    logger.info(f"🚀 [JOB] Dominio recibido (tipo: {type(domain).__name__}, longitud: {len(domain)}): '{domain}'")
    logger.info(f"🚀 [JOB] Representación cruda del dominio: {repr(domain)}")
    logger.info(f"🚀 [JOB] Parámetros: recipient_email={recipient_email}, debug={debug}, full_nuclei_scan={full_nuclei_scan}")

    try:
        pipeline = ScanPipeline()
        logger.info(f"🚀 [JOB] Pipeline creado, ejecutando generate_pdf...")
        
        result = pipeline.generate_pdf(
            domain=domain,
            recipient_email=recipient_email,
            job_id=job_id, # Pasar el job_id
            hibp_api_key=hibp_api_key,
            debug=debug,
            full_nuclei_scan=full_nuclei_scan
        )
        
        logger.info(f"🚀 [JOB] ✅ Escaneo completado exitosamente para {domain}")
        logger.info(f"🚀 [JOB] Resultado: success={result.success}, report_path={result.report_path}")
        return result
        
    except Exception as e:
        logger.error(f"🚀 [JOB] ❌ Error ejecutando escaneo para {domain} (Job ID: {job_id}): {e}", exc_info=True)
        # Actualizar el estado del progreso a FAILED
        redis_manager = RedisManager(REDIS_URL)
        progress = ScanProgress(job_id=job_id, stage=ScanStage.FAILED, step='Error', percentage=100, error=str(e))
        redis_manager.publish_progress(progress)
        raise  # Re-lanzar la excepción para que RQ la capture

def enqueue_scan(domain: str, recipient_email: str, hibp_api_key: str = None, debug: bool = False, full_nuclei_scan: bool = False):
    """Encola un trabajo de escaneo en RQ."""
    q = Queue('scan_queue', connection=redis.from_url(REDIS_URL))
    job = q.enqueue(
        _run_scan_job,
        domain,
        recipient_email,
        hibp_api_key=hibp_api_key,
        debug=debug,
        full_nuclei_scan=full_nuclei_scan,
        result_ttl=86400, # Mantener el resultado por 24 horas
        job_timeout=3600 # Tiempo máximo de ejecución del trabajo (1 hora)
    )
    logging.getLogger(__name__).info(f"Enqueued scan job for {domain} with ID: {job.id}")
    return job.id

if __name__ == "__main__":
    setup_logging()
    
    if len(sys.argv) > 1 and sys.argv[1] == "worker":
        start_worker()
    else:
        # Modo de prueba
        import argparse
        parser = argparse.ArgumentParser(description="Scanner de seguridad")
        parser.add_argument("--domain", required=True, help="Dominio a escanear")
        parser.add_argument("--email", required=True, help="Email para notificaciones")
        parser.add_argument("--debug", action="store_true", help="Modo debug")
        
        args = parser.parse_args()
        
        pipeline = ScanPipeline()
        result = pipeline.generate_pdf(
            domain=args.domain,
            recipient_email=args.email,
            debug=args.debug
        )
        
        print(f"Resultado: {result.status}")
        if result.report_path:
            print(f"Informe: {result.report_path}")

# === SISTEMA MEJORADO DE DETECCION DE TECNOLOGIAS ===
try:
    # Inicializar integrador mejorado
    enhanced_integrator = EnhancedTechIntegrator(
        enable_fallback=True,
        enable_web_content_analysis=True
    )
    
    if enhanced_integrator.is_available():
        log.info("Sistema mejorado de deteccion disponible")
        
        # Reemplazar función de fingerprinting si está disponible
        def enhanced_fingerprint_wrapper(target, timeout=30):
            """Wrapper para fingerprinting mejorado con fallback."""
            try:
                result = enhanced_integrator.detect_technologies(target)
                if result and result.get('technologies'):
                    return result
            except Exception as e:
                log.warning(f"Fingerprinting mejorado falló, usando básico: {e}")
            
            # Fallback al sistema original
            from .fingerprint import fingerprint_target
            return fingerprint_target(target, timeout)
        
        # Reemplazar la función original
        globals()['fingerprint_target'] = enhanced_fingerprint_wrapper
        
    else:
        log.warning("Sistema mejorado no disponible, usando sistema básico")
        
except Exception as e:
    log.error(f"Error inicializando sistema mejorado: {e}")
    log.info("Continuando con sistema básico")
# === FIN SISTEMA MEJORADO ===
