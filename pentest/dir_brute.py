import logging
import asyncio
import json
import random
from pathlib import Path
from typing import List, Dict, Any, Optional

import httpx
from urllib.parse import urljoin

from pentest.exceptions import DirBruteError
from pentest.config import WAF_EVASION_CONFIG, WAF_USER_AGENTS, WAF_COMMON_HEADERS

log = logging.getLogger(__name__)


async def dir_brute_scan(
    httpx_file: Path,
    tmp_dir: Path,
    extensions: Optional[List[str]] = None,
    auto_extend: bool = False,
    max_concurrent_requests: int = 100,
    delay_between_requests: float = 0.01,
    filter_status_codes: Optional[List[int]] = None,
    filter_content_length: Optional[int] = None,
    tech_stack: Optional[str] = None,
) -> Path:
    """Realiza un escaneo de fuerza bruta de directorios en los hosts activos.

    Args:
        httpx_file: Archivo JSON con hosts activos.
        tmp_dir: Directorio temporal para almacenar resultados.
        extensions: Lista de extensiones a probar (ej. ['php', 'html', 'js']).
        auto_extend: Si es True, combina las palabras clave con las extensiones proporcionadas.
        max_concurrent_requests: N√∫mero m√°ximo de peticiones concurrentes.
        delay_between_requests: Retraso en segundos entre peticiones para evitar saturaci√≥n.
        filter_status_codes: Lista de c√≥digos de estado HTTP a ignorar.
        filter_content_length: Longitud de contenido a ignorar (para 404 uniformes).
        tech_stack: Pila tecnol√≥gica detectada (ej. 'wordpress', 'laravel').

    Returns:
        Path: Ruta al archivo JSON con los resultados del escaneo de fuerza bruta de directorios.
    """
    log.info("üïµÔ∏è‚Äç‚ôÄÔ∏è Iniciando escaneo de fuerza bruta de directorios")

    output_file = tmp_dir / "dir_brute.json"


    # Diccionario de palabras peque√±as para fuerza bruta de directorios
    wordlist_path = Path(__file__).parent / "wordlists" / "dir_brute_small.txt"
    if not wordlist_path.exists():
        raise DirBruteError(f"No se encontr√≥ el archivo de wordlist: {wordlist_path}")

    with open(wordlist_path, "r") as f:
        base_wordlist = [line.strip() for line in f if line.strip()]

    # Wordlists espec√≠ficas por tecnolog√≠a
    tech_wordlists = {
        "wordpress": [
            "wp-admin", "wp-login.php", "wp-content", "wp-includes", "xmlrpc.php",
            "wp-json", "wp-cron.php", "license.txt", "readme.html"
        ],
        "laravel": [
            "storage", "logs", "artisan", ".env", "vendor", "public",
            "bootstrap/cache", "routes", "database", "resources", "app"
        ],
        # A√±adir m√°s tecnolog√≠as seg√∫n sea necesario
    }

    # Fusionar wordlist base con wordlist espec√≠fica de tecnolog√≠a
    current_wordlist = list(base_wordlist)
    if tech_stack and tech_stack.lower() in tech_wordlists:
        log.info(f"Detectado stack tecnol√≥gico: {tech_stack}. A√±adiendo palabras clave espec√≠ficas.")
        current_wordlist.extend(tech_wordlists[tech_stack.lower()])

    wordlist = []
    if auto_extend and extensions:
        for word in current_wordlist:
            wordlist.append(word)
            for ext in extensions:
                wordlist.append(f"{word}.{ext}")
    else:
        wordlist = current_wordlist

    findings = []

    try:
        with open(httpx_file, "r") as f:
            hosts_data = json.load(f)

        async def fetch(client, target_url, word):
            try:
                await asyncio.sleep(delay_between_requests)  # Introduce el retraso
                test_url = urljoin(target_url, word)
                
                # Usar User-Agent aleatorio para evasi√≥n anti-WAF
                headers = WAF_COMMON_HEADERS.copy()
                headers['User-Agent'] = random.choice(WAF_USER_AGENTS)
                
                config = WAF_EVASION_CONFIG
                response = await client.get(
                    test_url, 
                    headers=headers,
                    follow_redirects=False, 
                    timeout=config['max_time']
                )

                # Aplicar filtros
                if filter_status_codes and response.status_code in filter_status_codes:
                    return
                if filter_content_length and len(response.content) == filter_content_length:
                    return

                if response.status_code == 200:
                    return {
                        "url": test_url,
                        "status_code": response.status_code,
                        "description": f"Directorio o archivo encontrado: {test_url}"
                    }
                elif response.status_code == 401 or response.status_code == 403:
                    return {
                        "url": test_url,
                        "status_code": response.status_code,
                        "description": f"Acceso denegado o requiere autenticaci√≥n: {test_url}"
                    }
                else:
                    log.debug(f"[-] No encontrado: {test_url} (Status: {response.status_code})")
                    return None
            except httpx.RequestError as e:
                log.warning(f"Error al escanear {test_url}: {e}")
                return None
            except Exception as e:
                log.error(f"Error inesperado al escanear {test_url}: {e}")
                return None

        # Configurar cliente httpx con evasi√≥n anti-WAF
        config = WAF_EVASION_CONFIG
        client_config = {
            'timeout': httpx.Timeout(config['max_time']),
            'verify': False,  # Ignorar certificados SSL
            'limits': httpx.Limits(
                max_keepalive_connections=20,
                max_connections=max_concurrent_requests
            )
        }
        
        async with httpx.AsyncClient(**client_config) as client:
            tasks = []
            for host_info in hosts_data:
                target_url = host_info.get("url")
                if not target_url:
                    continue

                log.debug(f"Escaneando directorios en {target_url}")

                for word in wordlist:
                    tasks.append(fetch(client, target_url, word))

            # Control de concurrencia
            results = []
            for i in range(0, len(tasks), max_concurrent_requests):
                batch = tasks[i:i + max_concurrent_requests]
                batch_results = await asyncio.gather(*batch)
                results.extend([r for r in batch_results if r is not None])

            # Agrupar resultados por host
            host_grouped_findings = {}
            for res in results:
                host_url = urljoin(res["url"], '/') # Get base URL
                if host_url not in host_grouped_findings:
                    host_grouped_findings[host_url] = []
                host_grouped_findings[host_url].append(res)
            
            for host_url, host_findings in host_grouped_findings.items():
                findings.append({"host": host_url, "found_paths": host_findings})

        with open(output_file, "w") as f:
            json.dump(findings, f, indent=2)

        log.info("‚úÖ Escaneo de fuerza bruta de directorios completado. Resultados guardados en %s", output_file)
        return output_file

    except Exception as e:
        raise DirBruteError(f"Error durante el escaneo de fuerza bruta de directorios: {str(e)}") from e