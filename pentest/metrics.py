#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Sistema de métricas para detección de tecnologías.
Monitorea rendimiento, precisión y eficiencia del análisis.
"""

import json
import time
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from collections import defaultdict, Counter
import statistics

log = logging.getLogger(__name__)

@dataclass
class DetectionMetric:
    """Métrica individual de detección."""
    url: str
    timestamp: float
    detection_time: float
    technologies_found: int
    tools_used: List[str]
    confidence_scores: List[float]
    success: bool
    error_message: Optional[str] = None
    
    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)

@dataclass
class ToolMetric:
    """Métrica de rendimiento por herramienta."""
    tool_name: str
    total_attempts: int = 0
    successful_attempts: int = 0
    failed_attempts: int = 0
    total_time: float = 0.0
    technologies_detected: int = 0
    avg_confidence: float = 0.0
    
    @property
    def success_rate(self) -> float:
        if self.total_attempts == 0:
            return 0.0
        return (self.successful_attempts / self.total_attempts) * 100
    
    @property
    def avg_time(self) -> float:
        if self.successful_attempts == 0:
            return 0.0
        return self.total_time / self.successful_attempts
    
    def to_dict(self) -> Dict[str, Any]:
        data = asdict(self)
        data['success_rate'] = self.success_rate
        data['avg_time'] = self.avg_time
        return data

class TechDetectionMetrics:
    """Sistema de métricas para detección de tecnologías."""
    
    def __init__(self, metrics_file: Optional[str] = None, max_metrics: int = 10000):
        self.metrics_file = metrics_file or str(Path(__file__).parent / 'detection_metrics.json')
        self.max_metrics = max_metrics
        
        # Métricas en memoria
        self.detection_metrics: List[DetectionMetric] = []
        self.tool_metrics: Dict[str, ToolMetric] = {}
        self.session_start = time.time()
        
        # Contadores
        self.total_detections = 0
        self.successful_detections = 0
        self.failed_detections = 0
        
        # Cargar métricas existentes
        self._load_metrics()
    
    def record_detection(self, url: str, technologies: List[Dict[str, Any]], 
                        detection_time: float, tools_used: List[str], 
                        success: bool = True, error_message: Optional[str] = None) -> None:
        """Registra métricas de una detección."""
        
        # Extraer scores de confianza
        confidence_scores = [tech.get('confidence', 0) for tech in technologies]
        
        # Crear métrica de detección
        metric = DetectionMetric(
            url=url,
            timestamp=time.time(),
            detection_time=detection_time,
            technologies_found=len(technologies),
            tools_used=tools_used,
            confidence_scores=confidence_scores,
            success=success,
            error_message=error_message
        )
        
        # Agregar a métricas
        self.detection_metrics.append(metric)
        self.total_detections += 1
        
        if success:
            self.successful_detections += 1
        else:
            self.failed_detections += 1
        
        # Actualizar métricas por herramienta
        self._update_tool_metrics(technologies, tools_used, detection_time, success)
        
        # Limpiar métricas antiguas si es necesario
        if len(self.detection_metrics) > self.max_metrics:
            self._cleanup_old_metrics()
        
        # Guardar métricas periódicamente
        if self.total_detections % 10 == 0:
            self._save_metrics()
        
        log.debug(f"Métrica registrada para {url}: {len(technologies)} tecnologías, {detection_time:.2f}s")
    
    def _update_tool_metrics(self, technologies: List[Dict[str, Any]], 
                           tools_used: List[str], detection_time: float, 
                           success: bool) -> None:
        """Actualiza métricas por herramienta."""
        
        # Agrupar tecnologías por herramienta
        tech_by_tool = defaultdict(list)
        for tech in technologies:
            source = tech.get('source', 'unknown')
            tech_by_tool[source].append(tech)
        
        # Actualizar métricas para cada herramienta usada
        for tool in tools_used:
            if tool not in self.tool_metrics:
                self.tool_metrics[tool] = ToolMetric(tool_name=tool)
            
            tool_metric = self.tool_metrics[tool]
            tool_metric.total_attempts += 1
            
            if success and tool in tech_by_tool:
                tool_metric.successful_attempts += 1
                tool_metric.total_time += detection_time
                
                # Contar tecnologías detectadas por esta herramienta
                tool_techs = tech_by_tool[tool]
                tool_metric.technologies_detected += len(tool_techs)
                
                # Calcular confianza promedio
                if tool_techs:
                    confidences = [t.get('confidence', 0) for t in tool_techs]
                    if confidences:
                        current_avg = tool_metric.avg_confidence
                        new_avg = statistics.mean(confidences)
                        
                        # Promedio ponderado
                        if current_avg > 0:
                            tool_metric.avg_confidence = (current_avg + new_avg) / 2
                        else:
                            tool_metric.avg_confidence = new_avg
            else:
                tool_metric.failed_attempts += 1
    
    def get_performance_report(self, hours_back: Optional[int] = None) -> Dict[str, Any]:
        """Genera reporte de rendimiento."""
        
        # Filtrar métricas por tiempo si se especifica
        metrics_to_analyze = self.detection_metrics
        if hours_back:
            cutoff_time = time.time() - (hours_back * 3600)
            metrics_to_analyze = [
                m for m in self.detection_metrics 
                if m.timestamp >= cutoff_time
            ]
        
        if not metrics_to_analyze:
            return {'error': 'No hay datos de métricas para el período especificado'}
        
        # Calcular estadísticas básicas
        detection_times = [m.detection_time for m in metrics_to_analyze if m.success]
        tech_counts = [m.technologies_found for m in metrics_to_analyze if m.success]
        confidence_scores = []
        for m in metrics_to_analyze:
            confidence_scores.extend(m.confidence_scores)
        
        # Estadísticas de tiempo
        time_stats = {}
        if detection_times:
            time_stats = {
                'average': round(statistics.mean(detection_times), 2),
                'median': round(statistics.median(detection_times), 2),
                'min': round(min(detection_times), 2),
                'max': round(max(detection_times), 2),
                'std_dev': round(statistics.stdev(detection_times) if len(detection_times) > 1 else 0, 2)
            }
        
        # Estadísticas de tecnologías
        tech_stats = {}
        if tech_counts:
            tech_stats = {
                'average_per_site': round(statistics.mean(tech_counts), 1),
                'median_per_site': round(statistics.median(tech_counts), 1),
                'max_per_site': max(tech_counts),
                'total_detected': sum(tech_counts)
            }
        
        # Estadísticas de confianza
        confidence_stats = {}
        if confidence_scores:
            confidence_stats = {
                'average': round(statistics.mean(confidence_scores), 1),
                'median': round(statistics.median(confidence_scores), 1),
                'distribution': self._get_confidence_distribution(confidence_scores)
            }
        
        # Métricas por herramienta
        tool_performance = {}
        for tool_name, tool_metric in self.tool_metrics.items():
            tool_performance[tool_name] = {
                'success_rate': f"{tool_metric.success_rate:.1f}%",
                'avg_time': f"{tool_metric.avg_time:.2f}s",
                'technologies_detected': tool_metric.technologies_detected,
                'avg_confidence': f"{tool_metric.avg_confidence:.1f}%",
                'total_attempts': tool_metric.total_attempts
            }
        
        # Errores más comunes
        error_analysis = self._analyze_errors(metrics_to_analyze)
        
        # Tendencias temporales
        temporal_analysis = self._analyze_temporal_trends(metrics_to_analyze)
        
        return {
            'summary': {
                'total_detections': len(metrics_to_analyze),
                'successful_detections': len([m for m in metrics_to_analyze if m.success]),
                'failed_detections': len([m for m in metrics_to_analyze if not m.success]),
                'success_rate': f"{(len([m for m in metrics_to_analyze if m.success]) / len(metrics_to_analyze) * 100):.1f}%" if metrics_to_analyze else "0%",
                'analysis_period': f"Últimas {hours_back} horas" if hours_back else "Toda la sesión"
            },
            'performance': {
                'detection_time': time_stats,
                'technologies': tech_stats,
                'confidence': confidence_stats
            },
            'tools': tool_performance,
            'errors': error_analysis,
            'trends': temporal_analysis,
            'recommendations': self._generate_performance_recommendations()
        }
    
    def _get_confidence_distribution(self, scores: List[float]) -> Dict[str, int]:
        """Obtiene distribución de scores de confianza."""
        distribution = {
            'high (80-100%)': 0,
            'medium (60-79%)': 0,
            'low (40-59%)': 0,
            'very_low (<40%)': 0
        }
        
        for score in scores:
            if score >= 80:
                distribution['high (80-100%)'] += 1
            elif score >= 60:
                distribution['medium (60-79%)'] += 1
            elif score >= 40:
                distribution['low (40-59%)'] += 1
            else:
                distribution['very_low (<40%)'] += 1
        
        return distribution
    
    def _analyze_errors(self, metrics: List[DetectionMetric]) -> Dict[str, Any]:
        """Analiza errores más comunes."""
        failed_metrics = [m for m in metrics if not m.success]
        
        if not failed_metrics:
            return {'total_errors': 0, 'common_errors': []}
        
        # Contar errores por tipo
        error_counter = Counter()
        for metric in failed_metrics:
            if metric.error_message:
                # Simplificar mensaje de error para agrupación
                error_type = self._categorize_error(metric.error_message)
                error_counter[error_type] += 1
        
        return {
            'total_errors': len(failed_metrics),
            'error_rate': f"{(len(failed_metrics) / len(metrics) * 100):.1f}%",
            'common_errors': [
                {'error_type': error, 'count': count, 'percentage': f"{(count/len(failed_metrics)*100):.1f}%"}
                for error, count in error_counter.most_common(5)
            ]
        }
    
    def _categorize_error(self, error_message: str) -> str:
        """Categoriza un mensaje de error."""
        error_lower = error_message.lower()
        
        if 'timeout' in error_lower:
            return 'Timeout'
        elif 'connection' in error_lower:
            return 'Connection Error'
        elif 'not found' in error_lower or '404' in error_lower:
            return 'Tool Not Found'
        elif 'permission' in error_lower or 'access' in error_lower:
            return 'Permission Error'
        elif 'json' in error_lower or 'parse' in error_lower:
            return 'Parsing Error'
        else:
            return 'Other Error'
    
    def _analyze_temporal_trends(self, metrics: List[DetectionMetric]) -> Dict[str, Any]:
        """Analiza tendencias temporales."""
        if len(metrics) < 10:
            return {'insufficient_data': True}
        
        # Agrupar por horas
        hourly_stats = defaultdict(list)
        for metric in metrics:
            hour = datetime.fromtimestamp(metric.timestamp).strftime('%H')
            if metric.success:
                hourly_stats[hour].append({
                    'detection_time': metric.detection_time,
                    'technologies': metric.technologies_found
                })
        
        # Calcular promedios por hora
        hourly_averages = {}
        for hour, data in hourly_stats.items():
            if data:
                avg_time = statistics.mean([d['detection_time'] for d in data])
                avg_techs = statistics.mean([d['technologies'] for d in data])
                hourly_averages[hour] = {
                    'avg_detection_time': round(avg_time, 2),
                    'avg_technologies': round(avg_techs, 1),
                    'samples': len(data)
                }
        
        return {
            'hourly_performance': hourly_averages,
            'peak_hours': self._find_peak_hours(hourly_averages),
            'performance_trend': self._calculate_trend(metrics)
        }
    
    def _find_peak_hours(self, hourly_data: Dict[str, Dict]) -> Dict[str, str]:
        """Encuentra horas pico de rendimiento."""
        if not hourly_data:
            return {}
        
        best_time = min(hourly_data.items(), 
                       key=lambda x: x[1]['avg_detection_time'])
        worst_time = max(hourly_data.items(), 
                        key=lambda x: x[1]['avg_detection_time'])
        
        return {
            'fastest_hour': f"{best_time[0]}:00 ({best_time[1]['avg_detection_time']}s avg)",
            'slowest_hour': f"{worst_time[0]}:00 ({worst_time[1]['avg_detection_time']}s avg)"
        }
    
    def _calculate_trend(self, metrics: List[DetectionMetric]) -> str:
        """Calcula tendencia de rendimiento."""
        if len(metrics) < 20:
            return "Datos insuficientes"
        
        # Dividir en dos mitades y comparar
        mid_point = len(metrics) // 2
        first_half = metrics[:mid_point]
        second_half = metrics[mid_point:]
        
        first_avg = statistics.mean([m.detection_time for m in first_half if m.success])
        second_avg = statistics.mean([m.detection_time for m in second_half if m.success])
        
        if abs(first_avg - second_avg) < 0.5:
            return "Estable"
        elif second_avg < first_avg:
            improvement = ((first_avg - second_avg) / first_avg) * 100
            return f"Mejorando ({improvement:.1f}% más rápido)"
        else:
            degradation = ((second_avg - first_avg) / first_avg) * 100
            return f"Degradando ({degradation:.1f}% más lento)"
    
    def _generate_performance_recommendations(self) -> List[str]:
        """Genera recomendaciones de rendimiento."""
        recommendations = []
        
        # Analizar métricas de herramientas
        for tool_name, tool_metric in self.tool_metrics.items():
            if tool_metric.success_rate < 70:
                recommendations.append(
                    f"Herramienta '{tool_name}' tiene baja tasa de éxito ({tool_metric.success_rate:.1f}%). "
                    "Considerar revisar configuración o timeouts."
                )
            
            if tool_metric.avg_time > 30:
                recommendations.append(
                    f"Herramienta '{tool_name}' es lenta (promedio {tool_metric.avg_time:.1f}s). "
                    "Considerar optimizar timeouts o paralelización."
                )
        
        # Analizar métricas generales
        if self.failed_detections / max(self.total_detections, 1) > 0.2:
            recommendations.append(
                "Alta tasa de fallos detectada (>20%). Revisar conectividad y configuración de herramientas."
            )
        
        # Recomendaciones de cache
        if self.total_detections > 100:
            recommendations.append(
                "Considerar implementar cache más agresivo para dominios similares."
            )
        
        if not recommendations:
            recommendations.append("Rendimiento dentro de parámetros normales.")
        
        return recommendations
    
    def _cleanup_old_metrics(self) -> None:
        """Limpia métricas antiguas para mantener el tamaño bajo control."""
        # Mantener solo las métricas más recientes
        self.detection_metrics = self.detection_metrics[-self.max_metrics//2:]
        log.debug(f"Limpieza de métricas: mantenidas {len(self.detection_metrics)} métricas")
    
    def _save_metrics(self) -> None:
        """Guarda métricas en archivo."""
        try:
            metrics_data = {
                'session_start': self.session_start,
                'last_update': time.time(),
                'detection_metrics': [m.to_dict() for m in self.detection_metrics[-1000:]],  # Solo últimas 1000
                'tool_metrics': {name: metric.to_dict() for name, metric in self.tool_metrics.items()},
                'summary': {
                    'total_detections': self.total_detections,
                    'successful_detections': self.successful_detections,
                    'failed_detections': self.failed_detections
                }
            }
            
            with open(self.metrics_file, 'w', encoding='utf-8') as f:
                json.dump(metrics_data, f, indent=2, ensure_ascii=False)
            
            log.debug(f"Métricas guardadas en {self.metrics_file}")
            
        except Exception as e:
            log.error(f"Error guardando métricas: {e}")
    
    def _load_metrics(self) -> None:
        """Carga métricas desde archivo."""
        try:
            metrics_file = Path(self.metrics_file)
            if not metrics_file.exists():
                return
            
            with open(metrics_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # Cargar métricas de detección
            for metric_data in data.get('detection_metrics', []):
                metric = DetectionMetric(**metric_data)
                self.detection_metrics.append(metric)
            
            # Cargar métricas de herramientas
            for tool_name, tool_data in data.get('tool_metrics', {}).items():
                # Remover campos calculados antes de crear el objeto
                tool_data_clean = {k: v for k, v in tool_data.items() 
                                 if k not in ['success_rate', 'avg_time']}
                self.tool_metrics[tool_name] = ToolMetric(**tool_data_clean)
            
            # Cargar resumen
            summary = data.get('summary', {})
            self.total_detections = summary.get('total_detections', 0)
            self.successful_detections = summary.get('successful_detections', 0)
            self.failed_detections = summary.get('failed_detections', 0)
            
            log.info(f"Métricas cargadas: {len(self.detection_metrics)} detecciones, {len(self.tool_metrics)} herramientas")
            
        except Exception as e:
            log.warning(f"Error cargando métricas: {e}")
    
    def export_metrics(self, output_file: str, format: str = 'json') -> bool:
        """Exporta métricas a archivo."""
        try:
            report = self.get_performance_report()
            
            if format.lower() == 'json':
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(report, f, indent=2, ensure_ascii=False)
            else:
                raise ValueError(f"Formato no soportado: {format}")
            
            log.info(f"Métricas exportadas a {output_file}")
            return True
            
        except Exception as e:
            log.error(f"Error exportando métricas: {e}")
            return False
    
    def reset_metrics(self) -> None:
        """Reinicia todas las métricas."""
        self.detection_metrics.clear()
        self.tool_metrics.clear()
        self.total_detections = 0
        self.successful_detections = 0
        self.failed_detections = 0
        self.session_start = time.time()
        
        # Eliminar archivo de métricas
        try:
            Path(self.metrics_file).unlink(missing_ok=True)
        except Exception as e:
            log.warning(f"Error eliminando archivo de métricas: {e}")
        
        log.info("Métricas reiniciadas")

if __name__ == "__main__":
    # Ejemplo de uso
    metrics = TechDetectionMetrics()
    
    # Simular algunas detecciones
    import random
    
    for i in range(20):
        url = f"https://example{i}.com"
        technologies = [
            {'name': 'WordPress', 'confidence': random.randint(70, 95), 'source': 'wappalyzer'},
            {'name': 'PHP', 'confidence': random.randint(60, 85), 'source': 'custom_patterns'}
        ]
        detection_time = random.uniform(5, 30)
        tools_used = ['wappalyzer', 'custom_patterns']
        success = random.choice([True, True, True, False])  # 75% éxito
        
        metrics.record_detection(url, technologies, detection_time, tools_used, success)
    
    # Generar reporte
    report = metrics.get_performance_report()
    print(json.dumps(report, indent=2, ensure_ascii=False))