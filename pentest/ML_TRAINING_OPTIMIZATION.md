# üöÄ Optimizaci√≥n del Entrenamiento de Modelos ML

## üìã √çndice
1. [Estrategias de Mejora de Datos](#estrategias-de-mejora-de-datos)
2. [T√©cnicas de Augmentaci√≥n](#t√©cnicas-de-augmentaci√≥n)
3. [Optimizaci√≥n del Pipeline](#optimizaci√≥n-del-pipeline)
4. [M√©tricas y Evaluaci√≥n](#m√©tricas-y-evaluaci√≥n)
5. [Implementaci√≥n Pr√°ctica](#implementaci√≥n-pr√°ctica)

## üéØ Estrategias de Mejora de Datos

### 1. Calidad de Datos

#### Limpieza y Preprocesamiento
```python
# Ejemplo de limpieza de datos
def clean_training_data(events: List[SecurityEvent]) -> List[SecurityEvent]:
    """Limpia y valida datos de entrenamiento"""
    cleaned_events = []
    
    for event in events:
        # Validar completitud de datos
        if not event.target_domain or not event.timestamp:
            continue
            
        # Normalizar dominios
        event.target_domain = event.target_domain.lower().strip()
        
        # Validar rangos de valores
        if event.response_time < 0 or event.response_time > 60:
            event.response_time = min(max(event.response_time, 0), 60)
            
        # Filtrar eventos duplicados
        if not is_duplicate_event(event, cleaned_events):
            cleaned_events.append(event)
    
    return cleaned_events
```

#### Balanceo de Clases
```python
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline

def balance_training_data(X, y):
    """Balancea datos de entrenamiento"""
    # Combinar over-sampling y under-sampling
    over = SMOTE(sampling_strategy=0.3)
    under = RandomUnderSampler(sampling_strategy=0.7)
    
    pipeline = Pipeline(steps=[('o', over), ('u', under)])
    X_balanced, y_balanced = pipeline.fit_resample(X, y)
    
    return X_balanced, y_balanced
```

### 2. Enriquecimiento de Caracter√≠sticas

#### Caracter√≠sticas Derivadas
```python
def extract_enhanced_features(event: SecurityEvent) -> Dict[str, float]:
    """Extrae caracter√≠sticas mejoradas"""
    features = {}
    
    # Caracter√≠sticas temporales
    features['hour_of_day'] = event.timestamp.hour
    features['day_of_week'] = event.timestamp.weekday()
    features['is_weekend'] = event.timestamp.weekday() >= 5
    
    # Caracter√≠sticas de dominio
    features['domain_length'] = len(event.target_domain)
    features['subdomain_count'] = event.target_domain.count('.')
    features['has_suspicious_tld'] = any(tld in event.target_domain for tld in ['.tk', '.ml', '.ga'])
    
    # Caracter√≠sticas de red
    features['port_diversity'] = len(set(event.ports_open)) if event.ports_open else 0
    features['dangerous_ports_ratio'] = calculate_dangerous_ports_ratio(event.ports_open)
    
    # Caracter√≠sticas de vulnerabilidades
    features['vuln_severity_score'] = calculate_severity_score(event.vulnerabilities)
    features['vuln_density'] = len(event.vulnerabilities) / max(len(event.technologies), 1)
    
    return features
```

## üîÑ T√©cnicas de Augmentaci√≥n

### 1. Augmentaci√≥n de Datos de Seguridad

```python
import random
from copy import deepcopy

def augment_security_data(events: List[SecurityEvent], augmentation_factor: int = 2) -> List[SecurityEvent]:
    """Aumenta datos de entrenamiento con variaciones realistas"""
    augmented_events = list(events)  # Mantener originales
    
    for event in events:
        for _ in range(augmentation_factor):
            augmented_event = deepcopy(event)
            
            # Variaciones de tiempo
            time_delta = random.randint(-3600, 3600)  # ¬±1 hora
            augmented_event.timestamp += timedelta(seconds=time_delta)
            
            # Variaciones de respuesta
            augmented_event.response_time *= random.uniform(0.8, 1.2)
            
            # Variaciones de puertos (mantener l√≥gica)
            if augmented_event.ports_open:
                # Agregar/quitar puertos relacionados
                augmented_event.ports_open = vary_ports(augmented_event.ports_open)
            
            # Variaciones de tecnolog√≠as
            if augmented_event.technologies:
                augmented_event.technologies = vary_technologies(augmented_event.technologies)
            
            augmented_events.append(augmented_event)
    
    return augmented_events

def vary_ports(original_ports: List[int]) -> List[int]:
    """Var√≠a puertos manteniendo coherencia"""
    varied_ports = list(original_ports)
    
    # Mapeo de puertos relacionados
    related_ports = {
        80: [8080, 8000, 3000],
        443: [8443, 9443],
        22: [2222, 2200],
        21: [2121],
        3306: [3307, 3308]
    }
    
    for port in original_ports:
        if port in related_ports and random.random() < 0.3:
            related = random.choice(related_ports[port])
            if related not in varied_ports:
                varied_ports.append(related)
    
    return varied_ports
```

### 2. Generaci√≥n Sint√©tica de Datos

```python
from faker import Faker
import numpy as np

def generate_synthetic_events(count: int, malicious_ratio: float = 0.3) -> List[SecurityEvent]:
    """Genera eventos sint√©ticos para entrenamiento"""
    fake = Faker()
    events = []
    
    for i in range(count):
        is_malicious = random.random() < malicious_ratio
        
        # Generar caracter√≠sticas base
        event = SecurityEvent(
            timestamp=fake.date_time_between(start_date='-1y', end_date='now'),
            event_id=f"synthetic_{i}",
            target_domain=generate_realistic_domain(is_malicious),
            source_ip=fake.ipv4(),
            event_type="synthetic_scan",
            is_malicious=is_malicious
        )
        
        # Caracter√≠sticas espec√≠ficas seg√∫n tipo
        if is_malicious:
            event.vulnerabilities = generate_malicious_vulns()
            event.ports_open = generate_malicious_ports()
            event.response_time = random.uniform(0.1, 2.0)  # M√°s r√°pido
        else:
            event.vulnerabilities = generate_benign_vulns()
            event.ports_open = generate_standard_ports()
            event.response_time = random.uniform(0.5, 5.0)  # M√°s lento
        
        events.append(event)
    
    return events

def generate_realistic_domain(is_malicious: bool) -> str:
    """Genera dominios realistas"""
    if is_malicious:
        # Dominios sospechosos
        suspicious_patterns = [
            f"{''.join(random.choices('abcdefghijklmnopqrstuvwxyz', k=random.randint(8, 15)))}.tk",
            f"secure-{random.randint(1000, 9999)}.ml",
            f"update-{random.choice(['microsoft', 'google', 'apple'])}.ga"
        ]
        return random.choice(suspicious_patterns)
    else:
        # Dominios leg√≠timos
        legitimate_patterns = [
            f"{random.choice(['api', 'www', 'app'])}.{random.choice(['company', 'service', 'platform'])}.com",
            f"{random.choice(['secure', 'portal', 'dashboard'])}.{random.choice(['enterprise', 'business'])}.org"
        ]
        return random.choice(legitimate_patterns)
```

## ‚öôÔ∏è Optimizaci√≥n del Pipeline

### 1. Pipeline de Entrenamiento Mejorado

```python
class EnhancedMLTrainingPipeline:
    def __init__(self):
        self.feature_engineering = FeatureEngineeringPipeline()
        self.data_validation = DataValidationPipeline()
        self.model_selection = AutoMLModelSelection()
        
    def train_optimized_models(self, raw_events: List[SecurityEvent]):
        """Pipeline completo de entrenamiento optimizado"""
        
        # 1. Validaci√≥n y limpieza
        logger.info("Iniciando validaci√≥n de datos...")
        clean_events = self.data_validation.validate_and_clean(raw_events)
        
        # 2. Ingenier√≠a de caracter√≠sticas
        logger.info("Extrayendo caracter√≠sticas mejoradas...")
        X, y = self.feature_engineering.extract_features(clean_events)
        
        # 3. Augmentaci√≥n de datos
        logger.info("Augmentando datos de entrenamiento...")
        X_aug, y_aug = self.augment_training_data(X, y)
        
        # 4. Balanceo de clases
        logger.info("Balanceando clases...")
        X_balanced, y_balanced = balance_training_data(X_aug, y_aug)
        
        # 5. Selecci√≥n autom√°tica de modelos
        logger.info("Seleccionando mejores modelos...")
        best_models = self.model_selection.find_best_models(X_balanced, y_balanced)
        
        # 6. Entrenamiento con validaci√≥n cruzada
        logger.info("Entrenando modelos finales...")
        final_models = self.train_with_cross_validation(best_models, X_balanced, y_balanced)
        
        return final_models
```

### 2. Selecci√≥n Autom√°tica de Hiperpar√°metros

```python
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
from scipy.stats import randint, uniform

def optimize_hyperparameters(X, y, model_type='ensemble'):
    """Optimiza hiperpar√°metros autom√°ticamente"""
    
    if model_type == 'random_forest':
        param_dist = {
            'n_estimators': randint(100, 500),
            'max_depth': randint(5, 20),
            'min_samples_split': randint(2, 20),
            'min_samples_leaf': randint(1, 10),
            'max_features': ['sqrt', 'log2', None]
        }
        
        rf = RandomForestClassifier(random_state=42, class_weight='balanced')
        search = RandomizedSearchCV(
            rf, param_dist, n_iter=50, cv=5, 
            scoring='f1', n_jobs=-1, random_state=42
        )
        
    elif model_type == 'gradient_boosting':
        param_dist = {
            'n_estimators': randint(50, 300),
            'learning_rate': uniform(0.01, 0.3),
            'max_depth': randint(3, 10),
            'subsample': uniform(0.6, 0.4)
        }
        
        gb = GradientBoostingClassifier(random_state=42)
        search = RandomizedSearchCV(
            gb, param_dist, n_iter=50, cv=5,
            scoring='f1', n_jobs=-1, random_state=42
        )
    
    search.fit(X, y)
    return search.best_estimator_, search.best_params_
```

## üìä M√©tricas y Evaluaci√≥n

### 1. M√©tricas Avanzadas

```python
def evaluate_model_comprehensive(model, X_test, y_test, X_train, y_train):
    """Evaluaci√≥n comprehensiva del modelo"""
    
    # Predicciones
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1]
    
    # M√©tricas b√°sicas
    metrics = {
        'accuracy': accuracy_score(y_test, y_pred),
        'precision': precision_score(y_test, y_pred),
        'recall': recall_score(y_test, y_pred),
        'f1_score': f1_score(y_test, y_pred),
        'auc_roc': roc_auc_score(y_test, y_pred_proba)
    }
    
    # M√©tricas avanzadas
    metrics['balanced_accuracy'] = balanced_accuracy_score(y_test, y_pred)
    metrics['matthews_corrcoef'] = matthews_corrcoef(y_test, y_pred)
    
    # An√°lisis de overfitting
    train_pred = model.predict(X_train)
    train_accuracy = accuracy_score(y_train, train_pred)
    metrics['overfitting_score'] = train_accuracy - metrics['accuracy']
    
    # An√°lisis de caracter√≠sticas importantes
    if hasattr(model, 'feature_importances_'):
        metrics['feature_importance_variance'] = np.var(model.feature_importances_)
    
    return metrics
```

### 2. Validaci√≥n Temporal

```python
def temporal_validation(events: List[SecurityEvent], model_class, test_months=3):
    """Validaci√≥n temporal para datos de seguridad"""
    
    # Ordenar por timestamp
    events_sorted = sorted(events, key=lambda x: x.timestamp)
    
    # Dividir temporalmente
    cutoff_date = events_sorted[-1].timestamp - timedelta(days=30 * test_months)
    
    train_events = [e for e in events_sorted if e.timestamp < cutoff_date]
    test_events = [e for e in events_sorted if e.timestamp >= cutoff_date]
    
    # Entrenar y evaluar
    model = model_class()
    model.train(train_events)
    
    results = []
    for event in test_events:
        prediction = model.predict(event)
        results.append({
            'timestamp': event.timestamp,
            'actual': event.is_malicious,
            'predicted': prediction.is_malicious,
            'confidence': prediction.confidence
        })
    
    return results
```

## üõ†Ô∏è Implementaci√≥n Pr√°ctica

### 1. Clase de Entrenamiento Mejorada

```python
class OptimizedMLTrainer:
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.feature_extractor = EnhancedFeatureExtractor()
        self.data_augmenter = SecurityDataAugmenter()
        self.model_optimizer = HyperparameterOptimizer()
        
    def train_with_best_practices(self, events: List[SecurityEvent]) -> Dict[str, Any]:
        """Entrena modelos siguiendo mejores pr√°cticas"""
        
        training_log = {
            'start_time': datetime.now(),
            'original_samples': len(events),
            'steps': []
        }
        
        try:
            # Paso 1: Validaci√≥n inicial
            self._log_step(training_log, "Validando datos iniciales")
            valid_events = self._validate_training_data(events)
            
            # Paso 2: An√°lisis exploratorio
            self._log_step(training_log, "Analizando distribuci√≥n de datos")
            data_analysis = self._analyze_data_distribution(valid_events)
            
            # Paso 3: Ingenier√≠a de caracter√≠sticas
            self._log_step(training_log, "Extrayendo caracter√≠sticas")
            X, y = self.feature_extractor.extract_enhanced_features(valid_events)
            
            # Paso 4: Augmentaci√≥n inteligente
            self._log_step(training_log, "Augmentando datos")
            X_aug, y_aug = self.data_augmenter.smart_augmentation(X, y, data_analysis)
            
            # Paso 5: Divisi√≥n estratificada
            self._log_step(training_log, "Dividiendo datos")
            X_train, X_test, y_train, y_test = self._stratified_split(X_aug, y_aug)
            
            # Paso 6: Optimizaci√≥n de modelos
            self._log_step(training_log, "Optimizando hiperpar√°metros")
            best_models = self.model_optimizer.optimize_multiple_models(X_train, y_train)
            
            # Paso 7: Evaluaci√≥n final
            self._log_step(training_log, "Evaluando modelos")
            evaluation_results = self._comprehensive_evaluation(best_models, X_test, y_test)
            
            training_log['end_time'] = datetime.now()
            training_log['duration'] = (training_log['end_time'] - training_log['start_time']).total_seconds()
            training_log['final_results'] = evaluation_results
            
            return {
                'models': best_models,
                'evaluation': evaluation_results,
                'training_log': training_log
            }
            
        except Exception as e:
            training_log['error'] = str(e)
            training_log['end_time'] = datetime.now()
            logger.error(f"Error en entrenamiento optimizado: {e}")
            raise
    
    def _log_step(self, training_log: Dict, step_name: str):
        """Registra paso del entrenamiento"""
        training_log['steps'].append({
            'step': step_name,
            'timestamp': datetime.now(),
            'memory_usage': self._get_memory_usage()
        })
        logger.info(f"Entrenamiento: {step_name}")
```

### 2. Endpoint de Entrenamiento Mejorado

```python
@ml_router.post("/train/optimized", summary="Entrenamiento optimizado de modelos ML")
async def train_optimized_models(
    training_request: OptimizedTrainingRequest,
    background_tasks: BackgroundTasks,
    api_key: str = Depends(verify_api_key)
):
    """Entrena modelos ML con t√©cnicas optimizadas"""
    try:
        # Validaciones de seguridad
        if api_key != "admin-key":
            raise HTTPException(status_code=403, detail="Acceso denegado")
        
        # Validaciones de datos
        if len(training_request.scan_results) < 100:
            raise HTTPException(
                status_code=400, 
                detail="Se requieren al menos 100 muestras para entrenamiento optimizado"
            )
        
        # Configurar entrenamiento
        trainer_config = {
            'augmentation_factor': training_request.augmentation_factor,
            'cross_validation_folds': training_request.cv_folds,
            'hyperparameter_optimization': training_request.optimize_hyperparams,
            'feature_selection': training_request.feature_selection,
            'ensemble_methods': training_request.ensemble_methods
        }
        
        # Ejecutar entrenamiento optimizado
        background_tasks.add_task(
            _run_optimized_training,
            training_request.scan_results,
            training_request.target_domains,
            trainer_config
        )
        
        return {
            "message": "Entrenamiento optimizado iniciado",
            "samples_count": len(training_request.scan_results),
            "config": trainer_config,
            "estimated_duration_minutes": estimate_training_time(len(training_request.scan_results)),
            "timestamp": datetime.now().isoformat()
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error en entrenamiento optimizado: {e}")
        raise HTTPException(status_code=500, detail=f"Error interno: {str(e)}")

async def _run_optimized_training(scan_results, target_domains, config):
    """Ejecuta entrenamiento optimizado en background"""
    try:
        trainer = OptimizedMLTrainer(config)
        
        # Convertir datos
        events = []
        for scan_result, domain in zip(scan_results, target_domains):
            event = convert_scan_to_security_event(scan_result, domain)
            events.append(event)
        
        # Entrenar con mejores pr√°cticas
        results = trainer.train_with_best_practices(events)
        
        # Guardar resultados
        save_training_results(results)
        
        logger.info("Entrenamiento optimizado completado exitosamente")
        
    except Exception as e:
        logger.error(f"Error en entrenamiento background: {e}")
```

## üéØ Recomendaciones Espec√≠ficas

### Para Datos de Pentesting:

1. **Diversidad de Objetivos**: Incluir diferentes tipos de aplicaciones (web, API, m√≥vil)
2. **Variedad Temporal**: Datos de diferentes per√≠odos para capturar evoluci√≥n de amenazas
3. **Balanceo de Severidad**: Equilibrar vulnerabilidades cr√≠ticas, altas, medias y bajas
4. **Contexto Geogr√°fico**: Incluir datos de diferentes regiones y proveedores
5. **Tipos de Ataque**: Cubrir OWASP Top 10, ataques de infraestructura, etc.

### M√©tricas de Calidad de Datos:

```python
def assess_data_quality(events: List[SecurityEvent]) -> Dict[str, float]:
    """Eval√∫a calidad de datos de entrenamiento"""
    
    quality_metrics = {
        'completeness': calculate_completeness(events),
        'consistency': calculate_consistency(events),
        'diversity': calculate_diversity(events),
        'balance': calculate_class_balance(events),
        'temporal_coverage': calculate_temporal_coverage(events),
        'feature_correlation': calculate_feature_correlation(events)
    }
    
    # Puntuaci√≥n general
    quality_metrics['overall_score'] = np.mean(list(quality_metrics.values()))
    
    return quality_metrics
```

## üìà Monitoreo Continuo

```python
class ModelPerformanceMonitor:
    def __init__(self):
        self.performance_history = []
        self.drift_detector = DataDriftDetector()
        
    def monitor_model_performance(self, model, new_data, predictions):
        """Monitorea rendimiento del modelo en producci√≥n"""
        
        # Detectar drift en datos
        drift_score = self.drift_detector.detect_drift(new_data)
        
        # Calcular m√©tricas de rendimiento
        performance_metrics = self._calculate_performance_metrics(predictions)
        
        # Determinar si necesita reentrenamiento
        needs_retraining = self._should_retrain(drift_score, performance_metrics)
        
        return {
            'drift_score': drift_score,
            'performance': performance_metrics,
            'needs_retraining': needs_retraining,
            'timestamp': datetime.now()
        }
```

Este documento proporciona una gu√≠a completa para optimizar el entrenamiento de modelos ML con mejores datos, t√©cnicas avanzadas y mejores pr√°cticas espec√≠ficas para sistemas de pentesting.