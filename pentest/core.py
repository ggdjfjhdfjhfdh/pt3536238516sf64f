"""Pipeline principal del escáner de seguridad con notificaciones de progreso."""

from __future__ import annotations
import json
import logging
import os
import shutil
import sys
import tempfile
import datetime as dt
import asyncio
from pathlib import Path
from typing import Dict, Optional, Callable, List, Any, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed, Future
from dataclasses import dataclass, field
from enum import Enum
import redis
from contextlib import contextmanager

# --- módulos de tu proyecto -----------------------------------------------
from pentest.config import REDIS_URL, SAFE_DOMAIN
from pentest.exceptions import ScanError
from pentest.recon import recon
from pentest.fingerprint import fingerprint
from pentest.nuclei_scan import nuclei_scan
from pentest.leaks import check_leaks
from pentest.typosquat import check_typosquats
from pentest.cve_scan import cve_scan
from pentest.nmap_scan import nmap_scan
from pentest.security_config import security_config_scan
from pentest.dir_brute import dir_brute_scan
from pentest.threat_intel import check_threat_feeds
from pentest.screenshots import take_screenshots
from pentest.report import build_pdf, send_notification
from pentest.cisa_kev import cisa_kev_monitor
from pentest.greynoise import is_ip_malicious_greynoise

# ---------------------------------------------------------------------------
# CONFIGURACIÓN Y TIPOS
# ---------------------------------------------------------------------------

class ScanStage(Enum):
    """Estados posibles del escaneo."""
    QUEUED = "queued"
    WORKING = "working"
    COMPLETED = "completed"
    FAILED = "failed"

@dataclass
class ScanProgress:
    """Modelo para el progreso del escaneo."""
    job_id: str
    stage: ScanStage
    step: str
    percentage: int
    error: Optional[str] = None
    extra_data: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        """Convierte el progreso a diccionario para serialización."""
        return {
            "state": self.stage.value,
            "step": self.step,
            "pct": self.percentage,
            "error": self.error,
            **self.extra_data
        }

@dataclass
class ScanStep:
    """Modelo para un paso del escaneo."""
    key: str
    percentage: int
    runner: Callable
    dependencies: List[str] = field(default_factory=list)
    parallel: bool = True

@dataclass
class ScanResult:
    """Resultado de un escaneo completo."""
    status: str
    job_id: str
    report_path: Optional[str] = None
    error: Optional[str] = None
    metrics: Dict[str, int] = field(default_factory=dict)

# ---------------------------------------------------------------------------
# CONFIGURACIÓN DE LOGGING MEJORADA
# ---------------------------------------------------------------------------

class DeduplicateFilter(logging.Filter):
    """Filtro optimizado para deduplicar mensajes de log repetidos."""
    
    def __init__(self, name: str = '', interval: int = 5, max_entries: int = 1000):
        super().__init__(name)
        self.last_messages: Dict[Tuple[str, str, str], dt.datetime] = {}
        self.deduplication_interval = interval
        self.max_entries = max_entries

    def filter(self, record: logging.LogRecord) -> bool:
        """Filtra mensajes duplicados con limpieza periódica."""
        message = record.getMessage()
        unique_key = (record.levelname, record.name, message)
        
        now = dt.datetime.now()
        
        # Limpieza periódica del cache
        if len(self.last_messages) > self.max_entries:
            cutoff = now - dt.timedelta(seconds=self.deduplication_interval * 2)
            self.last_messages = {
                k: v for k, v in self.last_messages.items() if v > cutoff
            }
        
        if unique_key in self.last_messages:
            last_time = self.last_messages[unique_key]
            if (now - last_time).total_seconds() < self.deduplication_interval:
                return False
        
        self.last_messages[unique_key] = now
        return True

def setup_logging():
    """Configuración optimizada del sistema de logging."""
    logging.basicConfig(
        stream=sys.stdout,
        level=logging.INFO,  # Cambiar a INFO para reducir verbosidad
        format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S"
    )
    
    # Aplicar filtro de deduplicación
    dedupe_filter = DeduplicateFilter()
    for handler in logging.root.handlers:
        handler.addFilter(dedupe_filter)

# ---------------------------------------------------------------------------
# GESTIÓN DE REDIS MEJORADA
# ---------------------------------------------------------------------------

class RedisManager:
    """Gestor optimizado para operaciones Redis con pool de conexiones."""
    
    def __init__(self, url: str):
        self.pool = redis.ConnectionPool.from_url(
            url, 
            max_connections=20,
            retry_on_timeout=True,
            socket_keepalive=True,
            socket_keepalive_options={}
        )
        self._redis = redis.Redis(connection_pool=self.pool)
    
    def publish_progress(self, progress: ScanProgress) -> None:
        """Publica progreso con manejo de errores mejorado."""
        try:
            payload = json.dumps(progress.to_dict())
            pipe = self._redis.pipeline()
            pipe.publish(progress.job_id, payload)
            pipe.hset(f"rq:job:{progress.job_id}", "meta", 
                     json.dumps({"progress": progress.to_dict()}))
            pipe.execute()
        except redis.ConnectionError:
            logging.getLogger(__name__).warning(
                f"Redis connection failed for job {progress.job_id}"
            )
        except Exception as e:
            logging.getLogger(__name__).error(
                f"Unexpected Redis error for job {progress.job_id}: {e}"
            )
    
    def get_job_progress(self, job_id: str) -> Optional[Dict]:
        """Obtiene el progreso de un trabajo."""
        try:
            meta = self._redis.hget(f"rq:job:{job_id}", "meta")
            if meta:
                return json.loads(meta).get("progress")
        except Exception:
            pass
        return None
    
    def blpop(self, key: str, timeout: int = 0) -> Tuple[str, str]:
        """Operación BLPOP con manejo mejorado."""
        return self._redis.blpop(key, timeout)

# ---------------------------------------------------------------------------
# PIPELINE DE ESCANEO OPTIMIZADO
# ---------------------------------------------------------------------------

class ScanPipeline:
    """Pipeline optimizado para escaneos de seguridad."""
    
    def __init__(self):
        self.redis_manager = RedisManager(REDIS_URL)
        self.logger = logging.getLogger(self.__class__.__name__)
        self._setup_steps()
    
    def _setup_steps(self):
        """Configuración de pasos con dependencias explícitas."""
        self.steps = {
            "recon": ScanStep(
                key="recon", 
                percentage=10, 
                runner=recon,
                parallel=False  # Debe ejecutarse primero
            ),
            "finger": ScanStep(
                key="finger", 
                percentage=30, 
                runner=fingerprint,
                dependencies=["recon"],
                parallel=False  # Depende de recon
            ),
            "nuclei": ScanStep(
                key="nuclei", 
                percentage=55, 
                runner=nuclei_scan,
                dependencies=["finger"]
            ),
                "leaks": ScanStep(
                key="leaks", 
                percentage=85, 
                runner=check_leaks
            ),
            "typos": ScanStep(
                key="typos", 
                percentage=90, 
                runner=check_typosquats
            ),
            "cve": ScanStep(
                key="cve", 
                percentage=95, 
                runner=cve_scan,
                dependencies=["finger"]
            ),
            "nmap": ScanStep(
                key="nmap", 
                percentage=96, 
                runner=nmap_scan,
                dependencies=["finger"]
            ),
            "cisa_kev": ScanStep(
                key="cisa_kev", 
                percentage=97, 
                runner=cisa_kev_monitor
            ),
            "greynoise": ScanStep(
                key="greynoise", 
                percentage=98, 
                runner=is_ip_malicious_greynoise
            ),
            "security_config": ScanStep(
                key="security_config", 
                percentage=98.5, 
                runner=security_config_scan,
                dependencies=["finger"]
            ),
            "dir_brute": ScanStep(
                key="dir_brute", 
                percentage=99, 
                runner=dir_brute_scan,
                dependencies=["finger"]
            ),
            "screenshots": ScanStep(
                key="screenshots", 
                percentage=99.5, 
                runner=take_screenshots,
                dependencies=["finger"]
            ),
        }

    def _tls_scan_wrapper(self, *args, **kwargs):
        """Wrapper para tls_scan - implementar según tu lógica."""
        # Implementar la función tls_scan que falta en tu código original
        # Por ahora retorna un archivo vacío
        return Path(args[1]) / "tls_results.json"

    @contextmanager
    def _temp_directory(self, domain: str):
        """Context manager para manejo seguro de directorios temporales."""
        tmp_dir = Path(tempfile.mkdtemp(prefix=f"scan_{domain}_"))
        self.logger.info(f"Created temporary directory: {tmp_dir}")
        try:
            yield tmp_dir
        finally:
            if tmp_dir.exists():
                shutil.rmtree(tmp_dir)
                self.logger.info(f"Cleaned up temporary directory: {tmp_dir}")

    def _validate_domain(self, domain: str) -> None:
        """Validación mejorada del dominio."""
        if not domain or not isinstance(domain, str):
            raise ScanError("Dominio debe ser una cadena no vacía")
        
        if not SAFE_DOMAIN.match(domain):
            raise ScanError(f"Dominio inválido: {domain}")

    def _update_progress(self, job_id: str, step_key: str, percentage: int, 
                        stage: ScanStage = ScanStage.WORKING, 
                        error: str = None, **extra) -> None:
        """Actualiza el progreso del escaneo."""
        progress = ScanProgress(
            job_id=job_id,
            stage=stage,
            step=step_key,
            percentage=percentage,
            error=error,
            extra_data=extra
        )
        self.redis_manager.publish_progress(progress)

    def _execute_step(self, step: ScanStep, job_id: str, domain: str, 
                     tmp_dir: Path, files: Dict[str, Path], 
                     **kwargs) -> Tuple[str, Any]:
        """Ejecuta un paso individual del pipeline."""
        self._update_progress(job_id, step.key, step.percentage)
        self.logger.info(f"[{domain}] Ejecutando paso: {step.key} ({step.percentage}%)")
        
        try:
            # Preparar argumentos según el paso
            args, step_kwargs = self._prepare_step_args(
                step.key, domain, tmp_dir, files, kwargs
            )
            
            result = step.runner(*args, **step_kwargs)
            self.logger.info(f"[{domain}] Completado: {step.key}")
            return step.key, result
            
        except Exception as e:
            error_msg = f"Error en {step.key}: {str(e)}"
            self.logger.error(f"[{domain}] {error_msg}")
            raise ScanError(error_msg) from e

    def _prepare_step_args(self, step_key: str, domain: str, tmp_dir: Path, 
                          files: Dict[str, Path], kwargs: Dict) -> Tuple[tuple, Dict]:
        """Prepara argumentos específicos para cada paso."""
        args = []
        step_kwargs = {}
        
        if step_key == "recon":
            args = [domain, tmp_dir]
        elif step_key == "finger":
            args = [files.get("subdomains"), tmp_dir]
        elif step_key == "leaks":
            args = [domain, tmp_dir, kwargs.get("hibp_api_key")]
        elif step_key == "nuclei":
            args = [files.get("httpx"), tmp_dir, kwargs.get("full_nuclei_scan", False)]
        elif step_key == "nmap":
            args = [files.get("httpx"), tmp_dir, True]
        elif step_key == "typos":
            args = [domain, tmp_dir]
        elif step_key == "dir_brute":
            args = [files.get("httpx"), tmp_dir]
            step_kwargs.update(self._get_dir_brute_config(files.get("httpx")))
        elif step_key in ["tls", "cve", "security_config", "screenshots"]:
            args = [files.get("httpx"), tmp_dir]
        elif step_key == "greynoise":
            args = ["", os.environ.get("GREYNOISE_API_KEY")]
        elif step_key == "cisa_kev":
            # Implementar lógica para detected_tech_stack
            args = [None]  # Placeholder
        
        return tuple(args), step_kwargs

    def _get_dir_brute_config(self, httpx_file: Optional[Path]) -> Dict:
        """Configuración optimizada para directory brute force."""
        config = {
            'extensions': ['php', 'html', 'js', 'asp', 'aspx', 'txt', 'zip', 'rar', 'tar.gz'],
            'auto_extend': True,
            'max_concurrent_requests': 50,  # Reducido para evitar sobrecarga
            'delay_between_requests': 0.01,
            'filter_status_codes': [301, 404],
            'filter_content_length': None,
            'tech_stack': None
        }
        
        # Detectar tech stack de los resultados de httpx
        if httpx_file and httpx_file.exists():
            try:
                with open(httpx_file, "r") as f:
                    httpx_results = json.load(f)
                    
                for host_result in httpx_results:
                    tech_list = host_result.get("tech", [])
                    for tech in tech_list:
                        tech_lower = tech.lower()
                        if "wordpress" in tech_lower:
                            config['tech_stack'] = "wordpress"
                            return config
                        elif "laravel" in tech_lower:
                            config['tech_stack'] = "laravel"
                            return config
                    
                    # Si no hay tecnología prioritaria, usar la primera
                    if tech_list and not config['tech_stack']:
                        config['tech_stack'] = tech_list[0]
                        
            except (json.JSONDecodeError, IOError) as e:
                self.logger.warning(f"Error leyendo httpx results: {e}")
        
        return config

    def _get_execution_order(self) -> List[List[str]]:
        """Determina el orden de ejecución basado en dependencias."""
        # Implementación simple por capas
        layers = [
            ["recon"],  # Capa 0: sin dependencias
            ["finger"],  # Capa 1: depende de recon
            # Capa 2: depende de finger y pueden ejecutarse en paralelo
            ["nuclei", "tls", "cve", "nmap", "security_config", "dir_brute", "screenshots"],
            # Capa 3: independientes
            ["leaks", "typos", "cisa_kev", "greynoise"]
        ]
        return layers

    def _safe_count_json(self, path: Optional[Path]) -> int:
        """Cuenta elementos en archivo JSON de forma segura."""
        if not path or not path.exists():
            return 0
        try:
            with open(path, "r") as f:
                data = json.load(f)
                return len(data) if isinstance(data, list) else 0
        except Exception:
            return 0

    def _count_lines(self, path: Optional[Path]) -> int:
        """Cuenta líneas no vacías en un archivo."""
        if not path or not path.exists():
            return 0
        try:
            with open(path, "r") as f:
                return len([line for line in f if line.strip()])
        except Exception:
            return 0

    def _enhance_with_threat_intel(self, nmap_file: Path) -> None:
        """Enriquece resultados de Nmap con threat intelligence."""
        if not nmap_file.exists():
            return
        
        try:
            with open(nmap_file, "r") as f:
                nmap_results = json.load(f)
            
            updated_results = []
            for host_data in nmap_results:
                ip_address = host_data.get("ip_address")
                if ip_address:
                    try:
                        threat_hits = check_threat_feeds(ip_address)
                        if threat_hits:
                            host_data["threat_intel"] = threat_hits
                    except Exception as e:
                        self.logger.warning(f"Error checking threat intel for {ip_address}: {e}")
                
                updated_results.append(host_data)
            
            with open(nmap_file, "w") as f:
                json.dump(updated_results, f, indent=2)
            
            self.logger.info("Threat intelligence added to Nmap results")
            
        except Exception as e:
            self.logger.error(f"Error enhancing Nmap results with threat intel: {e}")

    def generate_pdf(self, domain: str, recipient_email: str, job_id: str = None, *,
                     hibp_api_key: str = None, debug: bool = False, 
                     full_nuclei_scan: bool = False) -> ScanResult:
        """
        Ejecuta el pipeline completo de escaneo y genera el informe PDF.
        
        Args:
            domain: Dominio a escanear
            recipient_email: Email para notificaciones
            job_id: ID del trabajo (se genera automáticamente si no se proporciona)
            hibp_api_key: API key para Have I Been Pwned
            debug: Modo debug (mantiene archivos temporales)
            full_nuclei_scan: Ejecutar escaneo completo con Nuclei
            
        Returns:
            ScanResult con el resultado del escaneo
        """
        # Validaciones iniciales
        self._validate_domain(domain)
        
        if not job_id:
            job_id = f"standalone:{dt.datetime.utcnow().isoformat()}"
        
        self._update_progress(job_id, "queue", 5, ScanStage.QUEUED)
        
        try:
            with self._temp_directory(domain) as tmp_dir:
                files: Dict[str, Path] = {}
                execution_layers = self._get_execution_order()
                
                # Ejecutar por capas respetando dependencias
                for layer_idx, layer_steps in enumerate(execution_layers):
                    if layer_idx < 2:  # Capas secuenciales (0 y 1)
                        for step_key in layer_steps:
                            if step_key not in self.steps:
                                continue
                            
                            step = self.steps[step_key]
                            _, result = self._execute_step(
                                step, job_id, domain, tmp_dir, files,
                                hibp_api_key=hibp_api_key,
                                full_nuclei_scan=full_nuclei_scan
                            )
                            files[step_key] = result
                    
                    else:  # Capas paralelas (2 en adelante)
                        with ThreadPoolExecutor(max_workers=min(len(layer_steps), 6)) as executor:
                            future_to_step = {}
                            
                            for step_key in layer_steps:
                                if step_key not in self.steps:
                                    continue
                                
                                step = self.steps[step_key]
                                future = executor.submit(
                                    self._execute_step,
                                    step, job_id, domain, tmp_dir, files,
                                    hibp_api_key=hibp_api_key,
                                    full_nuclei_scan=full_nuclei_scan
                                )
                                future_to_step[future] = step_key
                            
                            # Procesar resultados
                            for future in as_completed(future_to_step):
                                step_key = future_to_step[future]
                                try:
                                    _, result = future.result()
                                    files[step_key] = result
                                except Exception as e:
                                    self.logger.error(f"Error in parallel step {step_key}: {e}")
                                    # Continuar con otros pasos
                
                # Enriquecer con threat intelligence
                if "nmap" in files:
                    self._enhance_with_threat_intel(files["nmap"])
                
                # Generar informe PDF
                self.logger.info(f"[{domain}] Generando informe PDF")
                pdf_path = build_pdf(
                    domain, recipient_email, tmp_dir,
                    httpx_file=files.get("finger"),
                    nuclei_file=files.get("nuclei"),
                    tls_file=files.get("tls"),
                    leaks_file=files.get("leaks"),
                    typosquats_file=files.get("typos"),
                    dir_brute_file=files.get("dir_brute"),
                    screenshots_file=files.get("screenshots"),
                    cves_file=files.get("cve"),
                    nmap_file=files.get("nmap"),
                    security_config_file=files.get("security_config"),
                    subdomains_file=files.get("recon")
                )
                
                # Calcular métricas
                metrics = {
                    "subdomains": self._count_lines(files.get("recon")),
                    "vulnerabilities": self._safe_count_json(files.get("nuclei")),
                    "cves": self._safe_count_json(files.get("cve")),
                    "ports": self._safe_count_json(files.get("nmap"))
                }
                
                self._update_progress(
                    job_id, "report", 100, ScanStage.COMPLETED,
                    report_path=str(pdf_path)
                )
                
                # Enviar notificación de éxito
                send_notification(
                    job_id=job_id,
                    status_message="Escaneo completado exitosamente",
                    status_type="success",
                    pdf_path=pdf_path,
                    recipient_email=recipient_email,
                    subc=metrics["subdomains"],
                    vulc=metrics["vulnerabilities"],
                    domain=domain
                )
                
                self.logger.info(f"[{domain}] Escaneo completado exitosamente")
                
                return ScanResult(
                    status="success",
                    job_id=job_id,
                    report_path=str(pdf_path),
                    metrics=metrics
                )
                
        except ScanError as e:
            self.logger.error(f"[{domain}] Error durante el escaneo: {e}")
            self._update_progress(job_id, "failed", 100, ScanStage.FAILED, str(e))
            
            send_notification(
                job_id=job_id,
                status_message=f"Escaneo fallido: {str(e)}",
                status_type="failed",
                pdf_path=None,
                recipient_email=recipient_email,
                subc=0, vulc=0, domain=domain
            )
            
            return ScanResult(status="failed", job_id=job_id, error=str(e))
            
        except Exception as e:
            self.logger.error(f"[{domain}] Error inesperado: {e}")
            self._update_progress(job_id, "failed", 100, ScanStage.FAILED, str(e))
            
            send_notification(
                job_id=job_id,
                status_message=f"Error inesperado: {str(e)}",
                status_type="failed",
                pdf_path=None,
                recipient_email=recipient_email,
                subc=0, vulc=0, domain=domain
            )
            
            return ScanResult(status="failed", job_id=job_id, error=str(e))

# ---------------------------------------------------------------------------
# WORKER MEJORADO
# ---------------------------------------------------------------------------

class ScanWorker:
    """Worker optimizado para procesar escaneos."""
    
    def __init__(self):
        self.pipeline = ScanPipeline()
        self.redis_manager = RedisManager(REDIS_URL)
        self.logger = logging.getLogger(self.__class__.__name__)
        self.running = False
    
    def start(self, queue_name: str = "scan_queue"):
        """Inicia el worker para procesar la cola de escaneos."""
        self.running = True
        self.logger.info(f"Worker iniciado, escuchando cola: {queue_name}")
        
        while self.running:
            try:
                self.logger.debug(f"Esperando mensajes en cola '{queue_name}'...")
                _, raw_message = self.redis_manager.blpop(queue_name, timeout=30)
                
                if not raw_message:
                    continue
                
                self._process_message(raw_message)
                
            except redis.ConnectionError:
                self.logger.error("Error de conexión Redis, reintentando en 5s...")
                import time
                time.sleep(5)
            except KeyboardInterrupt:
                self.logger.info("Worker detenido por usuario")
                self.running = False
            except Exception as e:
                self.logger.error(f"Error inesperado en worker: {e}")
    
    def _process_message(self, raw_message: bytes):
        """Procesa un mensaje de la cola."""
        job_id = f"job:{dt.datetime.utcnow().isoformat()}"
        
        try:
            self.logger.info(f"Procesando mensaje: {raw_message}")
            request = json.loads(raw_message)
            
            # Validar estructura del request
            if "domain" not in request or "email" not in request:
                raise ValueError("Request debe contener 'domain' y 'email'")
            
            # Marcar como en cola
            self.pipeline._update_progress(job_id, "queued", 0, ScanStage.QUEUED)
            
            # Ejecutar escaneo
            self.logger.info(f"Iniciando escaneo para dominio: {request['domain']}")
            result = self.pipeline.generate_pdf(
                domain=request["domain"],
                recipient_email=request["email"],
                job_id=job_id,
                hibp_api_key=request.get("hibp_api_key"),
                debug=request.get("debug", False),
                full_nuclei_scan=request.get("full_nuclei_scan", False)
            )
            
            self.logger.info(f"Escaneo completado para job_id: {job_id}, status: {result.status}")
            
        except json.JSONDecodeError as e:
            self.logger.error(f"Error decodificando JSON: {raw_message} - {e}")
        except ValueError as e:
            self.logger.error(f"Request inválido: {e}")
        except Exception as e:
            self.logger.error(f"Error procesando job {job_id}: {e}")
    
    def stop(self):
        """Detiene el worker de forma controlada."""
        self.running = False
        self.logger.info("Deteniendo worker...")

# ---------------------------------------------------------------------------
# FUNCIONES DE COMPATIBILIDAD
# ---------------------------------------------------------------------------

# Mantener compatibilidad con el código existente
def generate_pdf(domain: str, recipient_email: str, job_id: str = None, **kwargs) -> Dict:
    """Función de compatibilidad para generate_pdf."""
    pipeline = ScanPipeline()
    result = pipeline.generate_pdf(domain, recipient_email, job_id, **kwargs)
    
    return {
        "status": result.status,
        "job_id": result.job_id,
        "report_path": result.report_path,
        "error": result.error
    }

def start_worker():
    """Función de compatibilidad para start_worker."""
    worker = ScanWorker()
    worker.start()

# ---------------------------------------------------------------------------
# PUNTO DE ENTRADA
# ---------------------------------------------------------------------------

if __name__ == "__main__":
    setup_logging()
    
    if len(sys.argv) > 1 and sys.argv[1] == "worker":
        start_worker()
    else:
        # Modo de prueba
        import argparse
        parser = argparse.ArgumentParser(description="Scanner de seguridad")
        parser.add_argument("--domain", required=True, help="Dominio a escanear")
        parser.add_argument("--email", required=True, help="Email para notificaciones")
        parser.add_argument("--debug", action="store_true", help="Modo debug")
        
        args = parser.parse_args()
        
        pipeline = ScanPipeline()
        result = pipeline.generate_pdf(
            domain=args.domain,
            recipient_email=args.email,
            debug=args.debug
        )
        
        print(f"Resultado: {result.status}")
        if result.report_path:
            print(f"Informe: {result.report_path}")