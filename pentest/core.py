"""Pipeline principal del escÃ¡ner de seguridad con notificaciones de progreso."""

from __future__ import annotations
import json
from enhanced_integration import EnhancedTechIntegrator
import logging
import os
import shutil
import sys
import tempfile
import datetime as dt
import asyncio
import redis
import time
from pathlib import Path
from typing import Dict, Optional, Callable, List, Any, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed, Future
from dataclasses import dataclass, field
from enum import Enum
from contextlib import contextmanager
from rq import Queue, Worker


# --- mÃ³dulos de tu proyecto -----------------------------------------------
from pentest.config import REDIS_URL, SAFE_DOMAIN, DEFAULT_TIMEOUT
from pentest.exceptions import ScanError
from pentest.recon import recon
from pentest.fingerprint import fingerprint
from pentest.nuclei_scan import nuclei_scan
from pentest.leaks import check_leaks
from pentest.typosquat import check_typosquats
from pentest.cve_scan import cve_scan
from pentest.nmap_scan import nmap_scan
from pentest.security_config import security_config_scan
from pentest.dir_brute import dir_brute_scan
from pentest.threat_intel import check_threat_feeds

from pentest.report import build_pdf, send_notification
from pentest.cisa_kev import cisa_kev_monitor
from pentest.greynoise import is_ip_malicious_greynoise
from pentest.premium_adaptive_scan import premium_adaptive_scan_wrapper

# Importaciones del sistema ML predictivo
try:
    from pentest.ml_predictive_analysis import MLPredictiveAnalyzer
    from pentest.ml_integration import MLIntegrationManager, enhance_scan_with_ml
    ML_AVAILABLE = True
except ImportError as e:
    logging.getLogger(__name__).warning(f"Sistema ML no disponible: {e}")
    ML_AVAILABLE = False
    MLPredictiveAnalyzer = None
    MLIntegrationManager = None
    enhance_scan_with_ml = None

# Importaciones del sistema WAF
try:
    from pentest.waf_integration import WAFIntegratedScanner
    from pentest.waf_handler import WAFType
    WAF_AVAILABLE = True
except ImportError as e:
    logging.getLogger(__name__).warning(f"Sistema WAF no disponible: {e}")
    WAF_AVAILABLE = False
    WAFIntegratedScanner = None
    WAFType = None

# ---------------------------------------------------------------------------
# CONFIGURACIÃ“N Y TIPOS
# ---------------------------------------------------------------------------

class ScanStage(Enum):
    """Estados posibles del escaneo."""
    QUEUED = "queued"
    WORKING = "working"
    COMPLETED = "completed"
    FAILED = "failed"

@dataclass
class ScanProgress:
    """Modelo para el progreso del escaneo."""
    job_id: str
    stage: ScanStage
    step: str
    percentage: int
    error: Optional[str] = None
    extra_data: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        """Convierte el progreso a diccionario para serializaciÃ³n."""
        return {
            "state": self.stage.value,
            "step": self.step,
            "pct": self.percentage,
            "error": self.error,
            **self.extra_data
        }

@dataclass
class ScanStep:
    """Modelo para un paso del escaneo."""
    key: str
    percentage: int
    runner: Callable
    dependencies: List[str] = field(default_factory=list)
    parallel: bool = True

@dataclass
class ScanResult:
    """Resultado de un escaneo completo."""
    success: bool
    message: str
    job_id: str
    report_path: Optional[str] = None
    error: Optional[str] = None
    metrics: Dict[str, int] = field(default_factory=dict)

# ---------------------------------------------------------------------------
# CONFIGURACIÃ“N DE LOGGING MEJORADA
# ---------------------------------------------------------------------------

class DeduplicateFilter(logging.Filter):
    """Filtro optimizado para deduplicar mensajes de log repetidos."""
    
    def __init__(self, name: str = '', interval: int = 5, max_entries: int = 1000):
        super().__init__(name)
        self.last_messages: Dict[Tuple[str, str, str], dt.datetime] = {}
        self.deduplication_interval = interval
        self.max_entries = max_entries

    def filter(self, record: logging.LogRecord) -> bool:
        """Filtra mensajes duplicados con limpieza periÃ³dica."""
        message = record.getMessage()
        unique_key = (record.levelname, record.name, message)
        
        now = dt.datetime.now()
        
        # Limpieza periÃ³dica del cache
        if len(self.last_messages) > self.max_entries:
            cutoff = now - dt.timedelta(seconds=self.deduplication_interval * 2)
            self.last_messages = {
                k: v for k, v in self.last_messages.items() if v > cutoff
            }
        
        if unique_key in self.last_messages:
            last_time = self.last_messages[unique_key]
            if (now - last_time).total_seconds() < self.deduplication_interval:
                return False
        
        self.last_messages[unique_key] = now
        return True

def setup_logging():
    """ConfiguraciÃ³n optimizada del sistema de logging."""
    logging.basicConfig(
        stream=sys.stdout,
        level=logging.DEBUG,  # Cambiar a DEBUG para obtener mÃ¡s detalles
        format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S"
    )
    
    # Aplicar filtro de deduplicaciÃ³n (desactivado temporalmente para depuraciÃ³n)
    # dedupe_filter = DeduplicateFilter()
    # for handler in logging.root.handlers:
    #     handler.addFilter(dedupe_filter)

# ---------------------------------------------------------------------------
# GESTIÃ“N DE REDIS MEJORADA
# ---------------------------------------------------------------------------

def get_redis_connection():
    return redis.Redis.from_url(REDIS_URL)

class RedisManager:
    """Gestor optimizado para operaciones Redis con pool de conexiones."""
    
    def __init__(self, url: str):
        self.pool = redis.ConnectionPool.from_url(
            url, 
            max_connections=20,
            retry_on_timeout=True,
            socket_keepalive=True,
            socket_keepalive_options={}
        )
        self._redis = redis.Redis(connection_pool=self.pool)
    
    def publish_progress(self, progress: ScanProgress) -> None:
        """Publica progreso con manejo de errores mejorado."""
        try:
            payload = json.dumps(progress.to_dict())
            pipe = self._redis.pipeline()
            pipe.publish(progress.job_id, payload)
            pipe.hset(f"rq:job:{progress.job_id}", "meta", 
                     json.dumps({"progress": progress.to_dict()}))
            pipe.execute()
        except redis.ConnectionError:
            logging.getLogger(__name__).warning(
                f"Redis connection failed for job {progress.job_id}"
            )
        except Exception as e:
            logging.getLogger(__name__).error(
                f"Unexpected Redis error for job {progress.job_id}: {e}"
            )
    
    def get_job_progress(self, job_id: str) -> Optional[Dict]:
        """Obtiene el progreso de un trabajo."""
        try:
            meta = self._redis.hget(f"rq:job:{job_id}", "meta")
            if meta:
                return json.loads(meta).get("progress")
        except Exception:
            pass
        return None
    
    def blpop(self, key: str, timeout: int = 0) -> Tuple[str, str]:
        """OperaciÃ³n BLPOP con manejo mejorado."""
        return self._redis.blpop(key, timeout)

# ---------------------------------------------------------------------------
# PIPELINE DE ESCANEO OPTIMIZADO
# ---------------------------------------------------------------------------

class ScanPipeline:
    """Pipeline optimizado para escaneos de seguridad."""
    
    def __init__(self):
        self.redis_manager = RedisManager(REDIS_URL)
        self.logger = logging.getLogger(self.__class__.__name__)
        self._setup_steps()
        
        # Inicializar sistema ML si estÃ¡ disponible
        self.ml_manager = None
        if ML_AVAILABLE:
            try:
                self.ml_manager = MLIntegrationManager()
                self.logger.info("ğŸ¤– [ML] Sistema ML predictivo inicializado")
            except Exception as e:
                self.logger.warning(f"ğŸ¤– [ML] Error inicializando sistema ML: {e}")
                self.ml_manager = None
        else:
            self.logger.info("ğŸ¤– [ML] Sistema ML no disponible")
        
        # Inicializar sistema WAF si estÃ¡ disponible
        self.waf_scanner = None
        self.detected_waf = None
        if WAF_AVAILABLE:
            try:
                self.waf_scanner = WAFIntegratedScanner()
                self.logger.info("ğŸ›¡ï¸ [WAF] Sistema WAF inicializado")
            except Exception as e:
                self.logger.warning(f"ğŸ›¡ï¸ [WAF] Error inicializando sistema WAF: {e}")
                self.waf_scanner = None
        else:
            self.logger.info("ğŸ›¡ï¸ [WAF] Sistema WAF no disponible")
        
        self.logger.info("ğŸš€ [PIPELINE] ScanPipeline inicializado")
    
    def _setup_steps(self):
        """ConfiguraciÃ³n de pasos con dependencias explÃ­citas."""
        self.steps = {
            "waf_detection": ScanStep(
                key="waf_detection", 
                percentage=5, 
                runner=self._detect_waf,
                parallel=False  # Debe ejecutarse primero
            ),
            "recon": ScanStep(
                key="recon", 
                percentage=10, 
                runner=recon,
                dependencies=["waf_detection"],
                parallel=False  # Depende de WAF detection
            ),
            "finger": ScanStep(
                key="finger", 
                percentage=30, 
                runner=fingerprint,
                dependencies=["recon"],
                parallel=False  # Depende de recon
            ),
            "nuclei": ScanStep(
                key="nuclei", 
                percentage=55, 
                runner=nuclei_scan,
                dependencies=["finger"]
            ),
            "leaks": ScanStep(
                key="leaks", 
                percentage=85, 
                runner=check_leaks
            ),
            "typos": ScanStep(
                key="typos", 
                percentage=90, 
                runner=check_typosquats
            ),
            "cve": ScanStep(
                key="cve", 
                percentage=95, 
                runner=cve_scan,
                dependencies=["finger"]
            ),
            "nmap": ScanStep(
                key="nmap", 
                percentage=96, 
                runner=nmap_scan,
                dependencies=["finger"]
            ),
            "cisa_kev": ScanStep(
                key="cisa_kev", 
                percentage=97, 
                runner=cisa_kev_monitor
            ),
            "greynoise": ScanStep(
                key="greynoise", 
                percentage=98, 
                runner=is_ip_malicious_greynoise
            ),
            "security_config": ScanStep(
                key="security_config", 
                percentage=98.5, 
                runner=security_config_scan,
                dependencies=["finger"]
            ),
            "dir_brute": ScanStep(
                key="dir_brute", 
                percentage=99, 
                runner=dir_brute_scan,
                dependencies=["finger"]
            ),
            "premium_adaptive": ScanStep(
                key="premium_adaptive", 
                percentage=99.5, 
                runner=premium_adaptive_scan_wrapper,
                dependencies=["finger"]
            ),
            "ml_analysis": ScanStep(
                key="ml_analysis", 
                percentage=99.8, 
                runner=self._run_ml_analysis,
                dependencies=["finger", "nuclei", "cve"],
                parallel=False  # Debe ejecutarse despuÃ©s de otros anÃ¡lisis
            ),
        }



    @contextmanager
    def _temp_directory(self, domain: str):
        """Context manager para manejo seguro de directorios temporales."""
        tmp_dir = Path(tempfile.mkdtemp(prefix=f"scan_{domain}_"))
        self.logger.info(f"Created temporary directory: {tmp_dir}")
        try:
            yield tmp_dir
        finally:
            if tmp_dir.exists():
                shutil.rmtree(tmp_dir)
                self.logger.info(f"Cleaned up temporary directory: {tmp_dir}")

    def _validate_domain(self, domain: str) -> None:
        """ValidaciÃ³n mejorada del dominio."""
        if not domain or not isinstance(domain, str):
            raise ScanError("Dominio debe ser una cadena no vacÃ­a")
        
        if not SAFE_DOMAIN.match(domain):
            raise ScanError(f"Dominio invÃ¡lido: {domain}")

    def _update_progress(self, job_id: str, step_key: str, percentage: int, 
                        stage: ScanStage = ScanStage.WORKING, 
                        error: str = None, **extra) -> None:
        """Actualiza el progreso del escaneo."""
        progress = ScanProgress(
            job_id=job_id,
            stage=stage,
            step=step_key,
            percentage=percentage,
            error=error,
            extra_data=extra
        )
        self.redis_manager.publish_progress(progress)

    def _execute_step(self, step: ScanStep, job_id: str, domain: str, 
                     tmp_dir: Path, files: Dict[str, Path], 
                     **kwargs) -> Tuple[str, Any]:
        """Ejecuta un paso individual del pipeline."""
        self._update_progress(job_id, step.key, step.percentage)
        self.logger.info(f"ğŸš€ [STEP] [{domain}] Ejecutando paso: {step.key} ({step.percentage}%)")
        
        # Configurar timeout especÃ­fico por paso
        step_timeout = self._get_step_timeout(step.key)
        self.logger.info(f"ğŸš€ [STEP] [{domain}] Timeout configurado para {step.key}: {step_timeout}s")
        
        try:
            # Preparar argumentos segÃºn el paso
            args, step_kwargs = self._prepare_step_args(
                step.key, domain, tmp_dir, files, kwargs
            )
            
            self.logger.info(f"ğŸš€ [STEP] [{domain}] Llamando a {step.runner.__name__} con args={args} y kwargs={step_kwargs}")
            
            # Ejecutar con timeout usando ThreadPoolExecutor
            with ThreadPoolExecutor(max_workers=1) as executor:
                if asyncio.iscoroutinefunction(step.runner):
                    # Ejecutar coroutine en un nuevo loop de eventos
                    self.logger.info(f"ğŸš€ [STEP] [{domain}] Ejecutando funciÃ³n async: {step.runner.__name__}")
                    future = executor.submit(self._run_async_step, step.runner, args, step_kwargs)
                else:
                    self.logger.info(f"ğŸš€ [STEP] [{domain}] Ejecutando funciÃ³n sync: {step.runner.__name__}")
                    future = executor.submit(step.runner, *args, **step_kwargs)
                
                try:
                    result = future.result(timeout=step_timeout)
                except TimeoutError:
                    self.logger.error(f"ğŸš€ [STEP] [{domain}] â° Timeout en {step.key} despuÃ©s de {step_timeout}s")
                    future.cancel()
                    raise ScanError(f"Timeout en paso {step.key} despuÃ©s de {step_timeout}s")
            
            self.logger.info(f"ğŸš€ [STEP] [{domain}] âœ… Completado: {step.key}, resultado: {result}")
            
            # Verificar que el resultado es vÃ¡lido
            if result and isinstance(result, Path):
                if result.exists():
                    self.logger.info(f"ğŸš€ [STEP] [{domain}] âœ… Archivo resultado verificado: {result} (tamaÃ±o: {result.stat().st_size} bytes)")
                else:
                    self.logger.warning(f"ğŸš€ [STEP] [{domain}] âš ï¸ Archivo resultado no existe: {result}")
            else:
                self.logger.info(f"ğŸš€ [STEP] [{domain}] Resultado no es un archivo: {type(result)} - {result}")
            
            return step.key, result
            
        except Exception as e:
            error_msg = f"Error en {step.key}: {str(e)}"
            self.logger.error(f"ğŸš€ [STEP] [{domain}] âŒ {error_msg}", exc_info=True)
            self._update_progress(job_id, step.key, step.percentage, stage=ScanStage.FAILED, error=error_msg)
            raise ScanError(error_msg) # Relanzar la excepciÃ³n para detener el pipeline

    def _get_step_timeout(self, step_key: str) -> int:
        """Obtiene el timeout especÃ­fico para cada paso del pipeline."""
        base_timeouts = {
            "waf_detection": 60,  # 1 minuto para detecciÃ³n de WAF
            "recon": 300,      # 5 minutos para reconocimiento
            "finger": 180,     # 3 minutos para fingerprinting (puede ser bloqueado por Cloudflare)
            "nuclei": 600,     # 10 minutos para Nuclei
            "nmap": 900,       # 15 minutos para Nmap
            "leaks": 120,      # 2 minutos para HIBP
            "typos": 180,      # 3 minutos para typosquatting
            "dir_brute": 600,  # 10 minutos para directory brute force
            "cve": 300,        # 5 minutos para CVE
            "security_config": 180,  # 3 minutos para security config
            "greynoise": 60,   # 1 minuto para GreyNoise
            "cisa_kev": 120,   # 2 minutos para CISA KEV
            "premium_adaptive": 900,  # 15 minutos para escaneo premium adaptativo
            "ml_analysis": 240  # 4 minutos para anÃ¡lisis ML
        }
        
        timeout = base_timeouts.get(step_key, DEFAULT_TIMEOUT)
        
        # Ajustar timeouts segÃºn el WAF detectado
        if self.detected_waf and step_key in ["finger", "nuclei", "dir_brute", "security_config"]:
            if self.detected_waf.get("type") in ["Cloudflare", "AWS WAF", "Akamai"]:
                timeout = int(timeout * 1.5)  # Aumentar timeout 50% para WAFs estrictos
                self.logger.info(f"ğŸ›¡ï¸ [WAF] Timeout ajustado para {step_key}: {timeout}s (WAF: {self.detected_waf.get('type')})")
        
        return timeout

    def _run_async_step(self, async_func, args, kwargs):
        """Ejecuta una funciÃ³n asÃ­ncrona en un nuevo loop de eventos."""
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            return loop.run_until_complete(async_func(*args, **kwargs))
        finally:
            loop.close()

    def _detect_waf(self, domain: str, tmp_dir: Path) -> Path:
        """Detecta el WAF del dominio objetivo y configura evasiÃ³n global."""
        self.logger.info(f"ğŸ›¡ï¸ [WAF] Iniciando detecciÃ³n de WAF para {domain}")
        
        waf_result_file = tmp_dir / "waf_detection.json"
        
        if not self.waf_scanner:
            self.logger.warning(f"ğŸ›¡ï¸ [WAF] Scanner WAF no disponible para {domain}")
            # Crear archivo de resultado vacÃ­o
            waf_result = {
                "domain": domain,
                "waf_detected": False,
                "waf_type": None,
                "confidence": 0,
                "details": "WAF scanner no disponible",
                "timestamp": dt.datetime.now().isoformat()
            }
            with open(waf_result_file, "w") as f:
                json.dump(waf_result, f, indent=2)
            return waf_result_file
        
        try:
            # Detectar WAF usando el scanner integrado
            waf_info = self.waf_scanner.detect_waf(f"https://{domain}")
            
            # Guardar informaciÃ³n del WAF detectado
            self.detected_waf = waf_info
            
            # Aplicar configuraciÃ³n de evasiÃ³n global
            if waf_info and waf_info.get("type"):
                self._apply_waf_evasion_config(waf_info)
            
            # Preparar resultado
            waf_result = {
                "domain": domain,
                "waf_detected": bool(waf_info and waf_info.get("type")),
                "waf_type": waf_info.get("type") if waf_info else None,
                "confidence": waf_info.get("confidence", 0) if waf_info else 0,
                "details": waf_info.get("details", "") if waf_info else "No WAF detectado",
                "timestamp": dt.datetime.now().isoformat(),
                "evasion_techniques": waf_info.get("recommended_techniques", []) if waf_info else []
            }
            
            self.logger.info(f"ğŸ›¡ï¸ [WAF] DetecciÃ³n completada para {domain}: {waf_result['waf_type'] or 'No detectado'}")
            
        except Exception as e:
            self.logger.error(f"ğŸ›¡ï¸ [WAF] Error detectando WAF para {domain}: {e}")
            waf_result = {
                "domain": domain,
                "waf_detected": False,
                "waf_type": None,
                "confidence": 0,
                "details": f"Error en detecciÃ³n: {str(e)}",
                "timestamp": dt.datetime.now().isoformat(),
                "error": str(e)
            }
        
        # Guardar resultado en archivo
        with open(waf_result_file, "w") as f:
            json.dump(waf_result, f, indent=2)
        
        return waf_result_file
    
    def _apply_waf_evasion_config(self, waf_info: Dict) -> None:
        """Aplica configuraciÃ³n de evasiÃ³n global basada en el WAF detectado."""
        waf_type = waf_info.get("type")
        if not waf_type:
            return
        
        self.logger.info(f"ğŸ›¡ï¸ [WAF] Aplicando configuraciÃ³n de evasiÃ³n para {waf_type}")
        
        # Configuraciones especÃ­ficas por tipo de WAF
        if waf_type == "Cloudflare":
            # ConfiguraciÃ³n para Cloudflare
            os.environ["WAF_EVASION_MODE"] = "cloudflare"
            os.environ["HTTP_DELAY"] = "2.0"  # Delay mÃ¡s alto
            os.environ["USER_AGENT_ROTATION"] = "true"
            
        elif waf_type == "AWS WAF":
            # ConfiguraciÃ³n para AWS WAF
            os.environ["WAF_EVASION_MODE"] = "aws_waf"
            os.environ["HTTP_DELAY"] = "1.5"
            os.environ["REQUEST_ENCODING"] = "url_encode"
            
        elif waf_type == "Akamai":
            # ConfiguraciÃ³n para Akamai
            os.environ["WAF_EVASION_MODE"] = "akamai"
            os.environ["HTTP_DELAY"] = "1.0"
            os.environ["HEADER_MANIPULATION"] = "true"
            
        else:
            # ConfiguraciÃ³n genÃ©rica
            os.environ["WAF_EVASION_MODE"] = "generic"
            os.environ["HTTP_DELAY"] = "1.0"
        
        self.logger.info(f"ğŸ›¡ï¸ [WAF] ConfiguraciÃ³n de evasiÃ³n aplicada para {waf_type}")

    def _prepare_step_args(self, step_key: str, domain: str, tmp_dir: Path, 
                          files: Dict[str, Path], kwargs: Dict) -> Tuple[tuple, Dict]:
        """Prepara argumentos especÃ­ficos para cada paso."""
        args = []
        step_kwargs = {}
        
        if step_key == "waf_detection":
            args = [domain, tmp_dir]
        elif step_key == "recon":
            args = [domain, tmp_dir]
        elif step_key == "finger":
            args = [files.get("recon"), tmp_dir]
        elif step_key == "leaks":
            args = [domain, tmp_dir, kwargs.get("hibp_api_key")]
        elif step_key == "nuclei":
            finger_file = files.get("finger")
            if finger_file is None:
                return (), {}
            args = [finger_file, tmp_dir, kwargs.get("full_nuclei_scan", False)]
        elif step_key == "nmap":
            finger_file = files.get("finger")
            if finger_file is None:
                return (), {}
            args = [finger_file, tmp_dir, True]
        elif step_key == "typos":
            args = [domain, tmp_dir]
        elif step_key == "dir_brute":
            finger_file = files.get("finger")
            if finger_file is None:
                return (), {}
            args = [finger_file, tmp_dir]
            step_kwargs.update(self._get_dir_brute_config(finger_file))
        elif step_key in ["cve", "security_config"]: 
            finger_file = files.get("finger")
            if finger_file is None:
                return (), {} # Return empty args if finger_file is None
            args = [finger_file, tmp_dir]
        elif step_key == "greynoise":
            api_key = os.environ.get("GREYNOISE_API_KEY")
            if not api_key:
                self.logger.error("GREYNOISE_API_KEY no configurada. El paso greynoise no se ejecutarÃ¡.")
                raise ScanError("GREYNOISE_API_KEY no configurada. Por favor, configura la variable de entorno.")
            args = [domain, tmp_dir, api_key]
            self.logger.debug(f"[{domain}] Argumentos preparados para greynoise: domain={domain}, tmp_dir={tmp_dir}, api_key_present={bool(api_key)}")
        elif step_key == "cisa_kev":
            tech_stack = self._get_dir_brute_config(files.get("finger")).get("tech_stack")
            # Ensure tech_stack is always a list, even if None is returned
            tech_stack_for_cisa = tech_stack if tech_stack is not None else []
            args = [domain, tmp_dir, tech_stack_for_cisa]
        elif step_key == "premium_adaptive":
            finger_file = files.get("finger")
            if finger_file is None:
                return (), {}
            args = [domain, tmp_dir]  # premium_adaptive_scan_wrapper espera (target: str, output_dir: Path)
            step_kwargs["finger_file"] = finger_file  # Pasar finger_file como kwarg
        elif step_key == "ml_analysis":
            # El anÃ¡lisis ML necesita acceso a todos los archivos de resultados
            args = [domain, tmp_dir, files]
        
        return tuple(args), step_kwargs

    def _get_dir_brute_config(self, httpx_file: Optional[Path]) -> Dict:
        """ConfiguraciÃ³n optimizada para directory brute force."""
        config = {
            'extensions': ['php', 'html', 'js', 'asp', 'aspx', 'txt', 'zip', 'rar', 'tar.gz'],
            'auto_extend': True,
            'max_concurrent_requests': 50,  # Reducido para evitar sobrecarga
            'delay_between_requests': 0.01,
            'filter_status_codes': [301, 404],
            'filter_content_length': None,
            'tech_stack': None
        }
        
        # Detectar tech stack de los resultados de httpx
        if httpx_file and httpx_file.exists():
            try:
                with open(httpx_file, "r") as f:
                    httpx_results = json.load(f)
                    
                for host_result in httpx_results:
                    tech_list = host_result.get("tech", [])
                    for tech in tech_list:
                        tech_lower = tech.lower()
                        if "wordpress" in tech_lower:
                            config['tech_stack'] = "wordpress"
                            return config
                        elif "laravel" in tech_lower:
                            config['tech_stack'] = "laravel"
                            return config
                    
                    # Si no hay tecnologÃ­a prioritaria, usar la primera
                    if tech_list and not config['tech_stack']:
                        config['tech_stack'] = tech_list[0]
                        
            except (json.JSONDecodeError, IOError) as e:
                self.logger.warning(f"Error leyendo httpx results: {e}")
        
        return config

    def _get_execution_order(self) -> List[List[str]]:
        """Determina el orden de ejecuciÃ³n basado en dependencias."""
        # ImplementaciÃ³n simple por capas
        layers = [
            ["waf_detection"],  # Capa 0: detecciÃ³n de WAF (primer paso)
            ["recon"],  # Capa 1: reconocimiento (depende de WAF)
            ["finger"],  # Capa 2: fingerprinting (depende de recon)
            # Capa 3: depende de finger y pueden ejecutarse en paralelo
            ["nuclei", "cve", "nmap", "security_config", "dir_brute", "cisa_kev", "greynoise", "premium_adaptive"],
            # Capa 4: independientes
            ["leaks", "typos"],
            # Capa 5: anÃ¡lisis ML (depende de resultados anteriores)
            ["ml_analysis"]
        ]
        return layers

    def _safe_count_json(self, path: Optional[Path]) -> int:
        """Cuenta elementos en archivo JSON de forma segura."""
        if not path or not path.exists():
            return 0
        try:
            with open(path, "r") as f:
                data = json.load(f)
                if isinstance(data, list):
                    return len(data)
                elif isinstance(data, dict):
                    # Detectar tipo de archivo por su estructura
                    filename = path.name.lower() if path else ""
                    
                    # Para nuclei: contar elementos en 'findings'
                    if 'nuclei' in filename and 'findings' in data:
                        findings = data.get('findings', [])
                        return len(findings) if isinstance(findings, list) else 0
                    
                    # Para dir_brute: contar elementos en 'directories' o 'results'
                    elif 'dir_brute' in filename:
                        if 'directories' in data:
                            dirs = data.get('directories', [])
                            return len(dirs) if isinstance(dirs, list) else 0
                        elif 'results' in data:
                            results = data.get('results', [])
                            return len(results) if isinstance(results, list) else 0
                    
                    # Para security_config: contar elementos en 'results' o 'findings'
                    elif 'security_config' in filename:
                        if 'results' in data:
                            results = data.get('results', [])
                            return len(results) if isinstance(results, list) else 0
                        elif 'findings' in data:
                            findings = data.get('findings', [])
                            return len(findings) if isinstance(findings, list) else 0
                    
                    # Para cves: contar elementos en 'cves' o 'vulnerabilities'
                    elif 'cve' in filename:
                        if 'cves' in data:
                            cves = data.get('cves', [])
                            return len(cves) if isinstance(cves, list) else 0
                        elif 'vulnerabilities' in data:
                            vulns = data.get('vulnerabilities', [])
                            return len(vulns) if isinstance(vulns, list) else 0
                    
                    # Para cisa_kev: contar elementos en 'vulnerabilities' o 'kev_matches'
                    elif 'cisa_kev' in filename:
                        if 'vulnerabilities' in data:
                            vulns = data.get('vulnerabilities', [])
                            return len(vulns) if isinstance(vulns, list) else 0
                        elif 'kev_matches' in data:
                            matches = data.get('kev_matches', [])
                            return len(matches) if isinstance(matches, list) else 0
                    
                    # Para leaks: contar elementos en 'breaches' o 'leaks'
                    elif 'leak' in filename:
                        if 'breaches' in data:
                            breaches = data.get('breaches', [])
                            return len(breaches) if isinstance(breaches, list) else 0
                        elif 'leaks' in data:
                            leaks = data.get('leaks', [])
                            return len(leaks) if isinstance(leaks, list) else 0
                    
                    # Para typosquats: contar elementos en 'typosquats' o 'domains'
                    elif 'typo' in filename:
                        if 'typosquats' in data:
                            typos = data.get('typosquats', [])
                            return len(typos) if isinstance(typos, list) else 0
                        elif 'domains' in data:
                            domains = data.get('domains', [])
                            return len(domains) if isinstance(domains, list) else 0
                    
                    # Para premium_adaptive: contar elementos en 'findings' o 'vulnerabilities'
                    elif 'premium_adaptive' in filename:
                        if 'findings' in data:
                            findings = data.get('findings', [])
                            return len(findings) if isinstance(findings, list) else 0
                        elif 'vulnerabilities' in data:
                            vulns = data.get('vulnerabilities', [])
                            return len(vulns) if isinstance(vulns, list) else 0
                        elif 'recommendations' in data:
                            recs = data.get('recommendations', [])
                            return len(recs) if isinstance(recs, list) else 0
                    
                    # Para GreyNoise y otros: si hay datos, consideramos que hay 1 resultado
                    return 1 if data else 0
                return 0
        except Exception:
            return 0

    def _count_lines(self, path: Optional[Path]) -> int:
        """Cuenta lÃ­neas no vacÃ­as en un archivo."""
        if not path or not path.exists():
            return 0
        try:
            with open(path, "r") as f:
                return len([line for line in f if line.strip()])
        except Exception:
            return 0

    def _enhance_with_threat_intel(self, nmap_file: Path) -> None:
        """Enriquece resultados de Nmap con threat intelligence."""
        if not nmap_file.exists():
            return
        
        try:
            with open(nmap_file, "r") as f:
                nmap_results = json.load(f)
            
            updated_results = []
            for host_data in nmap_results:
                ip_address = host_data.get("ip_address")
                if ip_address:
                    try:
                        threat_hits = check_threat_feeds(ip_address)
                        if threat_hits:
                            host_data["threat_intel"] = threat_hits
                    except Exception as e:
                        self.logger.warning(f"Error checking threat intel for {ip_address}: {e}")
                
                updated_results.append(host_data)
            
            with open(nmap_file, "w") as f:
                json.dump(updated_results, f, indent=2)
            
            self.logger.info("Threat intelligence added to Nmap results")
            
        except Exception as e:
            self.logger.error(f"Error enhancing Nmap results with threat intel: {e}")

    def _run_ml_analysis(self, domain: str, tmp_dir: Path, files: Dict[str, Path], **kwargs) -> Path:
        """Ejecuta anÃ¡lisis ML predictivo sobre los resultados del escaneo."""
        self.logger.info(f"ğŸ¤– [ML] Iniciando anÃ¡lisis ML para {domain}")
        
        if not self.ml_manager:
            self.logger.warning(f"ğŸ¤– [ML] Sistema ML no disponible, saltando anÃ¡lisis")
            # Crear archivo vacÃ­o para mantener consistencia
            ml_file = tmp_dir / f"{domain}_ml_analysis.json"
            ml_file.write_text(json.dumps({
                "status": "skipped",
                "reason": "ML system not available",
                "timestamp": dt.datetime.utcnow().isoformat()
            }))
            return ml_file
        
        try:
            # Recopilar todos los resultados de escaneo
            scan_result = {
                "scan_id": f"ml_analysis_{int(time.time())}",
                "target_domain": domain,
                "timestamp": dt.datetime.utcnow().isoformat(),
                "scan_type": "comprehensive",
                "status": "completed",
                "technologies": [],
                "vulnerabilities": [],
                "open_ports": [],
                "network_info": {},
                "security_headers": {},
                "geo_location": {}
            }
            
            # Extraer datos de fingerprinting
            if "finger" in files and files["finger"] and files["finger"].exists():
                try:
                    with open(files["finger"], "r") as f:
                        finger_data = json.load(f)
                    
                    for host_result in finger_data:
                        if "tech" in host_result:
                            for tech in host_result["tech"]:
                                scan_result["technologies"].append({
                                    "name": tech,
                                    "confidence": 85  # Valor por defecto
                                })
                        
                        if "status_code" in host_result:
                            scan_result["status_code"] = host_result["status_code"]
                        
                        if "content_length" in host_result:
                            scan_result["content_length"] = host_result["content_length"]
                        
                        if "response_time" in host_result:
                            scan_result["network_info"]["avg_response_time"] = host_result["response_time"]
                        
                        # Extraer headers de seguridad
                        headers = host_result.get("headers", {})
                        scan_result["security_headers"] = {
                            "strict_transport_security": "strict-transport-security" in headers,
                            "content_security_policy": "content-security-policy" in headers,
                            "x_frame_options": "x-frame-options" in headers
                        }
                        
                        break  # Solo procesar el primer host
                        
                except Exception as e:
                    self.logger.warning(f"ğŸ¤– [ML] Error procesando fingerprint data: {e}")
            
            # Extraer vulnerabilidades de Nuclei
            if "nuclei" in files and files["nuclei"] and files["nuclei"].exists():
                try:
                    with open(files["nuclei"], "r") as f:
                        nuclei_data = json.load(f)
                    
                    findings = nuclei_data.get("findings", [])
                    for finding in findings:
                        scan_result["vulnerabilities"].append({
                            "id": finding.get("template-id", "unknown"),
                            "title": finding.get("info", {}).get("name", "Unknown"),
                            "severity": finding.get("info", {}).get("severity", "UNKNOWN").upper(),
                            "description": finding.get("info", {}).get("description", "")
                        })
                        
                except Exception as e:
                    self.logger.warning(f"ğŸ¤– [ML] Error procesando nuclei data: {e}")
            
            # Extraer vulnerabilidades de CVE
            if "cve" in files and files["cve"] and files["cve"].exists():
                try:
                    with open(files["cve"], "r") as f:
                        cve_data = json.load(f)
                    
                    cves = cve_data.get("cves", [])
                    for cve in cves:
                        scan_result["vulnerabilities"].append({
                            "id": cve.get("id", "unknown"),
                            "title": cve.get("summary", "Unknown CVE"),
                            "severity": cve.get("severity", "UNKNOWN").upper(),
                            "cvss_score": cve.get("cvss_score", 0),
                            "description": cve.get("description", "")
                        })
                        
                except Exception as e:
                    self.logger.warning(f"ğŸ¤– [ML] Error procesando CVE data: {e}")
            
            # Extraer puertos de Nmap
            if "nmap" in files and files["nmap"] and files["nmap"].exists():
                try:
                    with open(files["nmap"], "r") as f:
                        nmap_data = json.load(f)
                    
                    for host in nmap_data:
                        ports = host.get("ports", [])
                        for port_info in ports:
                            if port_info.get("state") == "open":
                                scan_result["open_ports"].append(port_info.get("port", 0))
                        break  # Solo procesar el primer host
                        
                except Exception as e:
                    self.logger.warning(f"ğŸ¤– [ML] Error procesando nmap data: {e}")
            
            # Ejecutar anÃ¡lisis ML
            self.logger.info(f"ğŸ¤– [ML] Ejecutando anÃ¡lisis predictivo...")
            enhanced_result = self.ml_manager.enhance_scan_result(scan_result, domain)
            
            # Guardar resultados ML
            ml_file = tmp_dir / f"{domain}_ml_analysis.json"
            ml_results = {
                "ml_analysis": {
                    "prediction": enhanced_result.ml_prediction.to_dict() if enhanced_result.ml_prediction else None,
                    "risk_assessment": enhanced_result.risk_assessment,
                    "confidence_score": enhanced_result.confidence_score,
                    "recommendations": enhanced_result.recommendations,
                    "processing_time_ms": enhanced_result.processing_time * 1000,  # Convertir a ms
                    "model_version": getattr(enhanced_result, 'model_version', 'v1.0')
                },
                "original_scan": scan_result,
                "timestamp": dt.datetime.utcnow().isoformat(),
                "status": "completed"
            }
            
            with open(ml_file, "w") as f:
                json.dump(ml_results, f, indent=2)
            
            self.logger.info(f"ğŸ¤– [ML] AnÃ¡lisis ML completado. Archivo: {ml_file}")
            self.logger.info(f"ğŸ¤– [ML] Nivel de riesgo: {enhanced_result.risk_assessment.get('risk_level', 'unknown')}")
            self.logger.info(f"ğŸ¤– [ML] PuntuaciÃ³n de riesgo: {enhanced_result.risk_assessment.get('overall_risk', 0)}/100")
            
            return ml_file
            
        except Exception as e:
            self.logger.error(f"ğŸ¤– [ML] Error en anÃ¡lisis ML: {e}", exc_info=True)
            # Crear archivo de error
            ml_file = tmp_dir / f"{domain}_ml_analysis.json"
            ml_file.write_text(json.dumps({
                "status": "error",
                "error": str(e),
                "timestamp": dt.datetime.utcnow().isoformat()
            }))
            return ml_file

    def generate_pdf(self, domain: str, recipient_email: str, job_id: str = None, *, debug: bool = False, full_nuclei_scan: bool = False, hibp_api_key: str = None) -> ScanResult:
        self.logger.info(f"ğŸš€ [PIPELINE] Iniciando escaneo completo para {domain}")
        self.logger.info(f"ğŸš€ [PIPELINE] Dominio recibido (tipo: {type(domain).__name__}, longitud: {len(domain)}): '{domain}'")
        self.logger.info(f"ğŸš€ [PIPELINE] RepresentaciÃ³n cruda del dominio: {repr(domain)}")
        self.logger.info(f"ğŸš€ [PIPELINE] ParÃ¡metros: recipient={recipient_email}, job_id={job_id}, debug={debug}, full_nuclei_scan={full_nuclei_scan}")

        """Ejecuta el pipeline completo de escaneo y genera el informe PDF.

        Args:
            domain: Dominio a escanear
            recipient_email: Email para notificaciones
            job_id: ID del trabajo (se genera automÃ¡ticamente si no se proporciona)
            hibp_api_key: API key para Have I Been Pwned
            debug: Modo debug (mantiene archivos temporales)
            full_nuclei_scan: Ejecutar escaneo completo con Nuclei
            
        Returns:
            ScanResult con el resultado del escaneo
        """
        # Validaciones iniciales
        self.logger.info(f"ğŸš€ [PIPELINE] Validando dominio: {domain}")
        self._validate_domain(domain)
        self.logger.info(f"ğŸš€ [PIPELINE] âœ… Dominio vÃ¡lido: {domain}")
        
        if not job_id:
            job_id = f"standalone:{dt.datetime.utcnow().isoformat()}"
            self.logger.info(f"ğŸš€ [PIPELINE] Job ID generado: {job_id}")
        
        self._update_progress(job_id, "queue", 5, ScanStage.QUEUED)
        self.logger.info(f"ğŸš€ [PIPELINE] Progreso actualizado: queue (5%)")
        
        tmp_dir = None
        try:
            with self._temp_directory(domain) as tmp_dir_context:
                tmp_dir = tmp_dir_context
                self.logger.info(f"ğŸš€ [PIPELINE] Directorio temporal creado: {tmp_dir}")
                
                # Verificar que el directorio temporal es escribible
                try:
                    test_file = tmp_dir / "test_write.txt"
                    test_file.write_text("test")
                    test_file.unlink()
                    self.logger.info(f"ğŸš€ [PIPELINE] âœ… Directorio temporal escribible")
                except Exception as e:
                    self.logger.error(f"ğŸš€ [PIPELINE] âŒ Error de permisos en directorio temporal: {e}")
                    raise ScanError(f"Error de permisos en directorio temporal: {e}")
                
                files: Dict[str, Path] = {}
                execution_layers = self._get_execution_order()
                self.logger.info(f"ğŸš€ [PIPELINE] Orden de ejecuciÃ³n: {execution_layers}")
                
                # Ejecutar por capas respetando dependencias
                for layer_idx, layer_steps in enumerate(execution_layers):
                    self.logger.info(f"ğŸš€ [PIPELINE] Ejecutando capa {layer_idx}: {layer_steps}")
                    if layer_idx < 2:  # Capas secuenciales (0 y 1)
                        for step_key in layer_steps:
                            if step_key not in self.steps:
                                self.logger.warning(f"ğŸš€ [PIPELINE] âš ï¸ Paso {step_key} no encontrado en steps")
                                continue
                            
                            self.logger.info(f"ğŸš€ [PIPELINE] Ejecutando paso secuencial: {step_key}")
                            step = self.steps[step_key]
                            _, result = self._execute_step(
                                step, job_id, domain, tmp_dir, files,
                                hibp_api_key=hibp_api_key,
                                full_nuclei_scan=full_nuclei_scan
                            )
                            files[step_key] = result
                            self.logger.info(f"ğŸš€ [PIPELINE] âœ… Paso {step_key} completado. Archivo resultado: {result}")
                            
                            # Verificar que el archivo existe
                            if result and isinstance(result, Path) and result.exists():
                                self.logger.info(f"ğŸš€ [PIPELINE] âœ… Archivo {step_key} verificado: {result} (tamaÃ±o: {result.stat().st_size} bytes)")
                            else:
                                self.logger.warning(f"ğŸš€ [PIPELINE] âš ï¸ Archivo {step_key} no existe o es None: {result}")
                    
                    else:  # Capas paralelas (2 en adelante)
                        with ThreadPoolExecutor(max_workers=min(len(layer_steps), 6)) as executor:
                            future_to_step = {}
                            
                            for step_key in layer_steps:
                                if step_key not in self.steps:
                                    self.logger.warning(f"ğŸš€ [PIPELINE] âš ï¸ Paso {step_key} no encontrado en steps")
                                    continue
                                
                                self.logger.info(f"ğŸš€ [PIPELINE] Encolando paso paralelo: {step_key}")
                                step = self.steps[step_key]
                                future = executor.submit(
                                    self._execute_step,
                                    step, job_id, domain, tmp_dir, files,
                                    hibp_api_key=hibp_api_key,
                                    full_nuclei_scan=full_nuclei_scan
                                )
                                future_to_step[future] = step_key
                            
                            # Procesar resultados
                            for future in as_completed(future_to_step):
                                step_key = future_to_step[future]
                                try:
                                    _, result = future.result()
                                    files[step_key] = result
                                    self.logger.info(f"ğŸš€ [PIPELINE] âœ… Paso paralelo {step_key} completado. Archivo resultado: {result}")
                                    
                                    # Verificar que el archivo existe
                                    if result and isinstance(result, Path) and result.exists():
                                        self.logger.info(f"ğŸš€ [PIPELINE] âœ… Archivo {step_key} verificado: {result} (tamaÃ±o: {result.stat().st_size} bytes)")
                                    else:
                                        self.logger.warning(f"ğŸš€ [PIPELINE] âš ï¸ Archivo {step_key} no existe o es None: {result}")
                                except Exception as e:
                                    self.logger.error(f"ğŸš€ [PIPELINE] âŒ Error in parallel step {step_key}: {e}")
                                    files[step_key] = None # Set to None if step fails to prevent further errors
                
                # Resumen de archivos generados
                self.logger.info(f"ğŸš€ [PIPELINE] Resumen de archivos generados:")
                for step_key, file_path in files.items():
                    if file_path and isinstance(file_path, Path) and file_path.exists():
                        self.logger.info(f"ğŸš€ [PIPELINE]   âœ… {step_key}: {file_path} ({file_path.stat().st_size} bytes)")
                    else:
                        self.logger.warning(f"ğŸš€ [PIPELINE]   âŒ {step_key}: {file_path} (no existe o es None)")
                
                # Enriquecer con threat intelligence
                if "nmap" in files and files["nmap"]:
                    self.logger.info(f"ğŸš€ [PIPELINE] Enriqueciendo con threat intelligence")
                    self._enhance_with_threat_intel(files["nmap"])
                else:
                    self.logger.info(f"ğŸš€ [PIPELINE] No hay archivo nmap para enriquecer con threat intelligence")
                
                # Generar informe PDF
                self.logger.info(f"ğŸš€ [PIPELINE] Generando informe PDF para {domain}")
                self.logger.info(f"ğŸš€ [PIPELINE] Archivos que se pasarÃ¡n a build_pdf:")
                self.logger.info(f"ğŸš€ [PIPELINE]   httpx_file: {files.get('finger')}")
                self.logger.info(f"ğŸš€ [PIPELINE]   nuclei_file: {files.get('nuclei')}")
                self.logger.info(f"ğŸš€ [PIPELINE]   leaks_file: {files.get('leaks')}")
                self.logger.info(f"ğŸš€ [PIPELINE]   typosquats_file: {files.get('typos')}")
                self.logger.info(f"ğŸš€ [PIPELINE]   dir_brute_file: {files.get('dir_brute')}")
                self.logger.info(f"ğŸš€ [PIPELINE]   cves_file: {files.get('cve')}")
                self.logger.info(f"ğŸš€ [PIPELINE]   nmap_file: {files.get('nmap')}")
                self.logger.info(f"ğŸš€ [PIPELINE]   security_config_file: {files.get('security_config')}")
                self.logger.info(f"ğŸš€ [PIPELINE]   cisa_kev_file: {files.get('cisa_kev')}")
                self.logger.info(f"ğŸš€ [PIPELINE]   greynoise_file: {files.get('greynoise')}")
                self.logger.info(f"ğŸš€ [PIPELINE]   premium_adaptive_file: {files.get('premium_adaptive')}")
                self.logger.info(f"ğŸš€ [PIPELINE]   waf_file: {files.get('waf_detection')}")
                
                pdf_path = build_pdf(
                    domain, recipient_email, tmp_dir,
                    httpx_file=files.get("finger"),
                    nuclei_file=files.get("nuclei"),
                    leaks_file=files.get("leaks"),
                    typosquats_file=files.get("typos"),
                    dir_brute_file=files.get("dir_brute"),
                    
                    cves_file=files.get("cve"),
                    nmap_file=files.get("nmap"),
                    security_config_file=files.get("security_config"),
                    cisa_kev_file=files.get("cisa_kev"),
                    greynoise_file=files.get("greynoise"),
                    premium_adaptive_file=files.get("premium_adaptive"),
                    ml_file=files.get("ml_analysis"),
                    waf_file=files.get("waf_detection")
                )
                self.logger.info(f"ğŸš€ [PIPELINE] âœ… PDF generado: {pdf_path}")
                
                # Calcular mÃ©tricas
                threat_intel_hits = 0
                nmap_file_path = files.get("nmap")
                if nmap_file_path and nmap_file_path.exists():
                    try:
                        with open(nmap_file_path, 'r') as f:
                            nmap_data_for_metrics = json.load(f)
                            if isinstance(nmap_data_for_metrics, list):
                                for item in nmap_data_for_metrics:
                                    if item.get("threat_intel"):
                                        threat_intel_hits += 1
                    except (json.JSONDecodeError, IOError) as e:
                        self.logger.warning(f"Error al leer o decodificar JSON de {nmap_file_path} para mÃ©tricas de threat intel: {e}")

                metrics = {
                    "subdomains": self._count_lines(files.get("recon")),
                    "vulnerabilities": self._safe_count_json(files.get("nuclei")),
                    "cves": self._safe_count_json(files.get("cve")),
                    "ports": self._safe_count_json(files.get("nmap")),
                    "cisa_kev_vulnerabilities": self._safe_count_json(files.get("cisa_kev")),
                    "greynoise_malicious_ips": self._safe_count_json(files.get("greynoise")),
                    "leaked_credentials": self._safe_count_json(files.get("leaks")),
                    "typosquats": self._safe_count_json(files.get("typos")),
                    "exposed_directories": self._safe_count_json(files.get("dir_brute")),
                    "security_misconfigurations": self._safe_count_json(files.get("security_config")),
                    "premium_adaptive_findings": self._safe_count_json(files.get("premium_adaptive")),
                    "threat_intel_hits": threat_intel_hits
                }
                
                self._update_progress(job_id, "report_generation", 100, ScanStage.COMPLETED)
                self.logger.info(f"ğŸš€ [PIPELINE] âœ… Escaneo completado para {domain}. Informe en: {pdf_path}")
                self.logger.info(f"ğŸš€ [PIPELINE] MÃ©tricas finales: {metrics}")
                
                # Enviar notificaciÃ³n por email con el informe adjunto
                from pentest.report import send_notification
                try:
                    self.logger.info(f"ğŸš€ [PIPELINE] Enviando notificaciÃ³n por email a {recipient_email}")
                    email_sent = send_notification(
                        job_id=job_id,
                        status_message="Completado exitosamente",
                        status_type="success",
                        pdf_path=pdf_path,
                        recipient_email=recipient_email,
                        subc=metrics["subdomains"],
                        vulc=metrics["vulnerabilities"],
                        domain=domain,
                        cisa_kev_vulnerabilities=metrics["cisa_kev_vulnerabilities"],
                        greynoise_malicious_ips=metrics["greynoise_malicious_ips"],
                        threat_intel_hits=metrics["threat_intel_hits"]
                    )
                    if email_sent:
                        self.logger.info(f"ğŸš€ [PIPELINE] âœ… NotificaciÃ³n enviada exitosamente a {recipient_email}")
                    else:
                        self.logger.warning(f"ğŸš€ [PIPELINE] âš ï¸ No se pudo enviar la notificaciÃ³n a {recipient_email}")
                except Exception as e:
                    self.logger.error(f"ğŸš€ [PIPELINE] âŒ Error enviando notificaciÃ³n: {e}")
                
                return ScanResult(
                    success=True,
                    message="Escaneo completado exitosamente",
                    job_id=job_id,
                    report_path=pdf_path,
                    metrics=metrics
                )
        except ScanError as e:
            self.logger.error(f"ğŸš€ [PIPELINE] âŒ Error de escaneo para {domain}: {e}")
            self._update_progress(job_id, "error", 100, ScanStage.FAILED, error=str(e))
            
            # Enviar notificaciÃ³n de error
            from pentest.report import send_notification
            try:
                self.logger.info(f"ğŸš€ [PIPELINE] Enviando notificaciÃ³n de error a {recipient_email}")
                send_notification(
                    job_id=job_id,
                    status_message=f"Error de escaneo: {e}",
                    status_type="failed",
                    pdf_path=None,
                    recipient_email=recipient_email,
                    subc=0,
                    vulc=0,
                    domain=domain,
                    cisa_kev_vulnerabilities=0,
                    greynoise_malicious_ips=0,
                    threat_intel_hits=0
                )
            except Exception as email_error:
                self.logger.error(f"ğŸš€ [PIPELINE] âŒ Error enviando notificaciÃ³n de error: {email_error}")
            
            return ScanResult(
                success=False,
                message=f"Error de escaneo: {e}",
                job_id=job_id,
                report_path=None,
                metrics={}
            )
        except Exception as e:
            self.logger.error(f"ğŸš€ [PIPELINE] âŒ Error fatal durante el escaneo de {domain}: {e}", exc_info=True)
            self._update_progress(job_id, "error", 100, ScanStage.FAILED, error=str(e))
            
            # Enviar notificaciÃ³n de error fatal
            from pentest.report import send_notification
            try:
                self.logger.info(f"ğŸš€ [PIPELINE] Enviando notificaciÃ³n de error fatal a {recipient_email}")
                send_notification(
                    job_id=job_id,
                    status_message=f"Error fatal durante el escaneo: {e}",
                    status_type="failed",
                    pdf_path=None,
                    recipient_email=recipient_email,
                    subc=0,
                    vulc=0,
                    domain=domain,
                    cisa_kev_vulnerabilities=0,
                    greynoise_malicious_ips=0,
                    threat_intel_hits=0
                )
            except Exception as email_error:
                self.logger.error(f"ğŸš€ [PIPELINE] âŒ Error enviando notificaciÃ³n de error fatal: {email_error}")
            
            return ScanResult(
                success=False,
                message=f"Error fatal durante el escaneo: {e}",
                job_id=job_id,
                report_path=None,
                metrics={}
            )
        finally:
            if tmp_dir and not debug:
                self.logger.info(f"ğŸš€ [PIPELINE] Limpiando directorio temporal para {domain}: {tmp_dir}")
                shutil.rmtree(tmp_dir, ignore_errors=True)
            elif debug:
                self.logger.info(f"ğŸš€ [PIPELINE] Modo debug activado. Directorio temporal conservado: {tmp_dir}")

# ---------------------------------------------------------------------------
# WORKER MEJORADO
# ---------------------------------------------------------------------------



# ---------------------------------------------------------------------------
# FUNCIONES DE COMPATIBILIDAD
# ---------------------------------------------------------------------------



def start_worker():
    """Inicia un worker de RQ para procesar la cola de escaneos."""
    # Configurar logging explÃ­citamente para el worker
    setup_logging()
    
    logger = logging.getLogger(__name__)
    logger.info(f"ğŸš€ [WORKER] Intentando conectar a Redis en {REDIS_URL}")
    
    redis_conn = redis.from_url(REDIS_URL)
    logger.info("ğŸš€ [WORKER] âœ… Conectado exitosamente a Redis")
    
    worker = Worker(['scan_queue'], connection=redis_conn)
    logger.info("ğŸš€ [WORKER] Worker RQ inicializado. Escuchando en 'scan_queue'...")
    
    # Configurar logging adicional para RQ
    import rq.worker
    rq_logger = logging.getLogger('rq.worker')
    rq_logger.setLevel(logging.INFO)
    
    logger.info("ğŸš€ [WORKER] Iniciando worker...")
    worker.work(logging_level='INFO')

# ---------------------------------------------------------------------------
# PUNTO DE ENTRADA
# ---------------------------------------------------------------------------

from rq import get_current_job, Queue, Worker

def _run_scan_job(domain: str, recipient_email: str, hibp_api_key: str = None, debug: bool = False, full_nuclei_scan: bool = False):
    """FunciÃ³n wrapper para ejecutar el pipeline de escaneo dentro de un trabajo de RQ."""
    # Configurar logging para asegurar que aparezcan los logs
    setup_logging()
    
    job = get_current_job()
    job_id = job.id if job else "unknown_job"
    
    logger = logging.getLogger(__name__)
    logger.info(f"ğŸš€ [JOB] Iniciando escaneo para dominio: {domain} (Job ID: {job_id})")
    logger.info(f"ğŸš€ [JOB] Dominio recibido (tipo: {type(domain).__name__}, longitud: {len(domain)}): '{domain}'")
    logger.info(f"ğŸš€ [JOB] RepresentaciÃ³n cruda del dominio: {repr(domain)}")
    logger.info(f"ğŸš€ [JOB] ParÃ¡metros: recipient_email={recipient_email}, debug={debug}, full_nuclei_scan={full_nuclei_scan}")

    try:
        pipeline = ScanPipeline()
        logger.info(f"ğŸš€ [JOB] Pipeline creado, ejecutando generate_pdf...")
        
        result = pipeline.generate_pdf(
            domain=domain,
            recipient_email=recipient_email,
            job_id=job_id, # Pasar el job_id
            hibp_api_key=hibp_api_key,
            debug=debug,
            full_nuclei_scan=full_nuclei_scan
        )
        
        logger.info(f"ğŸš€ [JOB] âœ… Escaneo completado exitosamente para {domain}")
        logger.info(f"ğŸš€ [JOB] Resultado: success={result.success}, report_path={result.report_path}")
        return result
        
    except Exception as e:
        logger.error(f"ğŸš€ [JOB] âŒ Error ejecutando escaneo para {domain} (Job ID: {job_id}): {e}", exc_info=True)
        # Actualizar el estado del progreso a FAILED
        redis_manager = RedisManager(REDIS_URL)
        progress = ScanProgress(job_id=job_id, stage=ScanStage.FAILED, step='Error', percentage=100, error=str(e))
        redis_manager.publish_progress(progress)
        raise  # Re-lanzar la excepciÃ³n para que RQ la capture

def enqueue_scan(domain: str, recipient_email: str, hibp_api_key: str = None, debug: bool = False, full_nuclei_scan: bool = False):
    """Encola un trabajo de escaneo en RQ."""
    q = Queue('scan_queue', connection=redis.from_url(REDIS_URL))
    job = q.enqueue(
        _run_scan_job,
        domain,
        recipient_email,
        hibp_api_key=hibp_api_key,
        debug=debug,
        full_nuclei_scan=full_nuclei_scan,
        result_ttl=86400, # Mantener el resultado por 24 horas
        job_timeout=3600 # Tiempo mÃ¡ximo de ejecuciÃ³n del trabajo (1 hora)
    )
    logging.getLogger(__name__).info(f"Enqueued scan job for {domain} with ID: {job.id}")
    return job.id

if __name__ == "__main__":
    setup_logging()
    
    if len(sys.argv) > 1 and sys.argv[1] == "worker":
        start_worker()
    else:
        # Modo de prueba
        import argparse
        parser = argparse.ArgumentParser(description="Scanner de seguridad")
        parser.add_argument("--domain", required=True, help="Dominio a escanear")
        parser.add_argument("--email", required=True, help="Email para notificaciones")
        parser.add_argument("--debug", action="store_true", help="Modo debug")
        
        args = parser.parse_args()
        
        pipeline = ScanPipeline()
        result = pipeline.generate_pdf(
            domain=args.domain,
            recipient_email=args.email,
            debug=args.debug
        )
        
        print(f"Resultado: {result.status}")
        if result.report_path:
            print(f"Informe: {result.report_path}")

# === SISTEMA MEJORADO DE DETECCION DE TECNOLOGIAS ===
try:
    # Configurar logger
    logger = logging.getLogger(__name__)
    
    # ConfiguraciÃ³n para el integrador mejorado
    enhanced_config = {
        "enable_enhanced_detection": True,
        "enable_web_content_analysis": True,
        "enable_legacy_fallback": True,
        "cache_enabled": True,
        "validate_tools_on_startup": False  # Evitar validaciÃ³n en startup para producciÃ³n
    }
    
    # Inicializar integrador mejorado
    enhanced_integrator = EnhancedTechIntegrator(config=enhanced_config)
    
    # Intentar inicializar el sistema mejorado
    if enhanced_integrator.initialize():
        logger.info("Sistema mejorado de deteccion disponible e inicializado")
        
        # Reemplazar funciÃ³n de fingerprinting si estÃ¡ disponible
        def enhanced_fingerprint_wrapper(target, timeout=30):
            """Wrapper para fingerprinting mejorado con fallback."""
            try:
                result = enhanced_integrator.detect_technologies(target)
                if result and hasattr(result, 'technologies') and result.technologies:
                    # Convertir resultado a formato compatible
                    return {
                        'technologies': result.technologies,
                        'confidence_scores': {tech.get('name', 'unknown'): tech.get('confidence', 0) for tech in result.technologies},
                        'detection_methods': list(result.tool_coverage.keys()),
                        'detection_time': result.detection_time,
                        'timestamp': int(time.time())
                    }
            except Exception as e:
                logger.warning(f"Fingerprinting mejorado fallÃ³, usando bÃ¡sico: {e}")
            
            # Fallback al sistema original
            from .fingerprint import fingerprint_target
            return fingerprint_target(target, timeout)
        
        # Reemplazar la funciÃ³n original
        globals()['fingerprint_target'] = enhanced_fingerprint_wrapper
        
    else:
        logger.warning("Sistema mejorado no se pudo inicializar, usando sistema bÃ¡sico")
        
except Exception as e:
    logger = logging.getLogger(__name__)
    logger.error(f"Error inicializando sistema mejorado: {e}")
    logger.info("Continuando con sistema bÃ¡sico")
# === FIN SISTEMA MEJORADO ===
