"""Pipeline principal del escáner de seguridad con notificaciones de progreso."""

from __future__ import annotations
import json, logging, os, shutil, sys, tempfile, datetime as dt
from pathlib import Path
from typing import Dict, Optional, Callable, List
from concurrent.futures import ThreadPoolExecutor, as_completed


import redis

# --- módulos de tu proyecto -----------------------------------------------
from pentest.config import REDIS_URL, SAFE_DOMAIN
from pentest.exceptions import ScanError
from pentest.recon         import recon
from pentest.fingerprint   import fingerprint
from pentest.nuclei_scan   import nuclei_scan
from pentest.tls_scan      import tls_scan
from pentest.leaks         import check_leaks
from pentest.typosquat     import check_typosquats
from pentest.cve_scan      import cve_scan
from pentest.nmap_scan     import nmap_scan
from pentest.security_config import security_config_scan # Import the new security config scan module
from pentest.dir_brute     import dir_brute_scan # Import the new dir brute scan module
from pentest.screenshots   import take_screenshots # Import the new screenshots module
from pentest.report        import build_pdf, send_notification

log = logging.getLogger("pentest")


# ---------------------------------------------------------------------------

class DeduplicateFilter(logging.Filter):
    """Filtro para deduplicar mensajes de log repetidos."""
    def __init__(self, name=''):
        super().__init__(name)
        self.last_messages = {}
        self.deduplication_interval = 5 # seconds

    def filter(self, record):
        message = record.getMessage()
        # Combinar el mensaje con el nivel y el nombre del logger para una deduplicación más específica
        unique_key = (record.levelname, record.name, message)
        
        now = dt.datetime.now()
        if unique_key in self.last_messages:
            last_time = self.last_messages[unique_key]
            if (now - last_time).total_seconds() < self.deduplication_interval:
                return False # Suprimir el mensaje
        
        self.last_messages[unique_key] = now
        return True # Permitir el mensaje

logging.basicConfig(
    stream=sys.stdout,
    level=logging.DEBUG,
    format="%(asctime)s [%(levelname)s] %(name)s: %(message)s"
)

# Añadir el filtro a todos los loggers
for handler in logging.root.handlers:
    handler.addFilter(DeduplicateFilter())

rds: redis.Redis = redis.from_url(REDIS_URL)

# Paso, porcentaje acumulado aproximado y función que lo ejecuta ------------
Step = Dict[str, object]
STEPS: List[Step] = [
    {"key": "recon",   "pct": 10, "runner": recon},
    {"key": "finger",  "pct": 30, "runner": fingerprint},
    {"key": "nuclei",  "pct": 55, "runner": nuclei_scan},
    {"key": "tls",     "pct": 70, "runner": tls_scan},
    {"key": "leaks",   "pct": 85, "runner": check_leaks},
    {"key": "typos",   "pct": 95, "runner": check_typosquats},
    {"key": "cve",     "pct": 97, "runner": cve_scan},
    {"key": "nmap",    "pct": 98, "runner": nmap_scan},
    {"key": "security_config", "pct": 98.5, "runner": security_config_scan}, # Add security config scan step
    {"key": "dir_brute", "pct": 98.7, "runner": dir_brute_scan}, # Add dir brute scan step
    {"key": "screenshots", "pct": 99, "runner": take_screenshots}, # Add screenshots step
]

def _publish(job_id: str, payload: Dict):
    """Envía el progreso por Pub/Sub y lo guarda en Redis hash."""
    try:
        rds.publish(job_id, json.dumps(payload))
        # Guardar progreso en Redis hash
        rds.hset(f"rq:job:{job_id}", "meta", json.dumps({"progress": payload}))
    except Exception:  # conexión Redis caída → no interrumpir el scan
        pass

def _mark(job_id: str, step_key: str, pct: int, extra: Dict | None = None):
    data = {"state": "working", "step": step_key, "pct": pct}
    if extra:
        data.update(extra)
    _publish(job_id, data)

# ---------------------------------------------------------------------------
def generate_pdf(domain: str,
                 recipient_email: str,
                 job_id: str | None = None,
                 *,
                 hibp_api_key: str | None = None,
                 debug: bool = False,
                 full_nuclei_scan: bool = False) -> Dict:
    """
    Ejecuta el pipeline completo y devuelve dict con estado final.
    """
    if not SAFE_DOMAIN.match(domain):
        raise ScanError(f"Dominio inválido: {domain}")

    if not job_id:
        job_id = f"standalone:{dt.datetime.utcnow().isoformat()}"

    _mark(job_id, "queue", 5) # Mark queue step

    tmp_dir = Path(tempfile.mkdtemp(prefix=f"scan_{domain}_"))
    log.info("➜ [%s] TMP %s", domain, tmp_dir)

    files: Dict[str, Path] = {}

    def _run_scan_step(step_key, step_pct, step_fn, *args, **kwargs):
        _mark(job_id, step_key, step_pct)
        log.info("→ [%s] Iniciando fase: %s (%s %%)", domain, step_key, step_pct)
        try:
            result = step_fn(*args, **kwargs)
            log.info("✅ [%s] Fase %s completada.", domain, step_key)
            return step_key, result
        except Exception as e:
            log.error("❌ [%s] Error en la fase %s: %s", domain, step_key, e)
            raise ScanError(f"Error en {step_key}: {e}") from e

    try:
        # Run recon and finger sequentially as they have dependencies
        recon_step = next(s for s in STEPS if s["key"] == "recon")
        key, pct, fn = recon_step["key"], recon_step["pct"], recon_step["runner"]
        _, files["subdomains"] = _run_scan_step(key, pct, fn, domain, tmp_dir)
        if files["subdomains"].exists():
            with open(files["subdomains"], "r") as f:
                subdomains_content = f.read()
            log.info("[%s] Contenido de subdomains.txt después de recon:\n%s", domain, subdomains_content)
        else:
            log.warning("El archivo subdomains.txt no fue creado por la fase recon.")

        finger_step = next(s for s in STEPS if s["key"] == "finger")
        key, pct, fn = finger_step["key"], finger_step["pct"], finger_step["runner"]
        _, files["httpx"] = _run_scan_step(key, pct, fn, files["subdomains"], tmp_dir)

        # Run remaining steps in parallel
        with ThreadPoolExecutor(max_workers=5) as executor:
            future_to_step = {}
            for step in STEPS:
                if step["key"] in ["recon", "finger"]: # Skip already run steps
                    continue

                key, pct, fn = step["key"], step["pct"], step["runner"]
                if key == "leaks":
                    future = executor.submit(_run_scan_step, key, pct, fn, domain, tmp_dir, hibp_api_key)
                elif key == "nmap":
                    future = executor.submit(_run_scan_step, key, pct, fn, files["httpx"], tmp_dir, True) # Pass full_scan=True
                elif key == "nuclei":
                    future = executor.submit(_run_scan_step, key, pct, fn, files["httpx"], tmp_dir, full_nuclei_scan)
                elif key == "dir_brute":
                    # Parámetros para dir_brute_scan
                    dir_brute_extensions = ['php', 'html', 'js', 'asp', 'aspx', 'txt', 'zip', 'rar', 'tar.gz']
                    dir_brute_auto_extend = True
                    dir_brute_max_concurrent_requests = 100
                    dir_brute_delay_between_requests = 0.01
                    dir_brute_filter_status_codes = [301, 404] # Ejemplo: filtrar 301 y 404
                    dir_brute_filter_content_length = None # Se podría obtener de una petición a una URL 404 conocida

                    # Obtener tech_stack de los resultados de httpx
                    detected_tech_stack = None
                    if files["httpx"].exists():
                        with open(files["httpx"], "r") as f:
                            httpx_results = json.load(f)
                            for host_result in httpx_results:
                                if "tech" in host_result:
                                    # Priorizar tecnologías conocidas como WordPress o Laravel
                                    for tech in host_result["tech"]:
                                        if "wordpress" in tech.lower():
                                            detected_tech_stack = "wordpress"
                                            break
                                        elif "laravel" in tech.lower():
                                            detected_tech_stack = "laravel"
                                            break
                                    if detected_tech_stack: # Si ya encontramos una tecnología prioritaria, salimos
                                        break
                                    if not detected_tech_stack and host_result["tech"]: # Si no hay prioritaria, tomamos la primera que encontremos
                                        detected_tech_stack = host_result["tech"][0]
                                        break

                    dir_brute_tech_stack = detected_tech_stack

                    future = executor.submit(
                        _run_scan_step, key, pct, fn, files["httpx"], tmp_dir,
                        extensions=dir_brute_extensions,
                        auto_extend=dir_brute_auto_extend,
                        max_concurrent_requests=dir_brute_max_concurrent_requests,
                        delay_between_requests=dir_brute_delay_between_requests,
                        filter_status_codes=dir_brute_filter_status_codes,
                        filter_content_length=dir_brute_filter_content_length,
                        tech_stack=dir_brute_tech_stack
                    )
                elif key in ["tls", "cve", "security_config", "screenshots"]:
                    future = executor.submit(_run_scan_step, key, pct, fn, files["httpx"], tmp_dir)
                elif key == "typos":
                    future = executor.submit(_run_scan_step, key, pct, fn, domain, tmp_dir)
                else:
                    continue # Should not happen
                future_to_step[future] = key

            for future in as_completed(future_to_step):
                step_key = future_to_step[future]
                try:
                    _, result = future.result()
                    if step_key == "leaks":
                        files["leaks"] = result
                    elif step_key == "nuclei":
                        files["nuclei"] = result
                    elif step_key == "tls":
                        files["tls"] = result
                    elif step_key == "typos":
                        files["typosquats"] = result
                    elif step_key == "cve":
                        files["cves"] = result
                    elif step_key == "nmap":
                        files["nmap"] = result
                    elif step_key == "security_config":
                        files["security_config"] = result
                    elif step_key == "dir_brute":
                        files["dir_brute"] = result
                    elif step_key == "screenshots":
                        files["screenshots"] = result
                except ScanError as e:
                    log.error("❌ [%s] Error al procesar el resultado de la fase %s: %s", domain, step_key, e)
                    # Decide if you want to re-raise or just log and continue
                    raise # Re-raise to fail the entire scan if a sub-task fails



        log.info("📄 [%s] Iniciando generación de informe PDF.", domain) # Log antes de generar PDF
        pdf_path = build_pdf(
            domain,
            recipient_email,
            tmp_dir,
            httpx_file=files.get("httpx"),
            nuclei_file=files.get("nuclei"),
            tls_file=files.get("tls"),
            leaks_file=files.get("leaks"),
            typosquats_file=files.get("typosquats"),
            dir_brute_file=files.get("dir_brute"),
            screenshots_file=files.get("screenshots"),
            cves_file=files.get("cves"),
            nmap_file=files.get("nmap"),
            security_config_file=files.get("security_config"),
            subdomains_file=files.get("subdomains")
        )

        _mark(job_id, "report", 100, {"report_path": str(pdf_path)})
        log.info("✅ [%s] Informe PDF generado exitosamente en %s", domain, pdf_path)

        # Extract subc and vulc from scan results
        subc = 0
        if files["subdomains"].exists():
            with open(files["subdomains"], "r") as f:
                subc = len([line for line in f if line.strip()])

        vulc = 0
        if files["nuclei"].exists():
            with open(files["nuclei"], "r") as f:
                try:
                    nuclei_results = json.load(f)
                    vulc = len(nuclei_results)
                except json.JSONDecodeError:
                    log.warning("[%s] No se pudo decodificar el JSON de los resultados de Nuclei.", domain)

        # Send notification
        send_notification(
            job_id=job_id,
            status_message="Escaneo completado exitosamente.",
            status_type="success",
            pdf_path=pdf_path,
            recipient_email=recipient_email,
            subc=subc,
            vulc=vulc,
            domain=domain
        )

        return {"status": "success", "report_path": str(pdf_path), "job_id": job_id}

    except ScanError as e:
        log.error("❌ [%s] Error durante el escaneo: %s", domain, e)
        _mark(job_id, "failed", 100, {"error": str(e)})
        # Attempt to send a failed notification if possible
        send_notification(
            job_id=job_id,
            status_message=f"Escaneo fallido: {str(e)}",
            status_type="failed",
            pdf_path=None, # No PDF generated on failure
            recipient_email=recipient_email,
            subc=0,
            vulc=0,
            domain=domain
        )
        return {"status": "failed", "error": str(e), "job_id": job_id}
    except Exception as e:
        log.error("❌ [%s] Error inesperado: %s", domain, e)
        _mark(job_id, "failed", 100, {"error": str(e)})
        # Attempt to send a failed notification if possible
        send_notification(
            job_id=job_id,
            status_message=f"Error inesperado durante el escaneo: {str(e)}",
            status_type="failed",
            pdf_path=None, # No PDF generated on failure
            recipient_email=recipient_email,
            subc=0,
            vulc=0,
            domain=domain
        )
        return {"status": "failed", "error": str(e), "job_id": job_id}
    finally:
        if not debug and tmp_dir.exists():
            shutil.rmtree(tmp_dir)
            log.info("🗑️ [%s] Directorio temporal %s eliminado.", domain, tmp_dir)

# ---------------------------------------------------------------------------
def _safe_count_json(path: Path | None) -> int:
    if not path or not path.exists(): return 0
    try:
        with open(path, "r") as f:
            return len(json.load(f))
    except Exception:
        return 0
# ---------------------------------------------------------------------------
#  MODO WORKER "BÁSICO" (sigue utilizando tu cola BLPOP)
# ---------------------------------------------------------------------------
def start_worker():
    log.info("⌛ Worker BLPOP escuchando en redis queue 'scan_queue'")
    while True:
        log.info("Esperando mensajes en la cola 'scan_queue'...")
        _, raw = rds.blpop("scan_queue")
        log.info(f"Mensaje recibido: {raw}")
        try:
            log.info(f"Mensaje raw recibido: {raw}")
            req = json.loads(raw)
            log.info(f"Mensaje parseado: {req}")
            job_id = f"job:{dt.datetime.utcnow().isoformat()}"
            _publish(job_id, {"state": "queued"})
            log.info(f"Iniciando generate_pdf para dominio: {req['domain']} con job_id: {job_id}")
            generate_pdf(req["domain"], req["email"], job_id)
            log.info(f"generate_pdf completado para job_id: {job_id}")
        except Exception as e:
            log.error("Error procesando job %s – %s", raw, e)

if __name__ == "__main__":
    start_worker()