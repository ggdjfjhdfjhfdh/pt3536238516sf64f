"""Pipeline principal del esc√°ner de seguridad con notificaciones de progreso."""

from __future__ import annotations
import json
from enhanced_integration import EnhancedTechIntegrator
import logging
import os
import shutil
import sys
import tempfile
import datetime as dt
import asyncio
import redis
import time
from pathlib import Path
from typing import Dict, Optional, Callable, List, Any, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed, Future
from dataclasses import dataclass, field
from enum import Enum
from contextlib import contextmanager
from rq import Queue, Worker


# --- m√≥dulos de tu proyecto -----------------------------------------------
from pentest.config import REDIS_URL, SAFE_DOMAIN, DEFAULT_TIMEOUT
from pentest.exceptions import ScanError
from pentest.recon import recon
from pentest.fingerprint import fingerprint
from pentest.nuclei_scan import nuclei_scan
from pentest.leaks import check_leaks
from pentest.typosquat import check_typosquats
from pentest.cve_scan import cve_scan
from pentest.nmap_scan import nmap_scan
from pentest.security_config import security_config_scan
from pentest.dir_brute import dir_brute_scan
from pentest.threat_intel import check_threat_feeds

from pentest.report import build_pdf, send_notification
from pentest.cisa_kev import cisa_kev_monitor
from pentest.greynoise import is_ip_malicious_greynoise
from pentest.premium_adaptive_scan import premium_adaptive_scan_wrapper

# Importaciones del sistema ML predictivo
try:
    from pentest.ml_predictive_analysis import MLPredictiveAnalyzer
    from pentest.ml_integration import MLIntegrationManager, enhance_scan_with_ml
    ML_AVAILABLE = True
except ImportError as e:
    logging.getLogger(__name__).warning(f"Sistema ML no disponible: {e}")
    ML_AVAILABLE = False
    MLPredictiveAnalyzer = None
    MLIntegrationManager = None
    enhance_scan_with_ml = None

# Importaciones del sistema WAF
try:
    from pentest.waf_integration import WAFIntegratedScanner
    from pentest.waf_handler import WAFType
    WAF_AVAILABLE = True
except ImportError as e:
    logging.getLogger(__name__).warning(f"Sistema WAF no disponible: {e}")
    WAF_AVAILABLE = False
    WAFIntegratedScanner = None
    WAFType = None

# ---------------------------------------------------------------------------
# CONFIGURACI√ìN Y TIPOS
# ---------------------------------------------------------------------------

class ScanStage(Enum):
    """Estados posibles del escaneo."""
    QUEUED = "queued"
    WORKING = "working"
    COMPLETED = "completed"
    FAILED = "failed"

@dataclass
class ScanProgress:
    """Modelo para el progreso del escaneo."""
    job_id: str
    stage: ScanStage
    step: str
    percentage: int
    error: Optional[str] = None
    extra_data: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        """Convierte el progreso a diccionario para serializaci√≥n."""
        return {
            "state": self.stage.value,
            "step": self.step,
            "pct": self.percentage,
            "error": self.error,
            **self.extra_data
        }

@dataclass
class ScanStep:
    """Modelo para un paso del escaneo."""
    key: str
    percentage: int
    runner: Callable
    dependencies: List[str] = field(default_factory=list)
    parallel: bool = True

@dataclass
class ScanResult:
    """Resultado de un escaneo completo."""
    success: bool
    message: str
    job_id: str
    report_path: Optional[str] = None
    error: Optional[str] = None
    metrics: Dict[str, int] = field(default_factory=dict)

# ---------------------------------------------------------------------------
# CONFIGURACI√ìN DE LOGGING MEJORADA
# ---------------------------------------------------------------------------

class DeduplicateFilter(logging.Filter):
    """Filtro optimizado para deduplicar mensajes de log repetidos."""
    
    def __init__(self, name: str = '', interval: int = 5, max_entries: int = 1000):
        super().__init__(name)
        self.last_messages: Dict[Tuple[str, str, str], dt.datetime] = {}
        self.deduplication_interval = interval
        self.max_entries = max_entries

    def filter(self, record: logging.LogRecord) -> bool:
        """Filtra mensajes duplicados con limpieza peri√≥dica."""
        message = record.getMessage()
        unique_key = (record.levelname, record.name, message)
        
        now = dt.datetime.now()
        
        # Limpieza peri√≥dica del cache
        if len(self.last_messages) > self.max_entries:
            cutoff = now - dt.timedelta(seconds=self.deduplication_interval * 2)
            self.last_messages = {
                k: v for k, v in self.last_messages.items() if v > cutoff
            }
        
        if unique_key in self.last_messages:
            last_time = self.last_messages[unique_key]
            if (now - last_time).total_seconds() < self.deduplication_interval:
                return False
        
        self.last_messages[unique_key] = now
        return True

def setup_logging():
    """Configuraci√≥n optimizada del sistema de logging."""
    logging.basicConfig(
        stream=sys.stdout,
        level=logging.DEBUG,  # Cambiar a DEBUG para obtener m√°s detalles
        format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S"
    )
    
    # Aplicar filtro de deduplicaci√≥n (desactivado temporalmente para depuraci√≥n)
    # dedupe_filter = DeduplicateFilter()
    # for handler in logging.root.handlers:
    #     handler.addFilter(dedupe_filter)

# ---------------------------------------------------------------------------
# GESTI√ìN DE REDIS MEJORADA
# ---------------------------------------------------------------------------

def get_redis_connection():
    return redis.Redis.from_url(REDIS_URL)

class RedisManager:
    """Gestor optimizado para operaciones Redis con pool de conexiones."""
    
    def __init__(self, url: str):
        self.pool = redis.ConnectionPool.from_url(
            url, 
            max_connections=20,
            retry_on_timeout=True,
            socket_keepalive=True,
            socket_keepalive_options={}
        )
        self._redis = redis.Redis(connection_pool=self.pool)
    
    def publish_progress(self, progress: ScanProgress) -> None:
        """Publica progreso con manejo de errores mejorado."""
        try:
            payload = json.dumps(progress.to_dict())
            pipe = self._redis.pipeline()
            pipe.publish(progress.job_id, payload)
            pipe.hset(f"rq:job:{progress.job_id}", "meta", 
                     json.dumps({"progress": progress.to_dict()}))
            pipe.execute()
        except redis.ConnectionError:
            logging.getLogger(__name__).warning(
                f"Redis connection failed for job {progress.job_id}"
            )
        except Exception as e:
            logging.getLogger(__name__).error(
                f"Unexpected Redis error for job {progress.job_id}: {e}"
            )
    
    def get_job_progress(self, job_id: str) -> Optional[Dict]:
        """Obtiene el progreso de un trabajo."""
        try:
            meta = self._redis.hget(f"rq:job:{job_id}", "meta")
            if meta:
                return json.loads(meta).get("progress")
        except Exception:
            pass
        return None
    
    def blpop(self, key: str, timeout: int = 0) -> Tuple[str, str]:
        """Operaci√≥n BLPOP con manejo mejorado."""
        return self._redis.blpop(key, timeout)

# ---------------------------------------------------------------------------
# PIPELINE DE ESCANEO OPTIMIZADO
# ---------------------------------------------------------------------------

class ScanPipeline:
    """Pipeline optimizado para escaneos de seguridad."""
    
    def __init__(self):
        self.redis_manager = RedisManager(REDIS_URL)
        self.logger = logging.getLogger(self.__class__.__name__)
        self._setup_steps()
        
        # Inicializar sistema ML si est√° disponible
        self.ml_manager = None
        if ML_AVAILABLE:
            try:
                self.ml_manager = MLIntegrationManager()
                self.logger.info("ü§ñ [ML] Sistema ML predictivo inicializado")
            except Exception as e:
                self.logger.warning(f"ü§ñ [ML] Error inicializando sistema ML: {e}")
                self.ml_manager = None
        else:
            self.logger.info("ü§ñ [ML] Sistema ML no disponible")
        
        # Inicializar sistema WAF si est√° disponible
        self.waf_scanner = None
        self.detected_waf = None
        if WAF_AVAILABLE:
            try:
                self.waf_scanner = WAFIntegratedScanner()
                self.logger.info("üõ°Ô∏è [WAF] Sistema WAF inicializado")
            except Exception as e:
                self.logger.warning(f"üõ°Ô∏è [WAF] Error inicializando sistema WAF: {e}")
                self.waf_scanner = None
        else:
            self.logger.info("üõ°Ô∏è [WAF] Sistema WAF no disponible")
        
        self.logger.info("üöÄ [PIPELINE] ScanPipeline inicializado")
    
    def _setup_steps(self):
        """Configuraci√≥n de pasos con dependencias expl√≠citas."""
        self.steps = {
            "waf_detection": ScanStep(
                key="waf_detection", 
                percentage=5, 
                runner=self._detect_waf,
                parallel=False  # Debe ejecutarse primero
            ),
            "recon": ScanStep(
                key="recon", 
                percentage=10, 
                runner=recon,
                dependencies=["waf_detection"],
                parallel=False  # Depende de WAF detection
            ),
            "finger": ScanStep(
                key="finger", 
                percentage=30, 
                runner=fingerprint,
                dependencies=["recon"],
                parallel=False  # Depende de recon
            ),
            "nuclei": ScanStep(
                key="nuclei", 
                percentage=55, 
                runner=nuclei_scan,
                dependencies=["finger"]
            ),
            "leaks": ScanStep(
                key="leaks", 
                percentage=85, 
                runner=check_leaks
            ),
            "typos": ScanStep(
                key="typos", 
                percentage=90, 
                runner=check_typosquats
            ),
            "cve": ScanStep(
                key="cve", 
                percentage=95, 
                runner=cve_scan,
                dependencies=["finger"]
            ),
            "nmap": ScanStep(
                key="nmap", 
                percentage=96, 
                runner=nmap_scan,
                dependencies=["finger"]
            ),
            "cisa_kev": ScanStep(
                key="cisa_kev", 
                percentage=97, 
                runner=cisa_kev_monitor
            ),
            "greynoise": ScanStep(
                key="greynoise", 
                percentage=98, 
                runner=is_ip_malicious_greynoise
            ),
            "security_config": ScanStep(
                key="security_config", 
                percentage=98.5, 
                runner=security_config_scan,
                dependencies=["finger"]
            ),
            "dir_brute": ScanStep(
                key="dir_brute", 
                percentage=99, 
                runner=dir_brute_scan,
                dependencies=["finger"]
            ),
            "premium_adaptive": ScanStep(
                key="premium_adaptive", 
                percentage=99.5, 
                runner=premium_adaptive_scan_wrapper,
                dependencies=["finger"]
            ),
            "ml_analysis": ScanStep(
                key="ml_analysis", 
                percentage=99.8, 
                runner=self._run_ml_analysis,
                dependencies=["finger", "nuclei", "cve"],
                parallel=False  # Debe ejecutarse despu√©s de otros an√°lisis
            ),
        }



    @contextmanager
    def _temp_directory(self, domain: str):
        """Context manager para manejo seguro de directorios temporales."""
        tmp_dir = Path(tempfile.mkdtemp(prefix=f"scan_{domain}_"))
        self.logger.info(f"Created temporary directory: {tmp_dir}")
        try:
            yield tmp_dir
        finally:
            if tmp_dir.exists():
                shutil.rmtree(tmp_dir)
                self.logger.info(f"Cleaned up temporary directory: {tmp_dir}")

    def _validate_domain(self, domain: str) -> None:
        """Validaci√≥n mejorada del dominio."""
        if not domain or not isinstance(domain, str):
            raise ScanError("Dominio debe ser una cadena no vac√≠a")
        
        if not SAFE_DOMAIN.match(domain):
            raise ScanError(f"Dominio inv√°lido: {domain}")

    def _update_progress(self, job_id: str, step_key: str, percentage: int, 
                        stage: ScanStage = ScanStage.WORKING, 
                        error: str = None, **extra) -> None:
        """Actualiza el progreso del escaneo."""
        progress = ScanProgress(
            job_id=job_id,
            stage=stage,
            step=step_key,
            percentage=percentage,
            error=error,
            extra_data=extra
        )
        self.redis_manager.publish_progress(progress)

    def _execute_step(self, step: ScanStep, job_id: str, domain: str, 
                     tmp_dir: Path, files: Dict[str, Path], 
                     **kwargs) -> Tuple[str, Any]:
        """Ejecuta un paso individual del pipeline."""
        self._update_progress(job_id, step.key, step.percentage)
        self.logger.info(f"üöÄ [STEP] [{domain}] Ejecutando paso: {step.key} ({step.percentage}%)")
        
        # Configurar timeout espec√≠fico por paso
        step_timeout = self._get_step_timeout(step.key)
        self.logger.info(f"üöÄ [STEP] [{domain}] Timeout configurado para {step.key}: {step_timeout}s")
        
        try:
            # Preparar argumentos seg√∫n el paso
            args, step_kwargs = self._prepare_step_args(
                step.key, domain, tmp_dir, files, kwargs
            )
            
            self.logger.info(f"üöÄ [STEP] [{domain}] Llamando a {step.runner.__name__} con args={args} y kwargs={step_kwargs}")
            
            # Ejecutar con timeout usando ThreadPoolExecutor
            with ThreadPoolExecutor(max_workers=1) as executor:
                if asyncio.iscoroutinefunction(step.runner):
                    # Ejecutar coroutine en un nuevo loop de eventos
                    self.logger.info(f"üöÄ [STEP] [{domain}] Ejecutando funci√≥n async: {step.runner.__name__}")
                    future = executor.submit(self._run_async_step, step.runner, args, step_kwargs)
                else:
                    self.logger.info(f"üöÄ [STEP] [{domain}] Ejecutando funci√≥n sync: {step.runner.__name__}")
                    future = executor.submit(step.runner, *args, **step_kwargs)
                
                try:
                    result = future.result(timeout=step_timeout)
                except TimeoutError:
                    self.logger.error(f"üöÄ [STEP] [{domain}] ‚è∞ Timeout en {step.key} despu√©s de {step_timeout}s")
                    future.cancel()
                    raise ScanError(f"Timeout en paso {step.key} despu√©s de {step_timeout}s")
            
            self.logger.info(f"üöÄ [STEP] [{domain}] ‚úÖ Completado: {step.key}, resultado: {result}")
            
            # Verificar que el resultado es v√°lido
            if result and isinstance(result, Path):
                if result.exists():
                    self.logger.info(f"üöÄ [STEP] [{domain}] ‚úÖ Archivo resultado verificado: {result} (tama√±o: {result.stat().st_size} bytes)")
                else:
                    self.logger.warning(f"üöÄ [STEP] [{domain}] ‚ö†Ô∏è Archivo resultado no existe: {result}")
            else:
                self.logger.info(f"üöÄ [STEP] [{domain}] Resultado no es un archivo: {type(result)} - {result}")
            
            return step.key, result
            
        except Exception as e:
            error_msg = f"Error en {step.key}: {str(e)}"
            self.logger.error(f"üöÄ [STEP] [{domain}] ‚ùå {error_msg}", exc_info=True)
            self._update_progress(job_id, step.key, step.percentage, stage=ScanStage.FAILED, error=error_msg)
            raise ScanError(error_msg) # Relanzar la excepci√≥n para detener el pipeline

    def _get_step_timeout(self, step_key: str) -> int:
        """Obtiene el timeout espec√≠fico para cada paso del pipeline."""
        base_timeouts = {
            "waf_detection": 60,  # 1 minuto para detecci√≥n de WAF
            "recon": 300,      # 5 minutos para reconocimiento
            "finger": 180,     # 3 minutos para fingerprinting (puede ser bloqueado por Cloudflare)
            "nuclei": 600,     # 10 minutos para Nuclei
            "nmap": 900,       # 15 minutos para Nmap
            "leaks": 120,      # 2 minutos para HIBP
            "typos": 180,      # 3 minutos para typosquatting
            "dir_brute": 600,  # 10 minutos para directory brute force
            "cve": 300,        # 5 minutos para CVE
            "security_config": 180,  # 3 minutos para security config
            "greynoise": 60,   # 1 minuto para GreyNoise
            "cisa_kev": 120,   # 2 minutos para CISA KEV
            "premium_adaptive": 900,  # 15 minutos para escaneo premium adaptativo
            "ml_analysis": 240  # 4 minutos para an√°lisis ML
        }
        
        timeout = base_timeouts.get(step_key, DEFAULT_TIMEOUT)
        
        # Ajustar timeouts seg√∫n el WAF detectado
        if self.detected_waf and step_key in ["finger", "nuclei", "dir_brute", "security_config"]:
            if self.detected_waf.get("type") in ["Cloudflare", "AWS WAF", "Akamai"]:
                timeout = int(timeout * 1.5)  # Aumentar timeout 50% para WAFs estrictos
                self.logger.info(f"üõ°Ô∏è [WAF] Timeout ajustado para {step_key}: {timeout}s (WAF: {self.detected_waf.get('type')})")
        
        return timeout

    def _run_async_step(self, async_func, args, kwargs):
        """Ejecuta una funci√≥n as√≠ncrona en un nuevo loop de eventos."""
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            return loop.run_until_complete(async_func(*args, **kwargs))
        finally:
            loop.close()

    def _detect_waf(self, domain: str, tmp_dir: Path) -> Path:
        """Detecta el WAF del dominio objetivo y configura evasi√≥n global."""
        self.logger.info(f"üõ°Ô∏è [WAF] Iniciando detecci√≥n de WAF para {domain}")
        
        waf_result_file = tmp_dir / "waf_detection.json"
        
        if not self.waf_scanner:
            self.logger.warning(f"üõ°Ô∏è [WAF] Scanner WAF no disponible para {domain}")
            # Crear archivo de resultado vac√≠o
            waf_result = {
                "domain": domain,
                "waf_detected": False,
                "waf_type": None,
                "confidence": 0,
                "details": "WAF scanner no disponible",
                "timestamp": dt.datetime.now().isoformat()
            }
            with open(waf_result_file, "w") as f:
                json.dump(waf_result, f, indent=2)
            return waf_result_file
        
        try:
            # Detectar WAF usando el scanner integrado
            waf_info = self.waf_scanner.detect_waf(f"https://{domain}")
            
            # Guardar informaci√≥n del WAF detectado
            self.detected_waf = waf_info
            
            # Aplicar configuraci√≥n de evasi√≥n global
            if waf_info and waf_info.get("type"):
                self._apply_waf_evasion_config(waf_info)
            
            # Preparar resultado
            waf_result = {
                "domain": domain,
                "waf_detected": bool(waf_info and waf_info.get("type")),
                "waf_type": waf_info.get("type") if waf_info else None,
                "confidence": waf_info.get("confidence", 0) if waf_info else 0,
                "details": waf_info.get("details", "") if waf_info else "No WAF detectado",
                "timestamp": dt.datetime.now().isoformat(),
                "evasion_techniques": waf_info.get("recommended_techniques", []) if waf_info else []
            }
            
            self.logger.info(f"üõ°Ô∏è [WAF] Detecci√≥n completada para {domain}: {waf_result['waf_type'] or 'No detectado'}")
            
        except Exception as e:
            self.logger.error(f"üõ°Ô∏è [WAF] Error detectando WAF para {domain}: {e}")
            waf_result = {
                "domain": domain,
                "waf_detected": False,
                "waf_type": None,
                "confidence": 0,
                "details": f"Error en detecci√≥n: {str(e)}",
                "timestamp": dt.datetime.now().isoformat(),
                "error": str(e)
            }
        
        # Guardar resultado en archivo
        with open(waf_result_file, "w") as f:
            json.dump(waf_result, f, indent=2)
        
        return waf_result_file
    
    def _apply_waf_evasion_config(self, waf_info: Dict) -> None:
        """Aplica configuraci√≥n de evasi√≥n global basada en el WAF detectado."""
        waf_type = waf_info.get("type")
        if not waf_type:
            return
        
        self.logger.info(f"üõ°Ô∏è [WAF] Aplicando configuraci√≥n de evasi√≥n para {waf_type}")
        
        # Configuraciones espec√≠ficas por tipo de WAF
        if waf_type == "Cloudflare":
            # Configuraci√≥n para Cloudflare
            os.environ["WAF_EVASION_MODE"] = "cloudflare"
            os.environ["HTTP_DELAY"] = "2.0"  # Delay m√°s alto
            os.environ["USER_AGENT_ROTATION"] = "true"
            
        elif waf_type == "AWS WAF":
            # Configuraci√≥n para AWS WAF
            os.environ["WAF_EVASION_MODE"] = "aws_waf"
            os.environ["HTTP_DELAY"] = "1.5"
            os.environ["REQUEST_ENCODING"] = "url_encode"
            
        elif waf_type == "Akamai":
            # Configuraci√≥n para Akamai
            os.environ["WAF_EVASION_MODE"] = "akamai"
            os.environ["HTTP_DELAY"] = "1.0"
            os.environ["HEADER_MANIPULATION"] = "true"
            
        else:
            # Configuraci√≥n gen√©rica
            os.environ["WAF_EVASION_MODE"] = "generic"
            os.environ["HTTP_DELAY"] = "1.0"
        
        self.logger.info(f"üõ°Ô∏è [WAF] Configuraci√≥n de evasi√≥n aplicada para {waf_type}")

    def _prepare_step_args(self, step_key: str, domain: str, tmp_dir: Path, 
                          files: Dict[str, Path], kwargs: Dict) -> Tuple[tuple, Dict]:
        """Prepara argumentos espec√≠ficos para cada paso."""
        args = []
        step_kwargs = {}
        
        if step_key == "waf_detection":
            args = [domain, tmp_dir]
        elif step_key == "recon":
            args = [domain, tmp_dir]
        elif step_key == "finger":
            args = [files.get("recon"), tmp_dir]
        elif step_key == "leaks":
            args = [domain, tmp_dir, kwargs.get("hibp_api_key")]
        elif step_key == "nuclei":
            finger_file = files.get("finger")
            if finger_file is None:
                return (), {}
            args = [finger_file, tmp_dir, kwargs.get("full_nuclei_scan", False)]
        elif step_key == "nmap":
            finger_file = files.get("finger")
            if finger_file is None:
                return (), {}
            args = [finger_file, tmp_dir, True]
        elif step_key == "typos":
            args = [domain, tmp_dir]
        elif step_key == "dir_brute":
            finger_file = files.get("finger")
            if finger_file is None:
                return (), {}
            args = [finger_file, tmp_dir]
            step_kwargs.update(self._get_dir_brute_config(finger_file))
        elif step_key in ["cve", "security_config"]: 
            finger_file = files.get("finger")
            if finger_file is None:
                return (), {} # Return empty args if finger_file is None
            args = [finger_file, tmp_dir]
        elif step_key == "greynoise":
            api_key = os.environ.get("GREYNOISE_API_KEY")
            if not api_key:
                self.logger.error("GREYNOISE_API_KEY no configurada. El paso greynoise no se ejecutar√°.")
                raise ScanError("GREYNOISE_API_KEY no configurada. Por favor, configura la variable de entorno.")
            args = [domain, tmp_dir, api_key]
            self.logger.debug(f"[{domain}] Argumentos preparados para greynoise: domain={domain}, tmp_dir={tmp_dir}, api_key_present={bool(api_key)}")
        elif step_key == "cisa_kev":
            tech_stack = self._get_dir_brute_config(files.get("finger")).get("tech_stack")
            # Ensure tech_stack is always a list, even if None is returned
            tech_stack_for_cisa = tech_stack if tech_stack is not None else []
            args = [domain, tmp_dir, tech_stack_for_cisa]
        elif step_key == "premium_adaptive":
            finger_file = files.get("finger")
            if finger_file is None:
                return (), {}
            args = [domain, tmp_dir]  # premium_adaptive_scan_wrapper espera (target: str, output_dir: Path)
            step_kwargs["finger_file"] = finger_file  # Pasar finger_file como kwarg
        elif step_key == "ml_analysis":
            # El an√°lisis ML necesita acceso a todos los archivos de resultados
            args = [domain, tmp_dir, files]
        
        return tuple(args), step_kwargs

    def _get_dir_brute_config(self, httpx_file: Optional[Path]) -> Dict:
        """Configuraci√≥n optimizada para directory brute force."""
        config = {
            'extensions': ['php', 'html', 'js', 'asp', 'aspx', 'txt', 'zip', 'rar', 'tar.gz'],
            'auto_extend': True,
            'max_concurrent_requests': 50,  # Reducido para evitar sobrecarga
            'delay_between_requests': 0.01,
            'filter_status_codes': [301, 404],
            'filter_content_length': None,
            'tech_stack': None
        }
        
        # Detectar tech stack de los resultados de httpx
        if httpx_file and httpx_file.exists():
            try:
                with open(httpx_file, "r") as f:
                    httpx_results = json.load(f)
                    
                for host_result in httpx_results:
                    tech_list = host_result.get("tech", [])
                    for tech in tech_list:
                        tech_lower = tech.lower()
                        if "wordpress" in tech_lower:
                            config['tech_stack'] = "wordpress"
                            return config
                        elif "laravel" in tech_lower:
                            config['tech_stack'] = "laravel"
                            return config
                    
                    # Si no hay tecnolog√≠a prioritaria, usar la primera
                    if tech_list and not config['tech_stack']:
                        config['tech_stack'] = tech_list[0]
                        
            except (json.JSONDecodeError, IOError) as e:
                self.logger.warning(f"Error leyendo httpx results: {e}")
        
        return config

    def _get_execution_order(self) -> List[List[str]]:
        """Determina el orden de ejecuci√≥n basado en dependencias."""
        # Implementaci√≥n simple por capas
        layers = [
            ["waf_detection"],  # Capa 0: detecci√≥n de WAF (primer paso)
            ["recon"],  # Capa 1: reconocimiento (depende de WAF)
            ["finger"],  # Capa 2: fingerprinting (depende de recon)
            # Capa 3: depende de finger y pueden ejecutarse en paralelo
            ["nuclei", "cve", "nmap", "security_config", "dir_brute", "cisa_kev", "greynoise", "premium_adaptive"],
            # Capa 4: independientes
            ["leaks", "typos"],
            # Capa 5: an√°lisis ML (depende de resultados anteriores)
            ["ml_analysis"]
        ]
        return layers

    def _safe_count_json(self, path: Optional[Path]) -> int:
        """Cuenta elementos en archivo JSON de forma segura."""
        if not path or not path.exists():
            return 0
        try:
            with open(path, "r") as f:
                data = json.load(f)
                if isinstance(data, list):
                    return len(data)
                elif isinstance(data, dict):
                    # Detectar tipo de archivo por su estructura
                    filename = path.name.lower() if path else ""
                    
                    # Para nuclei: contar elementos en 'findings'
                    if 'nuclei' in filename and 'findings' in data:
                        findings = data.get('findings', [])
                        return len(findings) if isinstance(findings, list) else 0
                    
                    # Para dir_brute: contar elementos en 'directories' o 'results'
                    elif 'dir_brute' in filename:
                        if 'directories' in data:
                            dirs = data.get('directories', [])
                            return len(dirs) if isinstance(dirs, list) else 0
                        elif 'results' in data:
                            results = data.get('results', [])
                            return len(results) if isinstance(results, list) else 0
                    
                    # Para security_config: contar elementos en 'results' o 'findings'
                    elif 'security_config' in filename:
                        if 'results' in data:
                            results = data.get('results', [])
                            return len(results) if isinstance(results, list) else 0
                        elif 'findings' in data:
                            findings = data.get('findings', [])
                            return len(findings) if isinstance(findings, list) else 0
                    
                    # Para cves: contar elementos en 'cves' o 'vulnerabilities'
                    elif 'cve' in filename:
                        if 'cves' in data:
                            cves = data.get('cves', [])
                            return len(cves) if isinstance(cves, list) else 0
                        elif 'vulnerabilities' in data:
                            vulns = data.get('vulnerabilities', [])
                            return len(vulns) if isinstance(vulns, list) else 0
                    
                    # Para cisa_kev: contar elementos en 'vulnerabilities' o 'kev_matches'
                    elif 'cisa_kev' in filename:
                        if 'vulnerabilities' in data:
                            vulns = data.get('vulnerabilities', [])
                            return len(vulns) if isinstance(vulns, list) else 0
                        elif 'kev_matches' in data:
                            matches = data.get('kev_matches', [])
                            return len(matches) if isinstance(matches, list) else 0
                    
                    # Para leaks: contar elementos en 'breaches' o 'leaks'
                    elif 'leak' in filename:
                        if 'breaches' in data:
                            breaches = data.get('breaches', [])
                            return len(breaches) if isinstance(breaches, list) else 0
                        elif 'leaks' in data:
                            leaks = data.get('leaks', [])
                            return len(leaks) if isinstance(leaks, list) else 0
                    
                    # Para typosquats: contar elementos en 'typosquats' o 'domains'
                    elif 'typo' in filename:
                        if 'typosquats' in data:
                            typos = data.get('typosquats', [])
                            return len(typos) if isinstance(typos, list) else 0
                        elif 'domains' in data:
                            domains = data.get('domains', [])
                            return len(domains) if isinstance(domains, list) else 0
                    
                    # Para premium_adaptive: contar elementos en 'findings' o 'vulnerabilities'
                    elif 'premium_adaptive' in filename:
                        if 'findings' in data:
                            findings = data.get('findings', [])
                            return len(findings) if isinstance(findings, list) else 0
                        elif 'vulnerabilities' in data:
                            vulns = data.get('vulnerabilities', [])
                            return len(vulns) if isinstance(vulns, list) else 0
                        elif 'recommendations' in data:
                            recs = data.get('recommendations', [])
                            return len(recs) if isinstance(recs, list) else 0
                    
                    # Para GreyNoise y otros: si hay datos, consideramos que hay 1 resultado
                    return 1 if data else 0
                return 0
        except Exception:
            return 0

    def _count_lines(self, path: Optional[Path]) -> int:
        """Cuenta l√≠neas no vac√≠as en un archivo."""
        if not path or not path.exists():
            return 0
        try:
            with open(path, "r") as f:
                return len([line for line in f if line.strip()])
        except Exception:
            return 0

    def _enhance_with_threat_intel(self, nmap_file: Path) -> None:
        """Enriquece resultados de Nmap con threat intelligence."""
        if not nmap_file.exists():
            return
        
        try:
            with open(nmap_file, "r") as f:
                nmap_results = json.load(f)
            
            updated_results = []
            for host_data in nmap_results:
                ip_address = host_data.get("ip_address")
                if ip_address:
                    try:
                        threat_hits = check_threat_feeds(ip_address)
                        if threat_hits:
                            host_data["threat_intel"] = threat_hits
                    except Exception as e:
                        self.logger.warning(f"Error checking threat intel for {ip_address}: {e}")
                
                updated_results.append(host_data)
            
            with open(nmap_file, "w") as f:
                json.dump(updated_results, f, indent=2)
            
            self.logger.info("Threat intelligence added to Nmap results")
            
        except Exception as e:
            self.logger.error(f"Error enhancing Nmap results with threat intel: {e}")

    def _run_ml_analysis(self, domain: str, tmp_dir: Path, files: Dict[str, Path], **kwargs) -> Path:
        """Ejecuta an√°lisis ML predictivo sobre los resultados del escaneo."""
        self.logger.info(f"ü§ñ [ML] Iniciando an√°lisis ML para {domain}")
        
        if not self.ml_manager:
            self.logger.warning(f"ü§ñ [ML] Sistema ML no disponible, saltando an√°lisis")
            # Crear archivo vac√≠o para mantener consistencia
            ml_file = tmp_dir / f"{domain}_ml_analysis.json"
            ml_file.write_text(json.dumps({
                "status": "skipped",
                "reason": "ML system not available",
                "timestamp": dt.datetime.utcnow().isoformat()
            }))
            return ml_file
        
        try:
            # Recopilar todos los resultados de escaneo
            scan_result = {
                "scan_id": f"ml_analysis_{int(time.time())}",
                "target_domain": domain,
                "timestamp": dt.datetime.utcnow().isoformat(),
                "scan_type": "comprehensive",
                "status": "completed",
                "technologies": [],
                "vulnerabilities": [],
                "open_ports": [],
                "network_info": {},
                "security_headers": {},
                "geo_location": {}
            }
            
            # Extraer datos de fingerprinting
            if "finger" in files and files["finger"] and files["finger"].exists():
                try:
                    with open(files["finger"], "r") as f:
                        finger_data = json.load(f)
                    
                    for host_result in finger_data:
                        if "tech" in host_result:
                            for tech in host_result["tech"]:
                                scan_result["technologies"].append({
                                    "name": tech,
                                    "confidence": 85  # Valor por defecto
                                })
                        
                        if "status_code" in host_result:
                            scan_result["status_code"] = host_result["status_code"]
                        
                        if "content_length" in host_result:
                            scan_result["content_length"] = host_result["content_length"]
                        
                        if "response_time" in host_result:
                            scan_result["network_info"]["avg_response_time"] = host_result["response_time"]
                        
                        # Extraer headers de seguridad
                        headers = host_result.get("headers", {})
                        scan_result["security_headers"] = {
                            "strict_transport_security": "strict-transport-security" in headers,
                            "content_security_policy": "content-security-policy" in headers,
                            "x_frame_options": "x-frame-options" in headers
                        }
                        
                        break  # Solo procesar el primer host
                        
                except Exception as e:
                    self.logger.warning(f"ü§ñ [ML] Error procesando fingerprint data: {e}")
            
            # Extraer vulnerabilidades de Nuclei
            if "nuclei" in files and files["nuclei"] and files["nuclei"].exists():
                try:
                    with open(files["nuclei"], "r") as f:
                        nuclei_data = json.load(f)
                    
                    findings = nuclei_data.get("findings", [])
                    for finding in findings:
                        scan_result["vulnerabilities"].append({
                            "id": finding.get("template-id", "unknown"),
                            "title": finding.get("info", {}).get("name", "Unknown"),
                            "severity": finding.get("info", {}).get("severity", "UNKNOWN").upper(),
                            "description": finding.get("info", {}).get("description", "")
                        })
                        
                except Exception as e:
                    self.logger.warning(f"ü§ñ [ML] Error procesando nuclei data: {e}")
            
            # Extraer vulnerabilidades de CVE
            if "cve" in files and files["cve"] and files["cve"].exists():
                try:
                    with open(files["cve"], "r") as f:
                        cve_data = json.load(f)
                    
                    cves = cve_data.get("cves", [])
                    for cve in cves:
                        scan_result["vulnerabilities"].append({
                            "id": cve.get("id", "unknown"),
                            "title": cve.get("summary", "Unknown CVE"),
                            "severity": cve.get("severity", "UNKNOWN").upper(),
                            "cvss_score": cve.get("cvss_score", 0),
                            "description": cve.get("description", "")
                        })
                        
                except Exception as e:
                    self.logger.warning(f"ü§ñ [ML] Error procesando CVE data: {e}")
            
            # Extraer puertos de Nmap
            if "nmap" in files and files["nmap"] and files["nmap"].exists():
                try:
                    with open(files["nmap"], "r") as f:
                        nmap_data = json.load(f)
                    
                    for host in nmap_data:
                        ports = host.get("ports", [])
                        for port_info in ports:
                            if port_info.get("state") == "open":
                                scan_result["open_ports"].append(port_info.get("port", 0))
                        break  # Solo procesar el primer host
                        
                except Exception as e:
                    self.logger.warning(f"ü§ñ [ML] Error procesando nmap data: {e}")
            
            # Ejecutar an√°lisis ML
            self.logger.info(f"ü§ñ [ML] Ejecutando an√°lisis predictivo...")
            enhanced_result = self.ml_manager.enhance_scan_result(scan_result, domain)
            
            # Guardar resultados ML
            ml_file = tmp_dir / f"{domain}_ml_analysis.json"
            ml_results = {
                "ml_analysis": {
                    "prediction": enhanced_result.ml_prediction.to_dict() if enhanced_result.ml_prediction else None,
                    "risk_assessment": enhanced_result.risk_assessment,
                    "confidence_score": enhanced_result.confidence_score,
                    "recommendations": enhanced_result.recommendations,
                    "processing_time_ms": enhanced_result.processing_time * 1000,  # Convertir a ms
                    "model_version": getattr(enhanced_result, 'model_version', 'v1.0')
                },
                "original_scan": scan_result,
                "timestamp": dt.datetime.utcnow().isoformat(),
                "status": "completed"
            }
            
            with open(ml_file, "w") as f:
                json.dump(ml_results, f, indent=2)
            
            self.logger.info(f"ü§ñ [ML] An√°lisis ML completado. Archivo: {ml_file}")
            self.logger.info(f"ü§ñ [ML] Nivel de riesgo: {enhanced_result.risk_assessment.get('risk_level', 'unknown')}")
            self.logger.info(f"ü§ñ [ML] Puntuaci√≥n de riesgo: {enhanced_result.risk_assessment.get('overall_risk', 0)}/100")
            
            return ml_file
            
        except Exception as e:
            self.logger.error(f"ü§ñ [ML] Error en an√°lisis ML: {e}", exc_info=True)
            # Crear archivo de error
            ml_file = tmp_dir / f"{domain}_ml_analysis.json"
            ml_file.write_text(json.dumps({
                "status": "error",
                "error": str(e),
                "timestamp": dt.datetime.utcnow().isoformat()
            }))
            return ml_file

    def generate_pdf(self, domain: str, recipient_email: str, job_id: str = None, *, debug: bool = False, full_nuclei_scan: bool = False, hibp_api_key: str = None) -> ScanResult:
        self.logger.info(f"üöÄ [PIPELINE] Iniciando escaneo completo para {domain}")
        self.logger.info(f"üöÄ [PIPELINE] Dominio recibido (tipo: {type(domain).__name__}, longitud: {len(domain)}): '{domain}'")
        self.logger.info(f"üöÄ [PIPELINE] Representaci√≥n cruda del dominio: {repr(domain)}")
        self.logger.info(f"üöÄ [PIPELINE] Par√°metros: recipient={recipient_email}, job_id={job_id}, debug={debug}, full_nuclei_scan={full_nuclei_scan}")

        """Ejecuta el pipeline completo de escaneo y genera el informe PDF.

        Args:
            domain: Dominio a escanear
            recipient_email: Email para notificaciones
            job_id: ID del trabajo (se genera autom√°ticamente si no se proporciona)
            hibp_api_key: API key para Have I Been Pwned
            debug: Modo debug (mantiene archivos temporales)
            full_nuclei_scan: Ejecutar escaneo completo con Nuclei
            
        Returns:
            ScanResult con el resultado del escaneo
        """
        # Validaciones iniciales
        self.logger.info(f"üöÄ [PIPELINE] Validando dominio: {domain}")
        self._validate_domain(domain)
        self.logger.info(f"üöÄ [PIPELINE] ‚úÖ Dominio v√°lido: {domain}")
        
        if not job_id:
            job_id = f"standalone:{dt.datetime.utcnow().isoformat()}"
            self.logger.info(f"üöÄ [PIPELINE] Job ID generado: {job_id}")
        
        self._update_progress(job_id, "queue", 5, ScanStage.QUEUED)
        self.logger.info(f"üöÄ [PIPELINE] Progreso actualizado: queue (5%)")
        
        tmp_dir = None
        try:
            with self._temp_directory(domain) as tmp_dir_context:
                tmp_dir = tmp_dir_context
                self.logger.info(f"üöÄ [PIPELINE] Directorio temporal creado: {tmp_dir}")
                
                # Verificar que el directorio temporal es escribible
                try:
                    test_file = tmp_dir / "test_write.txt"
                    test_file.write_text("test")
                    test_file.unlink()
                    self.logger.info(f"üöÄ [PIPELINE] ‚úÖ Directorio temporal escribible")
                except Exception as e:
                    self.logger.error(f"üöÄ [PIPELINE] ‚ùå Error de permisos en directorio temporal: {e}")
                    raise ScanError(f"Error de permisos en directorio temporal: {e}")
                
                files: Dict[str, Path] = {}
                execution_layers = self._get_execution_order()
                self.logger.info(f"üöÄ [PIPELINE] Orden de ejecuci√≥n: {execution_layers}")
                
                # Ejecutar por capas respetando dependencias
                for layer_idx, layer_steps in enumerate(execution_layers):
                    self.logger.info(f"üöÄ [PIPELINE] Ejecutando capa {layer_idx}: {layer_steps}")
                    if layer_idx < 2:  # Capas secuenciales (0 y 1)
                        for step_key in layer_steps:
                            if step_key not in self.steps:
                                self.logger.warning(f"üöÄ [PIPELINE] ‚ö†Ô∏è Paso {step_key} no encontrado en steps")
                                continue
                            
                            self.logger.info(f"üöÄ [PIPELINE] Ejecutando paso secuencial: {step_key}")
                            step = self.steps[step_key]
                            _, result = self._execute_step(
                                step, job_id, domain, tmp_dir, files,
                                hibp_api_key=hibp_api_key,
                                full_nuclei_scan=full_nuclei_scan
                            )
                            files[step_key] = result
                            self.logger.info(f"üöÄ [PIPELINE] ‚úÖ Paso {step_key} completado. Archivo resultado: {result}")
                            
                            # Verificar que el archivo existe
                            if result and isinstance(result, Path) and result.exists():
                                self.logger.info(f"üöÄ [PIPELINE] ‚úÖ Archivo {step_key} verificado: {result} (tama√±o: {result.stat().st_size} bytes)")
                            else:
                                self.logger.warning(f"üöÄ [PIPELINE] ‚ö†Ô∏è Archivo {step_key} no existe o es None: {result}")
                    
                    else:  # Capas paralelas (2 en adelante)
                        with ThreadPoolExecutor(max_workers=min(len(layer_steps), 6)) as executor:
                            future_to_step = {}
                            
                            for step_key in layer_steps:
                                if step_key not in self.steps:
                                    self.logger.warning(f"üöÄ [PIPELINE] ‚ö†Ô∏è Paso {step_key} no encontrado en steps")
                                    continue
                                
                                self.logger.info(f"üöÄ [PIPELINE] Encolando paso paralelo: {step_key}")
                                step = self.steps[step_key]
                                future = executor.submit(
                                    self._execute_step,
                                    step, job_id, domain, tmp_dir, files,
                                    hibp_api_key=hibp_api_key,
                                    full_nuclei_scan=full_nuclei_scan
                                )
                                future_to_step[future] = step_key
                            
                            # Procesar resultados
                            for future in as_completed(future_to_step):
                                step_key = future_to_step[future]
                                try:
                                    _, result = future.result()
                                    files[step_key] = result
                                    self.logger.info(f"üöÄ [PIPELINE] ‚úÖ Paso paralelo {step_key} completado. Archivo resultado: {result}")
                                    
                                    # Verificar que el archivo existe
                                    if result and isinstance(result, Path) and result.exists():
                                        self.logger.info(f"üöÄ [PIPELINE] ‚úÖ Archivo {step_key} verificado: {result} (tama√±o: {result.stat().st_size} bytes)")
                                    else:
                                        self.logger.warning(f"üöÄ [PIPELINE] ‚ö†Ô∏è Archivo {step_key} no existe o es None: {result}")
                                except Exception as e:
                                    self.logger.error(f"üöÄ [PIPELINE] ‚ùå Error in parallel step {step_key}: {e}")
                                    files[step_key] = None # Set to None if step fails to prevent further errors
                
                # Resumen de archivos generados
                self.logger.info(f"üöÄ [PIPELINE] Resumen de archivos generados:")
                for step_key, file_path in files.items():
                    if file_path and isinstance(file_path, Path) and file_path.exists():
                        self.logger.info(f"üöÄ [PIPELINE]   ‚úÖ {step_key}: {file_path} ({file_path.stat().st_size} bytes)")
                    else:
                        self.logger.warning(f"üöÄ [PIPELINE]   ‚ùå {step_key}: {file_path} (no existe o es None)")
                
                # Enriquecer con threat intelligence
                if "nmap" in files and files["nmap"]:
                    self.logger.info(f"üöÄ [PIPELINE] Enriqueciendo con threat intelligence")
                    self._enhance_with_threat_intel(files["nmap"])
                else:
                    self.logger.info(f"üöÄ [PIPELINE] No hay archivo nmap para enriquecer con threat intelligence")
                
                # Generar informe PDF
                self.logger.info(f"üöÄ [PIPELINE] Generando informe PDF para {domain}")
                self.logger.info(f"üöÄ [PIPELINE] Archivos que se pasar√°n a build_pdf:")
                self.logger.info(f"üöÄ [PIPELINE]   httpx_file: {files.get('finger')}")
                self.logger.info(f"üöÄ [PIPELINE]   nuclei_file: {files.get('nuclei')}")
                self.logger.info(f"üöÄ [PIPELINE]   leaks_file: {files.get('leaks')}")
                self.logger.info(f"üöÄ [PIPELINE]   typosquats_file: {files.get('typos')}")
                self.logger.info(f"üöÄ [PIPELINE]   dir_brute_file: {files.get('dir_brute')}")
                self.logger.info(f"üöÄ [PIPELINE]   cves_file: {files.get('cve')}")
                self.logger.info(f"üöÄ [PIPELINE]   nmap_file: {files.get('nmap')}")
                self.logger.info(f"üöÄ [PIPELINE]   security_config_file: {files.get('security_config')}")
                self.logger.info(f"üöÄ [PIPELINE]   cisa_kev_file: {files.get('cisa_kev')}")
                self.logger.info(f"üöÄ [PIPELINE]   greynoise_file: {files.get('greynoise')}")
                self.logger.info(f"üöÄ [PIPELINE]   premium_adaptive_file: {files.get('premium_adaptive')}")
                self.logger.info(f"üöÄ [PIPELINE]   waf_file: {files.get('waf_detection')}")
                
                pdf_path = build_pdf(
                    domain, recipient_email, tmp_dir,
                    httpx_file=files.get("finger"),
                    nuclei_file=files.get("nuclei"),
                    leaks_file=files.get("leaks"),
                    typosquats_file=files.get("typos"),
                    dir_brute_file=files.get("dir_brute"),
                    
                    cves_file=files.get("cve"),
                    nmap_file=files.get("nmap"),
                    security_config_file=files.get("security_config"),
                    cisa_kev_file=files.get("cisa_kev"),
                    greynoise_file=files.get("greynoise"),
                    premium_adaptive_file=files.get("premium_adaptive"),
                    ml_file=files.get("ml_analysis"),
                    waf_file=files.get("waf_detection")
                )
                self.logger.info(f"üöÄ [PIPELINE] ‚úÖ PDF generado: {pdf_path}")
                
                # Calcular m√©tricas
                threat_intel_hits = 0
                nmap_file_path = files.get("nmap")
                if nmap_file_path and nmap_file_path.exists():
                    try:
                        with open(nmap_file_path, 'r') as f:
                            nmap_data_for_metrics = json.load(f)
                            if isinstance(nmap_data_for_metrics, list):
                                for item in nmap_data_for_metrics:
                                    if item.get("threat_intel"):
                                        threat_intel_hits += 1
                    except (json.JSONDecodeError, IOError) as e:
                        self.logger.warning(f"Error al leer o decodificar JSON de {nmap_file_path} para m√©tricas de threat intel: {e}")

                metrics = {
                    "subdomains": self._count_lines(files.get("recon")),
                    "vulnerabilities": self._safe_count_json(files.get("nuclei")),
                    "cves": self._safe_count_json(files.get("cve")),
                    "ports": self._safe_count_json(files.get("nmap")),
                    "cisa_kev_vulnerabilities": self._safe_count_json(files.get("cisa_kev")),
                    "greynoise_malicious_ips": self._safe_count_json(files.get("greynoise")),
                    "leaked_credentials": self._safe_count_json(files.get("leaks")),
                    "typosquats": self._safe_count_json(files.get("typos")),
                    "exposed_directories": self._safe_count_json(files.get("dir_brute")),
                    "security_misconfigurations": self._safe_count_json(files.get("security_config")),
                    "premium_adaptive_findings": self._safe_count_json(files.get("premium_adaptive")),
                    "threat_intel_hits": threat_intel_hits
                }
                
                self._update_progress(job_id, "report_generation", 100, ScanStage.COMPLETED)
                self.logger.info(f"üöÄ [PIPELINE] ‚úÖ Escaneo completado para {domain}. Informe en: {pdf_path}")
                self.logger.info(f"üöÄ [PIPELINE] M√©tricas finales: {metrics}")
                
                # Enviar notificaci√≥n por email con el informe adjunto
                from pentest.report import send_notification
                try:
                    self.logger.info(f"üöÄ [PIPELINE] Enviando notificaci√≥n por email a {recipient_email}")
                    email_sent = send_notification(
                        job_id=job_id,
                        status_message="Completado exitosamente",
                        status_type="success",
                        pdf_path=pdf_path,
                        recipient_email=recipient_email,
                        subc=metrics["subdomains"],
                        vulc=metrics["vulnerabilities"],
                        domain=domain,
                        cisa_kev_vulnerabilities=metrics["cisa_kev_vulnerabilities"],
                        greynoise_malicious_ips=metrics["greynoise_malicious_ips"],
                        threat_intel_hits=metrics["threat_intel_hits"]
                    )
                    if email_sent:
                        self.logger.info(f"üöÄ [PIPELINE] ‚úÖ Notificaci√≥n enviada exitosamente a {recipient_email}")
                    else:
                        self.logger.warning(f"üöÄ [PIPELINE] ‚ö†Ô∏è No se pudo enviar la notificaci√≥n a {recipient_email}")
                except Exception as e:
                    self.logger.error(f"üöÄ [PIPELINE] ‚ùå Error enviando notificaci√≥n: {e}")
                
                return ScanResult(
                    success=True,
                    message="Escaneo completado exitosamente",
                    job_id=job_id,
                    report_path=pdf_path,
                    metrics=metrics
                )
        except ScanError as e:
            self.logger.error(f"üöÄ [PIPELINE] ‚ùå Error de escaneo para {domain}: {e}")
            self._update_progress(job_id, "error", 100, ScanStage.FAILED, error=str(e))
            
            # Enviar notificaci√≥n de error
            from pentest.report import send_notification
            try:
                self.logger.info(f"üöÄ [PIPELINE] Enviando notificaci√≥n de error a {recipient_email}")
                send_notification(
                    job_id=job_id,
                    status_message=f"Error de escaneo: {e}",
                    status_type="failed",
                    pdf_path=None,
                    recipient_email=recipient_email,
                    subc=0,
                    vulc=0,
                    domain=domain,
                    cisa_kev_vulnerabilities=0,
                    greynoise_malicious_ips=0,
                    threat_intel_hits=0
                )
            except Exception as email_error:
                self.logger.error(f"üöÄ [PIPELINE] ‚ùå Error enviando notificaci√≥n de error: {email_error}")
            
            return ScanResult(
                success=False,
                message=f"Error de escaneo: {e}",
                job_id=job_id,
                report_path=None,
                metrics={}
            )
        except Exception as e:
            self.logger.error(f"üöÄ [PIPELINE] ‚ùå Error fatal durante el escaneo de {domain}: {e}", exc_info=True)
            self._update_progress(job_id, "error", 100, ScanStage.FAILED, error=str(e))
            
            # Enviar notificaci√≥n de error fatal
            from pentest.report import send_notification
            try:
                self.logger.info(f"üöÄ [PIPELINE] Enviando notificaci√≥n de error fatal a {recipient_email}")
                send_notification(
                    job_id=job_id,
                    status_message=f"Error fatal durante el escaneo: {e}",
                    status_type="failed",
                    pdf_path=None,
                    recipient_email=recipient_email,
                    subc=0,
                    vulc=0,
                    domain=domain,
                    cisa_kev_vulnerabilities=0,
                    greynoise_malicious_ips=0,
                    threat_intel_hits=0
                )
            except Exception as email_error:
                self.logger.error(f"üöÄ [PIPELINE] ‚ùå Error enviando notificaci√≥n de error fatal: {email_error}")
            
            return ScanResult(
                success=False,
                message=f"Error fatal durante el escaneo: {e}",
                job_id=job_id,
                report_path=None,
                metrics={}
            )
        finally:
            if tmp_dir and not debug:
                self.logger.info(f"üöÄ [PIPELINE] Limpiando directorio temporal para {domain}: {tmp_dir}")
                shutil.rmtree(tmp_dir, ignore_errors=True)
            elif debug:
                self.logger.info(f"üöÄ [PIPELINE] Modo debug activado. Directorio temporal conservado: {tmp_dir}")

# ---------------------------------------------------------------------------
# WORKER MEJORADO
# ---------------------------------------------------------------------------



# ---------------------------------------------------------------------------
# FUNCIONES DE COMPATIBILIDAD
# ---------------------------------------------------------------------------



def start_worker():
    """Inicia un worker de RQ para procesar la cola de escaneos."""
    # Configurar logging expl√≠citamente para el worker
    setup_logging()
    
    logger = logging.getLogger(__name__)
    logger.info(f"üöÄ [WORKER] Intentando conectar a Redis en {REDIS_URL}")
    
    redis_conn = redis.from_url(REDIS_URL)
    logger.info("üöÄ [WORKER] ‚úÖ Conectado exitosamente a Redis")
    
    worker = Worker(['scan_queue'], connection=redis_conn)
    logger.info("üöÄ [WORKER] Worker RQ inicializado. Escuchando en 'scan_queue'...")
    
    # Configurar logging adicional para RQ
    import rq.worker
    rq_logger = logging.getLogger('rq.worker')
    rq_logger.setLevel(logging.INFO)
    
    logger.info("üöÄ [WORKER] Iniciando worker...")
    worker.work(logging_level='INFO')

# ---------------------------------------------------------------------------
# PUNTO DE ENTRADA
# ---------------------------------------------------------------------------

from rq import get_current_job, Queue, Worker

def _run_scan_job(domain: str, recipient_email: str, hibp_api_key: str = None, debug: bool = False, full_nuclei_scan: bool = False):
    """Funci√≥n wrapper para ejecutar el pipeline de escaneo dentro de un trabajo de RQ."""
    # Configurar logging para asegurar que aparezcan los logs
    setup_logging()
    
    job = get_current_job()
    job_id = job.id if job else "unknown_job"
    
    logger = logging.getLogger(__name__)
    logger.info(f"üöÄ [JOB] Iniciando escaneo para dominio: {domain} (Job ID: {job_id})")
    logger.info(f"üöÄ [JOB] Dominio recibido (tipo: {type(domain).__name__}, longitud: {len(domain)}): '{domain}'")
    logger.info(f"üöÄ [JOB] Representaci√≥n cruda del dominio: {repr(domain)}")
    logger.info(f"üöÄ [JOB] Par√°metros: recipient_email={recipient_email}, debug={debug}, full_nuclei_scan={full_nuclei_scan}")

    try:
        pipeline = ScanPipeline()
        logger.info(f"üöÄ [JOB] Pipeline creado, ejecutando generate_pdf...")
        
        result = pipeline.generate_pdf(
            domain=domain,
            recipient_email=recipient_email,
            job_id=job_id, # Pasar el job_id
            hibp_api_key=hibp_api_key,
            debug=debug,
            full_nuclei_scan=full_nuclei_scan
        )
        
        logger.info(f"üöÄ [JOB] ‚úÖ Escaneo completado exitosamente para {domain}")
        logger.info(f"üöÄ [JOB] Resultado: success={result.success}, report_path={result.report_path}")
        return result
        
    except Exception as e:
        logger.error(f"üöÄ [JOB] ‚ùå Error ejecutando escaneo para {domain} (Job ID: {job_id}): {e}", exc_info=True)
        # Actualizar el estado del progreso a FAILED
        redis_manager = RedisManager(REDIS_URL)
        progress = ScanProgress(job_id=job_id, stage=ScanStage.FAILED, step='Error', percentage=100, error=str(e))
        redis_manager.publish_progress(progress)
        raise  # Re-lanzar la excepci√≥n para que RQ la capture

def enqueue_scan(domain: str, recipient_email: str, hibp_api_key: str = None, debug: bool = False, full_nuclei_scan: bool = False):
    """Encola un trabajo de escaneo en RQ."""
    q = Queue('scan_queue', connection=redis.from_url(REDIS_URL))
    job = q.enqueue(
        _run_scan_job,
        domain,
        recipient_email,
        hibp_api_key=hibp_api_key,
        debug=debug,
        full_nuclei_scan=full_nuclei_scan,
        result_ttl=86400, # Mantener el resultado por 24 horas
        job_timeout=3600 # Tiempo m√°ximo de ejecuci√≥n del trabajo (1 hora)
    )
    logging.getLogger(__name__).info(f"Enqueued scan job for {domain} with ID: {job.id}")
    return job.id

if __name__ == "__main__":
    setup_logging()
    
    if len(sys.argv) > 1 and sys.argv[1] == "worker":
        start_worker()
    else:
        # Modo de prueba
        import argparse
        parser = argparse.ArgumentParser(description="Scanner de seguridad")
        parser.add_argument("--domain", required=True, help="Dominio a escanear")
        parser.add_argument("--email", required=True, help="Email para notificaciones")
        parser.add_argument("--debug", action="store_true", help="Modo debug")
        
        args = parser.parse_args()
        
        pipeline = ScanPipeline()
        result = pipeline.generate_pdf(
            domain=args.domain,
            recipient_email=args.email,
            debug=args.debug
        )
        
        print(f"Resultado: {result.status}")
        if result.report_path:
            print(f"Informe: {result.report_path}")

# === SISTEMA MEJORADO DE DETECCION DE TECNOLOGIAS ===
try:
    # Configurar logger
    logger = logging.getLogger(__name__)
    
    # Configuraci√≥n para el integrador mejorado
    enhanced_config = {
        "enable_enhanced_detection": True,
        "enable_web_content_analysis": True,
        "enable_legacy_fallback": True,
        "cache_enabled": True,
        "validate_tools_on_startup": False  # Evitar validaci√≥n en startup para producci√≥n
    }
    
    # Inicializar integrador mejorado
    enhanced_integrator = EnhancedTechIntegrator(config=enhanced_config)
    
    # Intentar inicializar el sistema mejorado
    if enhanced_integrator.initialize():
        logger.info("Sistema mejorado de deteccion disponible e inicializado")
        
        # Reemplazar funci√≥n de fingerprinting si est√° disponible
        def enhanced_fingerprint_wrapper(target, timeout=30):
            """Wrapper para fingerprinting mejorado con fallback."""
            try:
                result = enhanced_integrator.detect_technologies(target)
                if result and hasattr(result, 'technologies') and result.technologies:
                    # Convertir resultado a formato compatible
                    return {
                        'technologies': result.technologies,
                        'confidence_scores': {tech.get('name', 'unknown'): tech.get('confidence', 0) for tech in result.technologies},
                        'detection_methods': list(result.tool_coverage.keys()),
                        'detection_time': result.detection_time,
                        'timestamp': int(time.time())
                    }
            except Exception as e:
                logger.warning(f"Fingerprinting mejorado fall√≥, usando b√°sico: {e}")
            
            # Fallback al sistema original
            from .fingerprint import fingerprint_target
            return fingerprint_target(target, timeout)
        
        # Reemplazar la funci√≥n original
        globals()['fingerprint_target'] = enhanced_fingerprint_wrapper
        
    else:
        logger.warning("Sistema mejorado no se pudo inicializar, usando sistema b√°sico")
        
except Exception as e:
    logger = logging.getLogger(__name__)
    logger.error(f"Error inicializando sistema mejorado: {e}")
    logger.info("Continuando con sistema b√°sico")
# === FIN SISTEMA MEJORADO ===
