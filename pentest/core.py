"""Pipeline principal del escáner de seguridad con notificaciones de progreso."""

from __future__ import annotations
import json, logging, os, shutil, sys, tempfile, datetime as dt
from pathlib import Path
from typing import Dict, Optional, Callable, List

import redis

# --- módulos de tu proyecto -----------------------------------------------
from pentest.config import REDIS_URL, SAFE_DOMAIN
from pentest.exceptions import ScanError
from pentest.recon         import recon
from pentest.fingerprint   import fingerprint
from pentest.nuclei_scan   import nuclei_scan
from pentest.tls_scan      import tls_scan
from pentest.leaks         import check_leaks
from pentest.typosquat     import check_typosquats
from pentest.report        import build_pdf, send_notification
# ---------------------------------------------------------------------------

log = logging.getLogger("pentest")
logging.basicConfig(
    stream=sys.stdout,
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(name)s: %(message)s"
)

rds: redis.Redis = redis.from_url(REDIS_URL)

# Paso, porcentaje acumulado aproximado y función que lo ejecuta ------------
Step = Dict[str, object]
STEPS: List[Step] = [
    {"key": "recon",   "pct": 10, "runner": recon},
    {"key": "finger",  "pct": 30, "runner": fingerprint},
    {"key": "nuclei",  "pct": 55, "runner": nuclei_scan},
    {"key": "tls",     "pct": 70, "runner": tls_scan},
    {"key": "leaks",   "pct": 85, "runner": check_leaks},
    {"key": "typos",   "pct": 95, "runner": check_typosquats},
]

def _publish(job_id: str, payload: Dict):
    """Envía el progreso por Pub/Sub y lo guarda en Redis hash."""
    try:
        rds.publish(job_id, json.dumps(payload))
        # Guardar progreso en Redis hash
        rds.hset(f"rq:job:{job_id}", "meta", json.dumps({"progress": payload}))
    except Exception:  # conexión Redis caída → no interrumpir el scan
        pass

def _mark(job_id: str, step_key: str, pct: int, extra: Dict | None = None):
    data = {"state": "working", "step": step_key, "pct": pct}
    if extra:
        data.update(extra)
    _publish(job_id, data)

# ---------------------------------------------------------------------------
def generate_pdf(domain: str,
                 recipient_email: str,
                 job_id: str | None = None,
                 *,
                 hibp_api_key: str | None = None,
                 debug: bool = False) -> Dict:
    """
    Ejecuta el pipeline completo y devuelve dict con estado final.
    """
    if not SAFE_DOMAIN.match(domain):
        raise ScanError(f"Dominio inválido: {domain}")

    if not job_id:
        job_id = f"standalone:{dt.datetime.utcnow().isoformat()}"

    tmp_dir = Path(tempfile.mkdtemp(prefix=f"scan_{domain}_"))
    log.info("➜ [%s] TMP %s", domain, tmp_dir)

    files: Dict[str, Path] = {}

    try:
        # -------- pipeline dinámico -------------------------------------------------
        for step in STEPS:
            key, pct, fn = step["key"], step["pct"], step["runner"]

            _mark(job_id, key, pct)
            log.info("→ [%s] Fase %s (%s %%)", domain, key, pct)

            match key:
                case "recon":
                    files["subdomains"] = fn(domain, tmp_dir)

                case "finger":
                    files["httpx"] = fn(files["subdomains"], tmp_dir)

                case "nuclei":
                    files["nuclei"] = fn(files["httpx"], tmp_dir)

                case "tls":
                    files["tls"] = fn(files["httpx"], tmp_dir)

                case "leaks":
                    files["leaks"] = fn(domain, tmp_dir, hibp_api_key)

                case "typos":
                    files["typosquats"] = fn(domain, tmp_dir)

        # -------- PDF ---------------------------------------------------------------
        pdf_path = build_pdf(domain, tmp_dir, **files)
        _mark(job_id, "pdf", 100, {"report_path": str(pdf_path)})

        # -------- Notificación email (Best-Effort) ----------------------------------
        subc = _safe_count_json(files.get("httpx"))
        vulc = _safe_count_json(files.get("nuclei"))
        send_notification(domain, pdf_path, recipient_email, subc, vulc)

        final = Path(f"report_{domain}_{dt.datetime.utcnow():%Y%m%d%H%M}.pdf")
        shutil.copy(pdf_path, final)

        result = {"state": "finished", "report_path": str(final)}
        _publish(job_id, result)
        log.info("✅ [%s] Escaneo finalizado", domain)
        return result

    except Exception as e:
        log.exception("❌ [%s] Error en el escaneo", domain)
        err = {"state": "failed", "error": str(e)}
        _publish(job_id, err)
        raise

    finally:
        if not debug:
            shutil.rmtree(tmp_dir, ignore_errors=True)

# ---------------------------------------------------------------------------
def _safe_count_json(path: Path | None) -> int:
    if not path or not path.exists(): return 0
    try:
        with open(path, "r") as f:
            return len(json.load(f))
    except Exception:
        return 0
# ---------------------------------------------------------------------------
#  MODO WORKER "BÁSICO" (sigue utilizando tu cola BLPOP)
# ---------------------------------------------------------------------------
def start_worker():
    log.info("⌛ Worker BLPOP escuchando en redis queue 'scan_queue'")
    while True:
        _, raw = rds.blpop("scan_queue")
        try:
            req = json.loads(raw)
            job_id = f"job:{dt.datetime.utcnow().isoformat()}"
            _publish(job_id, {"state": "queued"})
            generate_pdf(req["domain"], req["email"], job_id)
        except Exception as e:
            log.error("Error procesando job %s – %s", raw, e)

if __name__ == "__main__":
    start_worker()