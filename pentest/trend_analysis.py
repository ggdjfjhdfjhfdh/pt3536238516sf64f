#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Sistema de análisis de tendencias para datos de seguridad.
Identifica patrones temporales, evolución de vulnerabilidades y predicciones de riesgo.
"""

import logging
import statistics
from collections import defaultdict, Counter
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Tuple
from enum import Enum
import json

log = logging.getLogger(__name__)


class TrendDirection(Enum):
    """Dirección de la tendencia."""
    INCREASING = "increasing"
    DECREASING = "decreasing"
    STABLE = "stable"
    VOLATILE = "volatile"


class TrendSignificance(Enum):
    """Significancia de la tendencia."""
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"


@dataclass
class TrendPoint:
    """Punto de datos en una tendencia."""
    timestamp: datetime
    value: float
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class TrendAnalysis:
    """Resultado del análisis de tendencias."""
    metric_name: str
    direction: TrendDirection
    significance: TrendSignificance
    slope: float
    correlation_coefficient: float
    confidence_interval: Tuple[float, float]
    data_points: List[TrendPoint]
    prediction: Optional[float] = None
    prediction_confidence: float = 0.0
    insights: List[str] = field(default_factory=list)
    recommendations: List[str] = field(default_factory=list)


@dataclass
class SecurityMetrics:
    """Métricas de seguridad para análisis temporal."""
    timestamp: datetime
    total_vulnerabilities: int = 0
    critical_vulnerabilities: int = 0
    high_vulnerabilities: int = 0
    medium_vulnerabilities: int = 0
    low_vulnerabilities: int = 0
    info_vulnerabilities: int = 0
    mean_cvss_score: float = 0.0
    exploitable_vulnerabilities: int = 0
    patched_vulnerabilities: int = 0
    new_vulnerabilities: int = 0
    resolved_vulnerabilities: int = 0
    attack_surface_score: float = 0.0
    compliance_score: float = 0.0
    risk_score: float = 0.0
    exposed_services: int = 0
    leaked_credentials: int = 0
    configuration_issues: int = 0


class TrendAnalysisEngine:
    """Motor de análisis de tendencias para datos de seguridad."""
    
    def __init__(self, lookback_days: int = 90):
        self.log = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        self.lookback_days = lookback_days
        self.historical_data: List[SecurityMetrics] = []
        
        # Umbrales para determinar significancia
        self.significance_thresholds = {
            'slope': {'high': 0.5, 'medium': 0.2, 'low': 0.1},
            'correlation': {'high': 0.7, 'medium': 0.4, 'low': 0.2}
        }
    
    def add_historical_data(self, metrics: SecurityMetrics) -> None:
        """Añade datos históricos para análisis."""
        self.historical_data.append(metrics)
        
        # Mantener solo datos dentro del período de lookback
        cutoff_date = datetime.now() - timedelta(days=self.lookback_days)
        self.historical_data = [
            m for m in self.historical_data 
            if m.timestamp >= cutoff_date
        ]
        
        # Ordenar por timestamp
        self.historical_data.sort(key=lambda x: x.timestamp)
    
    def analyze_all_trends(self, current_data: Dict[str, Any]) -> Dict[str, Any]:
        """Ejecuta análisis completo de tendencias."""
        self.log.info("Iniciando análisis de tendencias")
        
        try:
            # Convertir datos actuales a métricas
            current_metrics = self._convert_to_metrics(current_data)
            self.add_historical_data(current_metrics)
            
            if len(self.historical_data) < 3:
                self.log.warning("Datos históricos insuficientes para análisis de tendencias")
                return self._create_minimal_analysis(current_metrics)
            
            # Analizar tendencias por métrica
            trend_analyses = {}
            
            # Métricas principales
            metrics_to_analyze = [
                'total_vulnerabilities',
                'critical_vulnerabilities',
                'high_vulnerabilities',
                'mean_cvss_score',
                'exploitable_vulnerabilities',
                'attack_surface_score',
                'risk_score',
                'exposed_services',
                'leaked_credentials',
                'configuration_issues'
            ]
            
            for metric in metrics_to_analyze:
                trend_analyses[metric] = self._analyze_metric_trend(metric)
            
            # Análisis de patrones específicos
            pattern_analysis = self._analyze_security_patterns()
            
            # Predicciones
            predictions = self._generate_predictions()
            
            # Insights y recomendaciones
            insights = self._generate_insights(trend_analyses)
            recommendations = self._generate_recommendations(trend_analyses, pattern_analysis)
            
            # Resumen ejecutivo
            executive_summary = self._create_executive_summary(trend_analyses, pattern_analysis)
            
            return {
                'trend_analyses': {k: self._trend_to_dict(v) for k, v in trend_analyses.items()},
                'pattern_analysis': pattern_analysis,
                'predictions': predictions,
                'insights': insights,
                'recommendations': recommendations,
                'executive_summary': executive_summary,
                'data_quality': self._assess_data_quality(),
                'analysis_metadata': {
                    'analysis_date': datetime.now().isoformat(),
                    'data_points': len(self.historical_data),
                    'lookback_days': self.lookback_days,
                    'earliest_data': self.historical_data[0].timestamp.isoformat() if self.historical_data else None,
                    'latest_data': self.historical_data[-1].timestamp.isoformat() if self.historical_data else None
                }
            }
            
        except Exception as e:
            self.log.error(f"Error en análisis de tendencias: {e}")
            return {'error': str(e), 'trend_analyses': {}, 'insights': [], 'recommendations': []}
    
    def _convert_to_metrics(self, data: Dict[str, Any]) -> SecurityMetrics:
        """Convierte datos de pentest a métricas estructuradas."""
        nuclei_data = data.get('nuclei_data', [])
        nmap_data = data.get('nmap_data', [])
        leaks_data = data.get('leaks_data', [])
        security_config_data = data.get('security_config_data', [])
        cves_data = data.get('cves_data', [])
        
        # Contar vulnerabilidades por severidad
        severity_counts = Counter()
        cvss_scores = []
        exploitable_count = 0
        
        for vuln in nuclei_data:
            severity = vuln.get('info', {}).get('severity', 'info')
            severity_counts[severity] += 1
            
            # Extraer CVSS si está disponible
            cvss = vuln.get('info', {}).get('classification', {}).get('cvss-score')
            if cvss:
                try:
                    cvss_scores.append(float(cvss))
                except (ValueError, TypeError):
                    pass
            
            # Determinar si es explotable
            tags = vuln.get('info', {}).get('tags', [])
            if any(tag in ['rce', 'sqli', 'xss', 'lfi', 'rfi'] for tag in tags):
                exploitable_count += 1
        
        # Servicios expuestos
        exposed_services = len([p for p in nmap_data if p.get('state') == 'open'])
        
        # Calcular puntuación de superficie de ataque
        attack_surface_score = (
            len(nuclei_data) * 2 +
            exposed_services * 1.5 +
            len(security_config_data) * 1
        )
        
        # Calcular puntuación de riesgo simple
        risk_score = (
            severity_counts.get('critical', 0) * 4 +
            severity_counts.get('high', 0) * 3 +
            severity_counts.get('medium', 0) * 2 +
            severity_counts.get('low', 0) * 1
        )
        
        return SecurityMetrics(
            timestamp=datetime.now(),
            total_vulnerabilities=len(nuclei_data),
            critical_vulnerabilities=severity_counts.get('critical', 0),
            high_vulnerabilities=severity_counts.get('high', 0),
            medium_vulnerabilities=severity_counts.get('medium', 0),
            low_vulnerabilities=severity_counts.get('low', 0),
            info_vulnerabilities=severity_counts.get('info', 0),
            mean_cvss_score=statistics.mean(cvss_scores) if cvss_scores else 0.0,
            exploitable_vulnerabilities=exploitable_count,
            attack_surface_score=attack_surface_score,
            risk_score=risk_score,
            exposed_services=exposed_services,
            leaked_credentials=len(leaks_data),
            configuration_issues=len(security_config_data)
        )
    
    def _analyze_metric_trend(self, metric_name: str) -> TrendAnalysis:
        """Analiza tendencia de una métrica específica."""
        # Extraer valores de la métrica
        data_points = []
        for metrics in self.historical_data:
            value = getattr(metrics, metric_name, 0)
            data_points.append(TrendPoint(
                timestamp=metrics.timestamp,
                value=float(value)
            ))
        
        if len(data_points) < 2:
            return TrendAnalysis(
                metric_name=metric_name,
                direction=TrendDirection.STABLE,
                significance=TrendSignificance.LOW,
                slope=0.0,
                correlation_coefficient=0.0,
                confidence_interval=(0.0, 0.0),
                data_points=data_points
            )
        
        # Calcular tendencia lineal
        x_values = [(point.timestamp - data_points[0].timestamp).days for point in data_points]
        y_values = [point.value for point in data_points]
        
        # Regresión lineal simple
        n = len(x_values)
        sum_x = sum(x_values)
        sum_y = sum(y_values)
        sum_xy = sum(x * y for x, y in zip(x_values, y_values))
        sum_x2 = sum(x * x for x in x_values)
        
        # Calcular pendiente y correlación
        if n * sum_x2 - sum_x * sum_x != 0:
            slope = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x * sum_x)
            
            # Coeficiente de correlación
            mean_x = sum_x / n
            mean_y = sum_y / n
            
            numerator = sum((x - mean_x) * (y - mean_y) for x, y in zip(x_values, y_values))
            denominator_x = sum((x - mean_x) ** 2 for x in x_values)
            denominator_y = sum((y - mean_y) ** 2 for y in y_values)
            
            if denominator_x > 0 and denominator_y > 0:
                correlation = numerator / (denominator_x * denominator_y) ** 0.5
            else:
                correlation = 0.0
        else:
            slope = 0.0
            correlation = 0.0
        
        # Determinar dirección
        if abs(slope) < 0.01:
            direction = TrendDirection.STABLE
        elif slope > 0:
            direction = TrendDirection.INCREASING
        else:
            direction = TrendDirection.DECREASING
        
        # Verificar volatilidad
        if len(y_values) > 2:
            volatility = statistics.stdev(y_values) / (statistics.mean(y_values) + 0.001)
            if volatility > 0.5:
                direction = TrendDirection.VOLATILE
        
        # Determinar significancia
        significance = self._determine_significance(abs(slope), abs(correlation))
        
        # Calcular intervalo de confianza (simplificado)
        if len(y_values) > 1:
            std_dev = statistics.stdev(y_values)
            confidence_interval = (mean_y - 1.96 * std_dev, mean_y + 1.96 * std_dev)
        else:
            confidence_interval = (y_values[0], y_values[0]) if y_values else (0.0, 0.0)
        
        # Generar predicción
        prediction = None
        prediction_confidence = 0.0
        if abs(correlation) > 0.3 and len(data_points) >= 5:
            # Predicción para 30 días en el futuro
            future_x = x_values[-1] + 30
            prediction = slope * future_x + (mean_y - slope * mean_x)
            prediction_confidence = min(abs(correlation), 0.8)
        
        # Generar insights
        insights = self._generate_metric_insights(metric_name, direction, slope, correlation, y_values)
        
        return TrendAnalysis(
            metric_name=metric_name,
            direction=direction,
            significance=significance,
            slope=slope,
            correlation_coefficient=correlation,
            confidence_interval=confidence_interval,
            data_points=data_points,
            prediction=prediction,
            prediction_confidence=prediction_confidence,
            insights=insights
        )
    
    def _determine_significance(self, slope: float, correlation: float) -> TrendSignificance:
        """Determina la significancia de una tendencia."""
        slope_thresholds = self.significance_thresholds['slope']
        correlation_thresholds = self.significance_thresholds['correlation']
        
        if slope >= slope_thresholds['high'] and correlation >= correlation_thresholds['high']:
            return TrendSignificance.HIGH
        elif slope >= slope_thresholds['medium'] and correlation >= correlation_thresholds['medium']:
            return TrendSignificance.MEDIUM
        else:
            return TrendSignificance.LOW
    
    def _analyze_security_patterns(self) -> Dict[str, Any]:
        """Analiza patrones específicos de seguridad."""
        patterns = {
            'vulnerability_lifecycle': self._analyze_vulnerability_lifecycle(),
            'attack_surface_evolution': self._analyze_attack_surface_evolution(),
            'risk_concentration': self._analyze_risk_concentration(),
            'seasonal_patterns': self._analyze_seasonal_patterns(),
            'correlation_patterns': self._analyze_metric_correlations()
        }
        
        return patterns
    
    def _analyze_vulnerability_lifecycle(self) -> Dict[str, Any]:
        """Analiza el ciclo de vida de vulnerabilidades."""
        if len(self.historical_data) < 3:
            return {}
        
        # Calcular tiempo promedio de resolución
        resolution_times = []
        discovery_rates = []
        
        for i in range(1, len(self.historical_data)):
            current = self.historical_data[i]
            previous = self.historical_data[i-1]
            
            # Tasa de descubrimiento
            new_vulns = max(0, current.total_vulnerabilities - previous.total_vulnerabilities)
            days_diff = (current.timestamp - previous.timestamp).days
            if days_diff > 0:
                discovery_rate = new_vulns / days_diff
                discovery_rates.append(discovery_rate)
        
        avg_discovery_rate = statistics.mean(discovery_rates) if discovery_rates else 0
        
        return {
            'average_discovery_rate': round(avg_discovery_rate, 2),
            'discovery_trend': 'increasing' if len(discovery_rates) > 1 and discovery_rates[-1] > discovery_rates[0] else 'stable',
            'total_periods_analyzed': len(discovery_rates)
        }
    
    def _analyze_attack_surface_evolution(self) -> Dict[str, Any]:
        """Analiza la evolución de la superficie de ataque."""
        if len(self.historical_data) < 2:
            return {}
        
        surface_scores = [m.attack_surface_score for m in self.historical_data]
        service_counts = [m.exposed_services for m in self.historical_data]
        
        surface_trend = 'increasing' if surface_scores[-1] > surface_scores[0] else 'decreasing'
        service_trend = 'increasing' if service_counts[-1] > service_counts[0] else 'decreasing'
        
        return {
            'surface_score_trend': surface_trend,
            'exposed_services_trend': service_trend,
            'current_surface_score': surface_scores[-1],
            'surface_score_change': surface_scores[-1] - surface_scores[0],
            'volatility': statistics.stdev(surface_scores) if len(surface_scores) > 1 else 0
        }
    
    def _analyze_risk_concentration(self) -> Dict[str, Any]:
        """Analiza la concentración de riesgo."""
        if len(self.historical_data) < 2:
            return {}
        
        latest = self.historical_data[-1]
        total_vulns = latest.total_vulnerabilities
        
        if total_vulns == 0:
            return {'risk_concentration': 'low', 'critical_ratio': 0}
        
        critical_ratio = latest.critical_vulnerabilities / total_vulns
        high_ratio = latest.high_vulnerabilities / total_vulns
        
        concentration = 'high' if critical_ratio > 0.2 or high_ratio > 0.4 else 'medium' if critical_ratio > 0.1 else 'low'
        
        return {
            'risk_concentration': concentration,
            'critical_ratio': round(critical_ratio, 3),
            'high_ratio': round(high_ratio, 3),
            'top_severity_percentage': round((critical_ratio + high_ratio) * 100, 1)
        }
    
    def _analyze_seasonal_patterns(self) -> Dict[str, Any]:
        """Analiza patrones estacionales (simplificado)."""
        if len(self.historical_data) < 4:
            return {}
        
        # Agrupar por día de la semana
        weekday_metrics = defaultdict(list)
        for metrics in self.historical_data:
            weekday = metrics.timestamp.weekday()
            weekday_metrics[weekday].append(metrics.total_vulnerabilities)
        
        # Calcular promedios por día
        weekday_averages = {}
        for weekday, values in weekday_metrics.items():
            if values:
                weekday_averages[weekday] = statistics.mean(values)
        
        return {
            'weekday_patterns': weekday_averages,
            'peak_day': max(weekday_averages.items(), key=lambda x: x[1])[0] if weekday_averages else None
        }
    
    def _analyze_metric_correlations(self) -> Dict[str, Any]:
        """Analiza correlaciones entre métricas."""
        if len(self.historical_data) < 3:
            return {}
        
        # Métricas para correlacionar
        metrics = {
            'total_vulnerabilities': [m.total_vulnerabilities for m in self.historical_data],
            'critical_vulnerabilities': [m.critical_vulnerabilities for m in self.historical_data],
            'exposed_services': [m.exposed_services for m in self.historical_data],
            'attack_surface_score': [m.attack_surface_score for m in self.historical_data],
            'risk_score': [m.risk_score for m in self.historical_data]
        }
        
        correlations = {}
        
        # Calcular correlaciones entre pares de métricas
        metric_names = list(metrics.keys())
        for i, metric1 in enumerate(metric_names):
            for metric2 in metric_names[i+1:]:
                values1 = metrics[metric1]
                values2 = metrics[metric2]
                
                if len(values1) == len(values2) and len(values1) > 1:
                    correlation = self._calculate_correlation(values1, values2)
                    if abs(correlation) > 0.3:  # Solo correlaciones significativas
                        correlations[f"{metric1}_vs_{metric2}"] = round(correlation, 3)
        
        return correlations
    
    def _calculate_correlation(self, x_values: List[float], y_values: List[float]) -> float:
        """Calcula coeficiente de correlación de Pearson."""
        if len(x_values) != len(y_values) or len(x_values) < 2:
            return 0.0
        
        n = len(x_values)
        mean_x = sum(x_values) / n
        mean_y = sum(y_values) / n
        
        numerator = sum((x - mean_x) * (y - mean_y) for x, y in zip(x_values, y_values))
        denominator_x = sum((x - mean_x) ** 2 for x in x_values)
        denominator_y = sum((y - mean_y) ** 2 for y in y_values)
        
        if denominator_x > 0 and denominator_y > 0:
            return numerator / (denominator_x * denominator_y) ** 0.5
        else:
            return 0.0
    
    def _generate_predictions(self) -> Dict[str, Any]:
        """Genera predicciones basadas en tendencias."""
        predictions = {}
        
        if len(self.historical_data) < 5:
            return predictions
        
        # Predicciones para métricas clave
        key_metrics = ['total_vulnerabilities', 'critical_vulnerabilities', 'risk_score']
        
        for metric in key_metrics:
            trend = self._analyze_metric_trend(metric)
            if trend.prediction is not None and trend.prediction_confidence > 0.3:
                predictions[metric] = {
                    'predicted_value': round(trend.prediction, 2),
                    'confidence': round(trend.prediction_confidence, 2),
                    'timeframe_days': 30,
                    'trend_direction': trend.direction.value
                }
        
        return predictions
    
    def _generate_metric_insights(self, metric_name: str, direction: TrendDirection, slope: float, correlation: float, values: List[float]) -> List[str]:
        """Genera insights específicos para una métrica."""
        insights = []
        
        # Insights por dirección de tendencia
        if direction == TrendDirection.INCREASING:
            if metric_name in ['total_vulnerabilities', 'critical_vulnerabilities', 'risk_score']:
                insights.append(f"⚠️ {metric_name.replace('_', ' ').title()} muestra tendencia creciente")
            elif metric_name in ['patched_vulnerabilities', 'resolved_vulnerabilities']:
                insights.append(f"✅ {metric_name.replace('_', ' ').title()} muestra mejora continua")
        
        elif direction == TrendDirection.DECREASING:
            if metric_name in ['total_vulnerabilities', 'critical_vulnerabilities', 'risk_score']:
                insights.append(f"✅ {metric_name.replace('_', ' ').title()} muestra tendencia decreciente")
            elif metric_name in ['patched_vulnerabilities', 'resolved_vulnerabilities']:
                insights.append(f"⚠️ {metric_name.replace('_', ' ').title()} muestra declive")
        
        elif direction == TrendDirection.VOLATILE:
            insights.append(f"📊 {metric_name.replace('_', ' ').title()} muestra alta volatilidad")
        
        # Insights por correlación
        if abs(correlation) > 0.7:
            insights.append(f"Tendencia muy consistente (correlación: {correlation:.2f})")
        elif abs(correlation) < 0.3:
            insights.append("Tendencia poco consistente - revisar factores externos")
        
        # Insights por valores extremos
        if values:
            current_value = values[-1]
            max_value = max(values)
            min_value = min(values)
            
            if current_value == max_value and len(values) > 2:
                insights.append("📈 Valor actual en máximo histórico")
            elif current_value == min_value and len(values) > 2:
                insights.append("📉 Valor actual en mínimo histórico")
        
        return insights
    
    def _generate_insights(self, trend_analyses: Dict[str, TrendAnalysis]) -> List[str]:
        """Genera insights generales del análisis."""
        insights = []
        
        # Contar tendencias por dirección
        direction_counts = Counter(analysis.direction for analysis in trend_analyses.values())
        
        if direction_counts[TrendDirection.INCREASING] > direction_counts[TrendDirection.DECREASING]:
            insights.append("🔺 La mayoría de métricas muestran tendencia creciente")
        elif direction_counts[TrendDirection.DECREASING] > direction_counts[TrendDirection.INCREASING]:
            insights.append("🔻 La mayoría de métricas muestran tendencia decreciente")
        
        # Identificar métricas de alta significancia
        high_significance = [name for name, analysis in trend_analyses.items() 
                           if analysis.significance == TrendSignificance.HIGH]
        
        if high_significance:
            insights.append(f"⚡ Tendencias de alta significancia: {', '.join(high_significance)}")
        
        # Identificar métricas volátiles
        volatile_metrics = [name for name, analysis in trend_analyses.items() 
                          if analysis.direction == TrendDirection.VOLATILE]
        
        if volatile_metrics:
            insights.append(f"📊 Métricas con alta volatilidad: {', '.join(volatile_metrics)}")
        
        return insights
    
    def _generate_recommendations(self, trend_analyses: Dict[str, TrendAnalysis], pattern_analysis: Dict[str, Any]) -> List[str]:
        """Genera recomendaciones basadas en el análisis."""
        recommendations = []
        
        # Recomendaciones por tendencias críticas
        critical_increasing = [name for name, analysis in trend_analyses.items() 
                             if analysis.direction == TrendDirection.INCREASING and 
                             name in ['critical_vulnerabilities', 'risk_score'] and
                             analysis.significance != TrendSignificance.LOW]
        
        if critical_increasing:
            recommendations.append("🚨 URGENTE: Implementar plan de remediación para tendencias críticas crecientes")
        
        # Recomendaciones por volatilidad
        volatile_count = sum(1 for analysis in trend_analyses.values() 
                           if analysis.direction == TrendDirection.VOLATILE)
        
        if volatile_count >= 3:
            recommendations.append("📊 Implementar monitoreo más frecuente debido a alta volatilidad")
        
        # Recomendaciones por superficie de ataque
        surface_evolution = pattern_analysis.get('attack_surface_evolution', {})
        if surface_evolution.get('surface_score_trend') == 'increasing':
            recommendations.append("🛡️ Revisar y reducir superficie de ataque expuesta")
        
        # Recomendaciones por concentración de riesgo
        risk_concentration = pattern_analysis.get('risk_concentration', {})
        if risk_concentration.get('risk_concentration') == 'high':
            recommendations.append("⚖️ Diversificar esfuerzos de seguridad - alta concentración de riesgo detectada")
        
        # Recomendaciones por predicciones
        for name, analysis in trend_analyses.items():
            if (analysis.prediction is not None and 
                analysis.prediction_confidence > 0.5 and 
                name in ['critical_vulnerabilities', 'risk_score']):
                
                if analysis.direction == TrendDirection.INCREASING:
                    recommendations.append(f"📈 Preparar recursos adicionales - {name} proyectado a aumentar")
        
        return recommendations
    
    def _create_executive_summary(self, trend_analyses: Dict[str, TrendAnalysis], pattern_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Crea resumen ejecutivo del análisis de tendencias."""
        # Métricas clave
        total_vulns_trend = trend_analyses.get('total_vulnerabilities')
        critical_vulns_trend = trend_analyses.get('critical_vulnerabilities')
        risk_score_trend = trend_analyses.get('risk_score')
        
        # Estado general
        improving_trends = sum(1 for analysis in trend_analyses.values() 
                             if analysis.direction == TrendDirection.DECREASING and 
                             analysis.metric_name in ['total_vulnerabilities', 'critical_vulnerabilities', 'risk_score'])
        
        worsening_trends = sum(1 for analysis in trend_analyses.values() 
                             if analysis.direction == TrendDirection.INCREASING and 
                             analysis.metric_name in ['total_vulnerabilities', 'critical_vulnerabilities', 'risk_score'])
        
        if improving_trends > worsening_trends:
            overall_trend = "Mejorando"
            trend_emoji = "📈✅"
        elif worsening_trends > improving_trends:
            overall_trend = "Empeorando"
            trend_emoji = "📉⚠️"
        else:
            overall_trend = "Estable"
            trend_emoji = "📊"
        
        return {
            'overall_trend': overall_trend,
            'trend_emoji': trend_emoji,
            'key_metrics': {
                'total_vulnerabilities': {
                    'direction': total_vulns_trend.direction.value if total_vulns_trend else 'unknown',
                    'significance': total_vulns_trend.significance.value if total_vulns_trend else 'unknown'
                },
                'critical_vulnerabilities': {
                    'direction': critical_vulns_trend.direction.value if critical_vulns_trend else 'unknown',
                    'significance': critical_vulns_trend.significance.value if critical_vulns_trend else 'unknown'
                },
                'risk_score': {
                    'direction': risk_score_trend.direction.value if risk_score_trend else 'unknown',
                    'significance': risk_score_trend.significance.value if risk_score_trend else 'unknown'
                }
            },
            'data_quality': self._assess_data_quality(),
            'analysis_confidence': self._calculate_overall_confidence(trend_analyses)
        }
    
    def _assess_data_quality(self) -> Dict[str, Any]:
        """Evalúa la calidad de los datos para el análisis."""
        data_points = len(self.historical_data)
        
        if data_points >= 10:
            quality = "Alta"
        elif data_points >= 5:
            quality = "Media"
        elif data_points >= 3:
            quality = "Baja"
        else:
            quality = "Insuficiente"
        
        # Verificar consistencia temporal
        if len(self.historical_data) > 1:
            time_gaps = []
            for i in range(1, len(self.historical_data)):
                gap = (self.historical_data[i].timestamp - self.historical_data[i-1].timestamp).days
                time_gaps.append(gap)
            
            avg_gap = statistics.mean(time_gaps) if time_gaps else 0
            gap_consistency = "Alta" if statistics.stdev(time_gaps) < 2 else "Media" if statistics.stdev(time_gaps) < 5 else "Baja"
        else:
            avg_gap = 0
            gap_consistency = "N/A"
        
        return {
            'quality_level': quality,
            'data_points': data_points,
            'average_time_gap_days': round(avg_gap, 1),
            'temporal_consistency': gap_consistency,
            'sufficient_for_prediction': data_points >= 5
        }
    
    def _calculate_overall_confidence(self, trend_analyses: Dict[str, TrendAnalysis]) -> float:
        """Calcula confianza general del análisis."""
        if not trend_analyses:
            return 0.0
        
        # Promedio de correlaciones absolutas
        correlations = [abs(analysis.correlation_coefficient) for analysis in trend_analyses.values()]
        avg_correlation = statistics.mean(correlations) if correlations else 0.0
        
        # Ajustar por cantidad de datos
        data_quality_factor = min(1.0, len(self.historical_data) / 10)
        
        overall_confidence = avg_correlation * data_quality_factor
        
        return round(overall_confidence, 2)
    
    def _create_minimal_analysis(self, current_metrics: SecurityMetrics) -> Dict[str, Any]:
        """Crea análisis mínimo cuando no hay suficientes datos históricos."""
        return {
            'trend_analyses': {},
            'pattern_analysis': {},
            'predictions': {},
            'insights': ["📊 Datos históricos insuficientes para análisis de tendencias"],
            'recommendations': [
                "📈 Continuar recopilando datos para análisis futuro",
                "🔄 Ejecutar escaneos de forma regular para establecer línea base"
            ],
            'executive_summary': {
                'overall_trend': 'Datos insuficientes',
                'trend_emoji': '📊',
                'data_quality': self._assess_data_quality()
            },
            'current_snapshot': {
                'total_vulnerabilities': current_metrics.total_vulnerabilities,
                'critical_vulnerabilities': current_metrics.critical_vulnerabilities,
                'risk_score': current_metrics.risk_score,
                'timestamp': current_metrics.timestamp.isoformat()
            }
        }
    
    def _trend_to_dict(self, trend: TrendAnalysis) -> Dict[str, Any]:
        """Convierte TrendAnalysis a diccionario."""
        return {
            'metric_name': trend.metric_name,
            'direction': trend.direction.value,
            'significance': trend.significance.value,
            'slope': round(trend.slope, 4),
            'correlation_coefficient': round(trend.correlation_coefficient, 3),
            'confidence_interval': [round(trend.confidence_interval[0], 2), round(trend.confidence_interval[1], 2)],
            'data_points_count': len(trend.data_points),
            'prediction': round(trend.prediction, 2) if trend.prediction is not None else None,
            'prediction_confidence': round(trend.prediction_confidence, 2),
            'insights': trend.insights,
            'recommendations': trend.recommendations
        }


# ============================================================================
# FUNCIONES DE UTILIDAD
# ============================================================================

def create_trend_engine(lookback_days: int = 90) -> TrendAnalysisEngine:
    """Factory function para crear motor de análisis de tendencias."""
    return TrendAnalysisEngine(lookback_days=lookback_days)


def analyze_security_trends(current_data: Dict[str, Any], historical_data: List[Dict[str, Any]] = None) -> Dict[str, Any]:
    """Función de conveniencia para análisis completo de tendencias."""
    engine = create_trend_engine()
    
    # Cargar datos históricos si se proporcionan
    if historical_data:
        for data in historical_data:
            metrics = engine._convert_to_metrics(data)
            engine.add_historical_data(metrics)
    
    return engine.analyze_all_trends(current_data)