#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Sistema de an√°lisis de tendencias para datos de seguridad.
Identifica patrones temporales, evoluci√≥n de vulnerabilidades y predicciones de riesgo.
"""

import logging
import statistics
from collections import defaultdict, Counter
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Tuple
from enum import Enum
import json

log = logging.getLogger(__name__)


class TrendDirection(Enum):
    """Direcci√≥n de la tendencia."""
    INCREASING = "increasing"
    DECREASING = "decreasing"
    STABLE = "stable"
    VOLATILE = "volatile"


class TrendSignificance(Enum):
    """Significancia de la tendencia."""
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"


@dataclass
class TrendPoint:
    """Punto de datos en una tendencia."""
    timestamp: datetime
    value: float
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class TrendAnalysis:
    """Resultado del an√°lisis de tendencias."""
    metric_name: str
    direction: TrendDirection
    significance: TrendSignificance
    slope: float
    correlation_coefficient: float
    confidence_interval: Tuple[float, float]
    data_points: List[TrendPoint]
    prediction: Optional[float] = None
    prediction_confidence: float = 0.0
    insights: List[str] = field(default_factory=list)
    recommendations: List[str] = field(default_factory=list)


@dataclass
class SecurityMetrics:
    """M√©tricas de seguridad para an√°lisis temporal."""
    timestamp: datetime
    total_vulnerabilities: int = 0
    critical_vulnerabilities: int = 0
    high_vulnerabilities: int = 0
    medium_vulnerabilities: int = 0
    low_vulnerabilities: int = 0
    info_vulnerabilities: int = 0
    mean_cvss_score: float = 0.0
    exploitable_vulnerabilities: int = 0
    patched_vulnerabilities: int = 0
    new_vulnerabilities: int = 0
    resolved_vulnerabilities: int = 0
    attack_surface_score: float = 0.0
    compliance_score: float = 0.0
    risk_score: float = 0.0
    exposed_services: int = 0
    leaked_credentials: int = 0
    configuration_issues: int = 0


class TrendAnalysisEngine:
    """Motor de an√°lisis de tendencias para datos de seguridad."""
    
    def __init__(self, lookback_days: int = 90):
        self.log = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        self.lookback_days = lookback_days
        self.historical_data: List[SecurityMetrics] = []
        
        # Umbrales para determinar significancia
        self.significance_thresholds = {
            'slope': {'high': 0.5, 'medium': 0.2, 'low': 0.1},
            'correlation': {'high': 0.7, 'medium': 0.4, 'low': 0.2}
        }
    
    def add_historical_data(self, metrics: SecurityMetrics) -> None:
        """A√±ade datos hist√≥ricos para an√°lisis."""
        self.historical_data.append(metrics)
        
        # Mantener solo datos dentro del per√≠odo de lookback
        cutoff_date = datetime.now() - timedelta(days=self.lookback_days)
        self.historical_data = [
            m for m in self.historical_data 
            if m.timestamp >= cutoff_date
        ]
        
        # Ordenar por timestamp
        self.historical_data.sort(key=lambda x: x.timestamp)
    
    def analyze_all_trends(self, current_data: Dict[str, Any]) -> Dict[str, Any]:
        """Ejecuta an√°lisis completo de tendencias."""
        self.log.info("Iniciando an√°lisis de tendencias")
        
        try:
            # Convertir datos actuales a m√©tricas
            current_metrics = self._convert_to_metrics(current_data)
            self.add_historical_data(current_metrics)
            
            if len(self.historical_data) < 3:
                self.log.warning("Datos hist√≥ricos insuficientes para an√°lisis de tendencias")
                return self._create_minimal_analysis(current_metrics)
            
            # Analizar tendencias por m√©trica
            trend_analyses = {}
            
            # M√©tricas principales
            metrics_to_analyze = [
                'total_vulnerabilities',
                'critical_vulnerabilities',
                'high_vulnerabilities',
                'mean_cvss_score',
                'exploitable_vulnerabilities',
                'attack_surface_score',
                'risk_score',
                'exposed_services',
                'leaked_credentials',
                'configuration_issues'
            ]
            
            for metric in metrics_to_analyze:
                trend_analyses[metric] = self._analyze_metric_trend(metric)
            
            # An√°lisis de patrones espec√≠ficos
            pattern_analysis = self._analyze_security_patterns()
            
            # Predicciones
            predictions = self._generate_predictions()
            
            # Insights y recomendaciones
            insights = self._generate_insights(trend_analyses)
            recommendations = self._generate_recommendations(trend_analyses, pattern_analysis)
            
            # Resumen ejecutivo
            executive_summary = self._create_executive_summary(trend_analyses, pattern_analysis)
            
            return {
                'trend_analyses': {k: self._trend_to_dict(v) for k, v in trend_analyses.items()},
                'pattern_analysis': pattern_analysis,
                'predictions': predictions,
                'insights': insights,
                'recommendations': recommendations,
                'executive_summary': executive_summary,
                'data_quality': self._assess_data_quality(),
                'analysis_metadata': {
                    'analysis_date': datetime.now().isoformat(),
                    'data_points': len(self.historical_data),
                    'lookback_days': self.lookback_days,
                    'earliest_data': self.historical_data[0].timestamp.isoformat() if self.historical_data else None,
                    'latest_data': self.historical_data[-1].timestamp.isoformat() if self.historical_data else None
                }
            }
            
        except Exception as e:
            self.log.error(f"Error en an√°lisis de tendencias: {e}")
            return {'error': str(e), 'trend_analyses': {}, 'insights': [], 'recommendations': []}
    
    def _convert_to_metrics(self, data: Dict[str, Any]) -> SecurityMetrics:
        """Convierte datos de pentest a m√©tricas estructuradas."""
        nuclei_data = data.get('nuclei_data', [])
        nmap_data = data.get('nmap_data', [])
        leaks_data = data.get('leaks_data', [])
        security_config_data = data.get('security_config_data', [])
        cves_data = data.get('cves_data', [])
        
        # Contar vulnerabilidades por severidad
        severity_counts = Counter()
        cvss_scores = []
        exploitable_count = 0
        
        for vuln in nuclei_data:
            severity = vuln.get('info', {}).get('severity', 'info')
            severity_counts[severity] += 1
            
            # Extraer CVSS si est√° disponible
            cvss = vuln.get('info', {}).get('classification', {}).get('cvss-score')
            if cvss:
                try:
                    cvss_scores.append(float(cvss))
                except (ValueError, TypeError):
                    pass
            
            # Determinar si es explotable
            tags = vuln.get('info', {}).get('tags', [])
            if any(tag in ['rce', 'sqli', 'xss', 'lfi', 'rfi'] for tag in tags):
                exploitable_count += 1
        
        # Servicios expuestos
        exposed_services = len([p for p in nmap_data if p.get('state') == 'open'])
        
        # Calcular puntuaci√≥n de superficie de ataque
        attack_surface_score = (
            len(nuclei_data) * 2 +
            exposed_services * 1.5 +
            len(security_config_data) * 1
        )
        
        # Calcular puntuaci√≥n de riesgo simple
        risk_score = (
            severity_counts.get('critical', 0) * 4 +
            severity_counts.get('high', 0) * 3 +
            severity_counts.get('medium', 0) * 2 +
            severity_counts.get('low', 0) * 1
        )
        
        return SecurityMetrics(
            timestamp=datetime.now(),
            total_vulnerabilities=len(nuclei_data),
            critical_vulnerabilities=severity_counts.get('critical', 0),
            high_vulnerabilities=severity_counts.get('high', 0),
            medium_vulnerabilities=severity_counts.get('medium', 0),
            low_vulnerabilities=severity_counts.get('low', 0),
            info_vulnerabilities=severity_counts.get('info', 0),
            mean_cvss_score=statistics.mean(cvss_scores) if cvss_scores else 0.0,
            exploitable_vulnerabilities=exploitable_count,
            attack_surface_score=attack_surface_score,
            risk_score=risk_score,
            exposed_services=exposed_services,
            leaked_credentials=len(leaks_data),
            configuration_issues=len(security_config_data)
        )
    
    def _analyze_metric_trend(self, metric_name: str) -> TrendAnalysis:
        """Analiza tendencia de una m√©trica espec√≠fica."""
        # Extraer valores de la m√©trica
        data_points = []
        for metrics in self.historical_data:
            value = getattr(metrics, metric_name, 0)
            data_points.append(TrendPoint(
                timestamp=metrics.timestamp,
                value=float(value)
            ))
        
        if len(data_points) < 2:
            return TrendAnalysis(
                metric_name=metric_name,
                direction=TrendDirection.STABLE,
                significance=TrendSignificance.LOW,
                slope=0.0,
                correlation_coefficient=0.0,
                confidence_interval=(0.0, 0.0),
                data_points=data_points
            )
        
        # Calcular tendencia lineal
        x_values = [(point.timestamp - data_points[0].timestamp).days for point in data_points]
        y_values = [point.value for point in data_points]
        
        # Regresi√≥n lineal simple
        n = len(x_values)
        sum_x = sum(x_values)
        sum_y = sum(y_values)
        sum_xy = sum(x * y for x, y in zip(x_values, y_values))
        sum_x2 = sum(x * x for x in x_values)
        
        # Calcular pendiente y correlaci√≥n
        if n * sum_x2 - sum_x * sum_x != 0:
            slope = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x * sum_x)
            
            # Coeficiente de correlaci√≥n
            mean_x = sum_x / n
            mean_y = sum_y / n
            
            numerator = sum((x - mean_x) * (y - mean_y) for x, y in zip(x_values, y_values))
            denominator_x = sum((x - mean_x) ** 2 for x in x_values)
            denominator_y = sum((y - mean_y) ** 2 for y in y_values)
            
            if denominator_x > 0 and denominator_y > 0:
                correlation = numerator / (denominator_x * denominator_y) ** 0.5
            else:
                correlation = 0.0
        else:
            slope = 0.0
            correlation = 0.0
        
        # Determinar direcci√≥n
        if abs(slope) < 0.01:
            direction = TrendDirection.STABLE
        elif slope > 0:
            direction = TrendDirection.INCREASING
        else:
            direction = TrendDirection.DECREASING
        
        # Verificar volatilidad
        if len(y_values) > 2:
            volatility = statistics.stdev(y_values) / (statistics.mean(y_values) + 0.001)
            if volatility > 0.5:
                direction = TrendDirection.VOLATILE
        
        # Determinar significancia
        significance = self._determine_significance(abs(slope), abs(correlation))
        
        # Calcular intervalo de confianza (simplificado)
        if len(y_values) > 1:
            std_dev = statistics.stdev(y_values)
            confidence_interval = (mean_y - 1.96 * std_dev, mean_y + 1.96 * std_dev)
        else:
            confidence_interval = (y_values[0], y_values[0]) if y_values else (0.0, 0.0)
        
        # Generar predicci√≥n
        prediction = None
        prediction_confidence = 0.0
        if abs(correlation) > 0.3 and len(data_points) >= 5:
            # Predicci√≥n para 30 d√≠as en el futuro
            future_x = x_values[-1] + 30
            prediction = slope * future_x + (mean_y - slope * mean_x)
            prediction_confidence = min(abs(correlation), 0.8)
        
        # Generar insights
        insights = self._generate_metric_insights(metric_name, direction, slope, correlation, y_values)
        
        return TrendAnalysis(
            metric_name=metric_name,
            direction=direction,
            significance=significance,
            slope=slope,
            correlation_coefficient=correlation,
            confidence_interval=confidence_interval,
            data_points=data_points,
            prediction=prediction,
            prediction_confidence=prediction_confidence,
            insights=insights
        )
    
    def _determine_significance(self, slope: float, correlation: float) -> TrendSignificance:
        """Determina la significancia de una tendencia."""
        slope_thresholds = self.significance_thresholds['slope']
        correlation_thresholds = self.significance_thresholds['correlation']
        
        if slope >= slope_thresholds['high'] and correlation >= correlation_thresholds['high']:
            return TrendSignificance.HIGH
        elif slope >= slope_thresholds['medium'] and correlation >= correlation_thresholds['medium']:
            return TrendSignificance.MEDIUM
        else:
            return TrendSignificance.LOW
    
    def _analyze_security_patterns(self) -> Dict[str, Any]:
        """Analiza patrones espec√≠ficos de seguridad."""
        patterns = {
            'vulnerability_lifecycle': self._analyze_vulnerability_lifecycle(),
            'attack_surface_evolution': self._analyze_attack_surface_evolution(),
            'risk_concentration': self._analyze_risk_concentration(),
            'seasonal_patterns': self._analyze_seasonal_patterns(),
            'correlation_patterns': self._analyze_metric_correlations()
        }
        
        return patterns
    
    def _analyze_vulnerability_lifecycle(self) -> Dict[str, Any]:
        """Analiza el ciclo de vida de vulnerabilidades."""
        if len(self.historical_data) < 3:
            return {}
        
        # Calcular tiempo promedio de resoluci√≥n
        resolution_times = []
        discovery_rates = []
        
        for i in range(1, len(self.historical_data)):
            current = self.historical_data[i]
            previous = self.historical_data[i-1]
            
            # Tasa de descubrimiento
            new_vulns = max(0, current.total_vulnerabilities - previous.total_vulnerabilities)
            days_diff = (current.timestamp - previous.timestamp).days
            if days_diff > 0:
                discovery_rate = new_vulns / days_diff
                discovery_rates.append(discovery_rate)
        
        avg_discovery_rate = statistics.mean(discovery_rates) if discovery_rates else 0
        
        return {
            'average_discovery_rate': round(avg_discovery_rate, 2),
            'discovery_trend': 'increasing' if len(discovery_rates) > 1 and discovery_rates[-1] > discovery_rates[0] else 'stable',
            'total_periods_analyzed': len(discovery_rates)
        }
    
    def _analyze_attack_surface_evolution(self) -> Dict[str, Any]:
        """Analiza la evoluci√≥n de la superficie de ataque."""
        if len(self.historical_data) < 2:
            return {}
        
        surface_scores = [m.attack_surface_score for m in self.historical_data]
        service_counts = [m.exposed_services for m in self.historical_data]
        
        surface_trend = 'increasing' if surface_scores[-1] > surface_scores[0] else 'decreasing'
        service_trend = 'increasing' if service_counts[-1] > service_counts[0] else 'decreasing'
        
        return {
            'surface_score_trend': surface_trend,
            'exposed_services_trend': service_trend,
            'current_surface_score': surface_scores[-1],
            'surface_score_change': surface_scores[-1] - surface_scores[0],
            'volatility': statistics.stdev(surface_scores) if len(surface_scores) > 1 else 0
        }
    
    def _analyze_risk_concentration(self) -> Dict[str, Any]:
        """Analiza la concentraci√≥n de riesgo."""
        if len(self.historical_data) < 2:
            return {}
        
        latest = self.historical_data[-1]
        total_vulns = latest.total_vulnerabilities
        
        if total_vulns == 0:
            return {'risk_concentration': 'low', 'critical_ratio': 0}
        
        critical_ratio = latest.critical_vulnerabilities / total_vulns
        high_ratio = latest.high_vulnerabilities / total_vulns
        
        concentration = 'high' if critical_ratio > 0.2 or high_ratio > 0.4 else 'medium' if critical_ratio > 0.1 else 'low'
        
        return {
            'risk_concentration': concentration,
            'critical_ratio': round(critical_ratio, 3),
            'high_ratio': round(high_ratio, 3),
            'top_severity_percentage': round((critical_ratio + high_ratio) * 100, 1)
        }
    
    def _analyze_seasonal_patterns(self) -> Dict[str, Any]:
        """Analiza patrones estacionales (simplificado)."""
        if len(self.historical_data) < 4:
            return {}
        
        # Agrupar por d√≠a de la semana
        weekday_metrics = defaultdict(list)
        for metrics in self.historical_data:
            weekday = metrics.timestamp.weekday()
            weekday_metrics[weekday].append(metrics.total_vulnerabilities)
        
        # Calcular promedios por d√≠a
        weekday_averages = {}
        for weekday, values in weekday_metrics.items():
            if values:
                weekday_averages[weekday] = statistics.mean(values)
        
        return {
            'weekday_patterns': weekday_averages,
            'peak_day': max(weekday_averages.items(), key=lambda x: x[1])[0] if weekday_averages else None
        }
    
    def _analyze_metric_correlations(self) -> Dict[str, Any]:
        """Analiza correlaciones entre m√©tricas."""
        if len(self.historical_data) < 3:
            return {}
        
        # M√©tricas para correlacionar
        metrics = {
            'total_vulnerabilities': [m.total_vulnerabilities for m in self.historical_data],
            'critical_vulnerabilities': [m.critical_vulnerabilities for m in self.historical_data],
            'exposed_services': [m.exposed_services for m in self.historical_data],
            'attack_surface_score': [m.attack_surface_score for m in self.historical_data],
            'risk_score': [m.risk_score for m in self.historical_data]
        }
        
        correlations = {}
        
        # Calcular correlaciones entre pares de m√©tricas
        metric_names = list(metrics.keys())
        for i, metric1 in enumerate(metric_names):
            for metric2 in metric_names[i+1:]:
                values1 = metrics[metric1]
                values2 = metrics[metric2]
                
                if len(values1) == len(values2) and len(values1) > 1:
                    correlation = self._calculate_correlation(values1, values2)
                    if abs(correlation) > 0.3:  # Solo correlaciones significativas
                        correlations[f"{metric1}_vs_{metric2}"] = round(correlation, 3)
        
        return correlations
    
    def _calculate_correlation(self, x_values: List[float], y_values: List[float]) -> float:
        """Calcula coeficiente de correlaci√≥n de Pearson."""
        if len(x_values) != len(y_values) or len(x_values) < 2:
            return 0.0
        
        n = len(x_values)
        mean_x = sum(x_values) / n
        mean_y = sum(y_values) / n
        
        numerator = sum((x - mean_x) * (y - mean_y) for x, y in zip(x_values, y_values))
        denominator_x = sum((x - mean_x) ** 2 for x in x_values)
        denominator_y = sum((y - mean_y) ** 2 for y in y_values)
        
        if denominator_x > 0 and denominator_y > 0:
            return numerator / (denominator_x * denominator_y) ** 0.5
        else:
            return 0.0
    
    def _generate_predictions(self) -> Dict[str, Any]:
        """Genera predicciones basadas en tendencias."""
        predictions = {}
        
        if len(self.historical_data) < 5:
            return predictions
        
        # Predicciones para m√©tricas clave
        key_metrics = ['total_vulnerabilities', 'critical_vulnerabilities', 'risk_score']
        
        for metric in key_metrics:
            trend = self._analyze_metric_trend(metric)
            if trend.prediction is not None and trend.prediction_confidence > 0.3:
                predictions[metric] = {
                    'predicted_value': round(trend.prediction, 2),
                    'confidence': round(trend.prediction_confidence, 2),
                    'timeframe_days': 30,
                    'trend_direction': trend.direction.value
                }
        
        return predictions
    
    def _generate_metric_insights(self, metric_name: str, direction: TrendDirection, slope: float, correlation: float, values: List[float]) -> List[str]:
        """Genera insights espec√≠ficos para una m√©trica."""
        insights = []
        
        # Insights por direcci√≥n de tendencia
        if direction == TrendDirection.INCREASING:
            if metric_name in ['total_vulnerabilities', 'critical_vulnerabilities', 'risk_score']:
                insights.append(f"‚ö†Ô∏è {metric_name.replace('_', ' ').title()} muestra tendencia creciente")
            elif metric_name in ['patched_vulnerabilities', 'resolved_vulnerabilities']:
                insights.append(f"‚úÖ {metric_name.replace('_', ' ').title()} muestra mejora continua")
        
        elif direction == TrendDirection.DECREASING:
            if metric_name in ['total_vulnerabilities', 'critical_vulnerabilities', 'risk_score']:
                insights.append(f"‚úÖ {metric_name.replace('_', ' ').title()} muestra tendencia decreciente")
            elif metric_name in ['patched_vulnerabilities', 'resolved_vulnerabilities']:
                insights.append(f"‚ö†Ô∏è {metric_name.replace('_', ' ').title()} muestra declive")
        
        elif direction == TrendDirection.VOLATILE:
            insights.append(f"üìä {metric_name.replace('_', ' ').title()} muestra alta volatilidad")
        
        # Insights por correlaci√≥n
        if abs(correlation) > 0.7:
            insights.append(f"Tendencia muy consistente (correlaci√≥n: {correlation:.2f})")
        elif abs(correlation) < 0.3:
            insights.append("Tendencia poco consistente - revisar factores externos")
        
        # Insights por valores extremos
        if values:
            current_value = values[-1]
            max_value = max(values)
            min_value = min(values)
            
            if current_value == max_value and len(values) > 2:
                insights.append("üìà Valor actual en m√°ximo hist√≥rico")
            elif current_value == min_value and len(values) > 2:
                insights.append("üìâ Valor actual en m√≠nimo hist√≥rico")
        
        return insights
    
    def _generate_insights(self, trend_analyses: Dict[str, TrendAnalysis]) -> List[str]:
        """Genera insights generales del an√°lisis."""
        insights = []
        
        # Contar tendencias por direcci√≥n
        direction_counts = Counter(analysis.direction for analysis in trend_analyses.values())
        
        if direction_counts[TrendDirection.INCREASING] > direction_counts[TrendDirection.DECREASING]:
            insights.append("üî∫ La mayor√≠a de m√©tricas muestran tendencia creciente")
        elif direction_counts[TrendDirection.DECREASING] > direction_counts[TrendDirection.INCREASING]:
            insights.append("üîª La mayor√≠a de m√©tricas muestran tendencia decreciente")
        
        # Identificar m√©tricas de alta significancia
        high_significance = [name for name, analysis in trend_analyses.items() 
                           if analysis.significance == TrendSignificance.HIGH]
        
        if high_significance:
            insights.append(f"‚ö° Tendencias de alta significancia: {', '.join(high_significance)}")
        
        # Identificar m√©tricas vol√°tiles
        volatile_metrics = [name for name, analysis in trend_analyses.items() 
                          if analysis.direction == TrendDirection.VOLATILE]
        
        if volatile_metrics:
            insights.append(f"üìä M√©tricas con alta volatilidad: {', '.join(volatile_metrics)}")
        
        return insights
    
    def _generate_recommendations(self, trend_analyses: Dict[str, TrendAnalysis], pattern_analysis: Dict[str, Any]) -> List[str]:
        """Genera recomendaciones basadas en el an√°lisis."""
        recommendations = []
        
        # Recomendaciones por tendencias cr√≠ticas
        critical_increasing = [name for name, analysis in trend_analyses.items() 
                             if analysis.direction == TrendDirection.INCREASING and 
                             name in ['critical_vulnerabilities', 'risk_score'] and
                             analysis.significance != TrendSignificance.LOW]
        
        if critical_increasing:
            recommendations.append("üö® URGENTE: Implementar plan de remediaci√≥n para tendencias cr√≠ticas crecientes")
        
        # Recomendaciones por volatilidad
        volatile_count = sum(1 for analysis in trend_analyses.values() 
                           if analysis.direction == TrendDirection.VOLATILE)
        
        if volatile_count >= 3:
            recommendations.append("üìä Implementar monitoreo m√°s frecuente debido a alta volatilidad")
        
        # Recomendaciones por superficie de ataque
        surface_evolution = pattern_analysis.get('attack_surface_evolution', {})
        if surface_evolution.get('surface_score_trend') == 'increasing':
            recommendations.append("üõ°Ô∏è Revisar y reducir superficie de ataque expuesta")
        
        # Recomendaciones por concentraci√≥n de riesgo
        risk_concentration = pattern_analysis.get('risk_concentration', {})
        if risk_concentration.get('risk_concentration') == 'high':
            recommendations.append("‚öñÔ∏è Diversificar esfuerzos de seguridad - alta concentraci√≥n de riesgo detectada")
        
        # Recomendaciones por predicciones
        for name, analysis in trend_analyses.items():
            if (analysis.prediction is not None and 
                analysis.prediction_confidence > 0.5 and 
                name in ['critical_vulnerabilities', 'risk_score']):
                
                if analysis.direction == TrendDirection.INCREASING:
                    recommendations.append(f"üìà Preparar recursos adicionales - {name} proyectado a aumentar")
        
        return recommendations
    
    def _create_executive_summary(self, trend_analyses: Dict[str, TrendAnalysis], pattern_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Crea resumen ejecutivo del an√°lisis de tendencias."""
        # M√©tricas clave
        total_vulns_trend = trend_analyses.get('total_vulnerabilities')
        critical_vulns_trend = trend_analyses.get('critical_vulnerabilities')
        risk_score_trend = trend_analyses.get('risk_score')
        
        # Estado general
        improving_trends = sum(1 for analysis in trend_analyses.values() 
                             if analysis.direction == TrendDirection.DECREASING and 
                             analysis.metric_name in ['total_vulnerabilities', 'critical_vulnerabilities', 'risk_score'])
        
        worsening_trends = sum(1 for analysis in trend_analyses.values() 
                             if analysis.direction == TrendDirection.INCREASING and 
                             analysis.metric_name in ['total_vulnerabilities', 'critical_vulnerabilities', 'risk_score'])
        
        if improving_trends > worsening_trends:
            overall_trend = "Mejorando"
            trend_emoji = "üìà‚úÖ"
        elif worsening_trends > improving_trends:
            overall_trend = "Empeorando"
            trend_emoji = "üìâ‚ö†Ô∏è"
        else:
            overall_trend = "Estable"
            trend_emoji = "üìä"
        
        return {
            'overall_trend': overall_trend,
            'trend_emoji': trend_emoji,
            'key_metrics': {
                'total_vulnerabilities': {
                    'direction': total_vulns_trend.direction.value if total_vulns_trend else 'unknown',
                    'significance': total_vulns_trend.significance.value if total_vulns_trend else 'unknown'
                },
                'critical_vulnerabilities': {
                    'direction': critical_vulns_trend.direction.value if critical_vulns_trend else 'unknown',
                    'significance': critical_vulns_trend.significance.value if critical_vulns_trend else 'unknown'
                },
                'risk_score': {
                    'direction': risk_score_trend.direction.value if risk_score_trend else 'unknown',
                    'significance': risk_score_trend.significance.value if risk_score_trend else 'unknown'
                }
            },
            'data_quality': self._assess_data_quality(),
            'analysis_confidence': self._calculate_overall_confidence(trend_analyses)
        }
    
    def _assess_data_quality(self) -> Dict[str, Any]:
        """Eval√∫a la calidad de los datos para el an√°lisis."""
        data_points = len(self.historical_data)
        
        if data_points >= 10:
            quality = "Alta"
        elif data_points >= 5:
            quality = "Media"
        elif data_points >= 3:
            quality = "Baja"
        else:
            quality = "Insuficiente"
        
        # Verificar consistencia temporal
        if len(self.historical_data) > 1:
            time_gaps = []
            for i in range(1, len(self.historical_data)):
                gap = (self.historical_data[i].timestamp - self.historical_data[i-1].timestamp).days
                time_gaps.append(gap)
            
            avg_gap = statistics.mean(time_gaps) if time_gaps else 0
            gap_consistency = "Alta" if statistics.stdev(time_gaps) < 2 else "Media" if statistics.stdev(time_gaps) < 5 else "Baja"
        else:
            avg_gap = 0
            gap_consistency = "N/A"
        
        return {
            'quality_level': quality,
            'data_points': data_points,
            'average_time_gap_days': round(avg_gap, 1),
            'temporal_consistency': gap_consistency,
            'sufficient_for_prediction': data_points >= 5
        }
    
    def _calculate_overall_confidence(self, trend_analyses: Dict[str, TrendAnalysis]) -> float:
        """Calcula confianza general del an√°lisis."""
        if not trend_analyses:
            return 0.0
        
        # Promedio de correlaciones absolutas
        correlations = [abs(analysis.correlation_coefficient) for analysis in trend_analyses.values()]
        avg_correlation = statistics.mean(correlations) if correlations else 0.0
        
        # Ajustar por cantidad de datos
        data_quality_factor = min(1.0, len(self.historical_data) / 10)
        
        overall_confidence = avg_correlation * data_quality_factor
        
        return round(overall_confidence, 2)
    
    def _create_minimal_analysis(self, current_metrics: SecurityMetrics) -> Dict[str, Any]:
        """Crea an√°lisis m√≠nimo cuando no hay suficientes datos hist√≥ricos."""
        return {
            'trend_analyses': {},
            'pattern_analysis': {},
            'predictions': {},
            'insights': ["üìä Datos hist√≥ricos insuficientes para an√°lisis de tendencias"],
            'recommendations': [
                "üìà Continuar recopilando datos para an√°lisis futuro",
                "üîÑ Ejecutar escaneos de forma regular para establecer l√≠nea base"
            ],
            'executive_summary': {
                'overall_trend': 'Datos insuficientes',
                'trend_emoji': 'üìä',
                'data_quality': self._assess_data_quality()
            },
            'current_snapshot': {
                'total_vulnerabilities': current_metrics.total_vulnerabilities,
                'critical_vulnerabilities': current_metrics.critical_vulnerabilities,
                'risk_score': current_metrics.risk_score,
                'timestamp': current_metrics.timestamp.isoformat()
            }
        }
    
    def _trend_to_dict(self, trend: TrendAnalysis) -> Dict[str, Any]:
        """Convierte TrendAnalysis a diccionario."""
        return {
            'metric_name': trend.metric_name,
            'direction': trend.direction.value,
            'significance': trend.significance.value,
            'slope': round(trend.slope, 4),
            'correlation_coefficient': round(trend.correlation_coefficient, 3),
            'confidence_interval': [round(trend.confidence_interval[0], 2), round(trend.confidence_interval[1], 2)],
            'data_points_count': len(trend.data_points),
            'prediction': round(trend.prediction, 2) if trend.prediction is not None else None,
            'prediction_confidence': round(trend.prediction_confidence, 2),
            'insights': trend.insights,
            'recommendations': trend.recommendations
        }


# ============================================================================
# FUNCIONES DE UTILIDAD
# ============================================================================

def create_trend_engine(lookback_days: int = 90) -> TrendAnalysisEngine:
    """Factory function para crear motor de an√°lisis de tendencias."""
    return TrendAnalysisEngine(lookback_days=lookback_days)


def analyze_security_trends(current_data: Dict[str, Any], historical_data: List[Dict[str, Any]] = None) -> Dict[str, Any]:
    """Funci√≥n de conveniencia para an√°lisis completo de tendencias."""
    engine = create_trend_engine()
    
    # Cargar datos hist√≥ricos si se proporcionan
    if historical_data:
        for data in historical_data:
            metrics = engine._convert_to_metrics(data)
            engine.add_historical_data(metrics)
    
    return engine.analyze_all_trends(current_data)